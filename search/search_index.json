{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NERC Technical Documentation NERC welcomes your contributions These pages are hosted from a git repository and contributions are welcome! Fork this repo","title":"Home"},{"location":"#nerc-technical-documentation","text":"NERC welcomes your contributions These pages are hosted from a git repository and contributions are welcome! Fork this repo","title":"NERC Technical Documentation"},{"location":"about/","text":"About NERC We are currently in the pilot phase of the project and are focusing on developing the technology to make it easy for researchers to take advantage of a suite of services ( IaaS, PaaS, SaaS ) that are not readily available today. This includes: The creation of the building blocks needed for production cloud services Begin collaboration with Systems Engineers from other institutions with well established RC groups On-board select proof of concept use cases from institutions within the MGHPCC consortium and other institutions within Massachusetts The longer term objectives will be centered around activities that will focus on: Engaging with various OpenStack communities by sharing best practices and setting standards for deployments Connecting regularly with the Mass Open Cloud (MOC) leadership to understand when new technologies they are developing with RedHat, Inc. \u2013 and as part of the new NSF funded Open Cloud Testbed \u2013 might be ready for adoption into the production NERC environment Broadening the local deployment team of NERC to include partner universities within the MGHPCC consortium. Figure 1: NERC Overview NERC production services ( red ) stand on top of the existing NESE storage services ( blue ) are built on the strong foundation of MGHPCC ( green ) that provides core facility and network access. The Innovation Hub ( grey ) enables new technologies to be rapidly adopted by the NERC or NESE services. On the far left ( purple ) are the Research and Learning communities which are the primary customers of NERC. As users proceed down the stack of production services from Web-apps, that require more technical skills, the Cloud Facilitators ( orange ) in the middle guide and educate users on how to best use the services. For more information, view NERC's concept document.","title":"About"},{"location":"about/#about-nerc","text":"We are currently in the pilot phase of the project and are focusing on developing the technology to make it easy for researchers to take advantage of a suite of services ( IaaS, PaaS, SaaS ) that are not readily available today. This includes: The creation of the building blocks needed for production cloud services Begin collaboration with Systems Engineers from other institutions with well established RC groups On-board select proof of concept use cases from institutions within the MGHPCC consortium and other institutions within Massachusetts The longer term objectives will be centered around activities that will focus on: Engaging with various OpenStack communities by sharing best practices and setting standards for deployments Connecting regularly with the Mass Open Cloud (MOC) leadership to understand when new technologies they are developing with RedHat, Inc. \u2013 and as part of the new NSF funded Open Cloud Testbed \u2013 might be ready for adoption into the production NERC environment Broadening the local deployment team of NERC to include partner universities within the MGHPCC consortium. Figure 1: NERC Overview NERC production services ( red ) stand on top of the existing NESE storage services ( blue ) are built on the strong foundation of MGHPCC ( green ) that provides core facility and network access. The Innovation Hub ( grey ) enables new technologies to be rapidly adopted by the NERC or NESE services. On the far left ( purple ) are the Research and Learning communities which are the primary customers of NERC. As users proceed down the stack of production services from Web-apps, that require more technical skills, the Cloud Facilitators ( orange ) in the middle guide and educate users on how to best use the services. For more information, view NERC's concept document.","title":"About NERC"},{"location":"get-started/create-a-user-portal-account/","text":"Overview NERC\u2019s Research allocations are available to faculty members and researchers, including postdoctoral researchers and students, at a U.S. based institution in New England. In order to get access to resources provided by NERC\u2019s computational infrastructure, you must first register and obtain a user account. The overall user flow can be summarized using the following sequence diagram: User Accounts NERC offers two types of user accounts: a Principal Investigator (PI) Account and a General User Account . All General Users must be sponsored by a PI with an active NERC PI account. All university faculty and staff researchers eligible to be a grant PI are eligible to obtain a NERC account by completing a PI Account Request. If you are unsure whether you qualify for an NERC PI account, see Principal Investigator Eligibility for information about the NERC\u2019s policy concerning PI eligibility. PI accounts are able to request Resource Allocations. A PI account enables a user to log into NERC's computational project space; apply for allocations of NERC resources and grant access to other users; and delegate responsibilities to other collaborators from the same institutions or elsewhere as managers using NERC\u2019s ColdFront interface . Getting Started Any faculty, staff, student, and external collaborator must request an user account through the MGHPCC Shared Services (MGHPCC-SS) Account Portal This is a web-based, single point-of-entry to the NERC system that displays a user welcome page. The welcome page of the account registration site displays instructions on how to register a General User account on NERC, which are shown in the image below: There are two options: either register for a new account or to manage an existing one. If you are new to NERC and want to register as a new MGHPCC-SS user, click on the \"Register for an Account\" button. This will redirect you to a new web page which shows details about how to register for a new MGHPCC-SS user account. NERC uses CILogon that supports login either using your Institutional Identity Provider (IdP). Clicking the \"Begin MGHPCC-SS Account Creation Process\" button will initiate the account creation process. You will be redirected to a site managed by CILogon where you will select your institutional or commercial identity provider as shown below: Once selected, you will be redirected to your institutional or commercial identity provider where you will login as shown here: At the completion of that logon, your browser will be redirected to the MGHPCC-SS Account Management site where you will complete your MGHPCC-SS account creation process. After successful logon, your browser will redirect back to the MGHPCC-SS Registration Page and ask for review and confirmation of creating your account with fetched information to complete the account creation process. Very Important If you don't click the \"Create MGHPCC-SS Account\" button, your account will not be created! So this is a very important step, to review and also click on the \"Create MGHPCC-SS Account\" button to save your information. You can make any corrections that you need and fill in any blank fields i.e. \"Research Domain\" and then click the \"Create MGHPCC-SS Account\" button. This will automatically send an email to your email address with a link to validate and confirm your account information. Once you get an email and click on the provided validation web link, you will make sure that your user account is created and valid by viewing the following page: Help If you have an institutional identity, it's preferable that you use that identity to create your MGHPCC-SS account. Institutional identities are vetted by identity management teams and provide a higher level of confidence to resource owners when granting access to resources. You can only link one university account to a MGHPCC-SS account; if you have multiple university accounts you will only be able to link one of those accounts to your MGHPCC-SS account. If at a later date, you want to change which account is connected to your MGHPCC-SS identity you can by contacting help@nese.mghpcc.org . How to update and modify your MGHPCC-SS account information? Login your MGHPCC-SS account Click on \"Manage Your MGHPCC-SS Account\" button Review your currently saved account information, change any fields that require correction or updates and then click on the \"Update MGHPCC-SS Account\" button. This will send an email to verify your updated account information so check your email address. Confirm and validate the new account details by verifying them by clicking the provided link on your email. How to request a Principal Investigator (PI) Account? The process for requesting and obtaining PI Account is relatively simple by filling out this NERC Principal Investigator (PI) Account Request form . Information So, once your PI user request is reviewed and approved by NERC's administration staff - you will recieve an email confirmation form NERC's RT system. You are going to get into NERC's ColdFront resource allocation management portal using the PI user role.","title":"How to Create a User Account"},{"location":"get-started/create-a-user-portal-account/#overview","text":"NERC\u2019s Research allocations are available to faculty members and researchers, including postdoctoral researchers and students, at a U.S. based institution in New England. In order to get access to resources provided by NERC\u2019s computational infrastructure, you must first register and obtain a user account. The overall user flow can be summarized using the following sequence diagram:","title":"Overview"},{"location":"get-started/create-a-user-portal-account/#user-accounts","text":"NERC offers two types of user accounts: a Principal Investigator (PI) Account and a General User Account . All General Users must be sponsored by a PI with an active NERC PI account. All university faculty and staff researchers eligible to be a grant PI are eligible to obtain a NERC account by completing a PI Account Request. If you are unsure whether you qualify for an NERC PI account, see Principal Investigator Eligibility for information about the NERC\u2019s policy concerning PI eligibility. PI accounts are able to request Resource Allocations. A PI account enables a user to log into NERC's computational project space; apply for allocations of NERC resources and grant access to other users; and delegate responsibilities to other collaborators from the same institutions or elsewhere as managers using NERC\u2019s ColdFront interface .","title":"User Accounts"},{"location":"get-started/create-a-user-portal-account/#getting-started","text":"Any faculty, staff, student, and external collaborator must request an user account through the MGHPCC Shared Services (MGHPCC-SS) Account Portal This is a web-based, single point-of-entry to the NERC system that displays a user welcome page. The welcome page of the account registration site displays instructions on how to register a General User account on NERC, which are shown in the image below: There are two options: either register for a new account or to manage an existing one. If you are new to NERC and want to register as a new MGHPCC-SS user, click on the \"Register for an Account\" button. This will redirect you to a new web page which shows details about how to register for a new MGHPCC-SS user account. NERC uses CILogon that supports login either using your Institutional Identity Provider (IdP). Clicking the \"Begin MGHPCC-SS Account Creation Process\" button will initiate the account creation process. You will be redirected to a site managed by CILogon where you will select your institutional or commercial identity provider as shown below: Once selected, you will be redirected to your institutional or commercial identity provider where you will login as shown here: At the completion of that logon, your browser will be redirected to the MGHPCC-SS Account Management site where you will complete your MGHPCC-SS account creation process. After successful logon, your browser will redirect back to the MGHPCC-SS Registration Page and ask for review and confirmation of creating your account with fetched information to complete the account creation process. Very Important If you don't click the \"Create MGHPCC-SS Account\" button, your account will not be created! So this is a very important step, to review and also click on the \"Create MGHPCC-SS Account\" button to save your information. You can make any corrections that you need and fill in any blank fields i.e. \"Research Domain\" and then click the \"Create MGHPCC-SS Account\" button. This will automatically send an email to your email address with a link to validate and confirm your account information. Once you get an email and click on the provided validation web link, you will make sure that your user account is created and valid by viewing the following page: Help If you have an institutional identity, it's preferable that you use that identity to create your MGHPCC-SS account. Institutional identities are vetted by identity management teams and provide a higher level of confidence to resource owners when granting access to resources. You can only link one university account to a MGHPCC-SS account; if you have multiple university accounts you will only be able to link one of those accounts to your MGHPCC-SS account. If at a later date, you want to change which account is connected to your MGHPCC-SS identity you can by contacting help@nese.mghpcc.org .","title":"Getting Started"},{"location":"get-started/create-a-user-portal-account/#how-to-update-and-modify-your-mghpcc-ss-account-information","text":"Login your MGHPCC-SS account Click on \"Manage Your MGHPCC-SS Account\" button Review your currently saved account information, change any fields that require correction or updates and then click on the \"Update MGHPCC-SS Account\" button. This will send an email to verify your updated account information so check your email address. Confirm and validate the new account details by verifying them by clicking the provided link on your email.","title":"How to update and modify your MGHPCC-SS account information?"},{"location":"get-started/create-a-user-portal-account/#how-to-request-a-principal-investigator-pi-account","text":"The process for requesting and obtaining PI Account is relatively simple by filling out this NERC Principal Investigator (PI) Account Request form . Information So, once your PI user request is reviewed and approved by NERC's administration staff - you will recieve an email confirmation form NERC's RT system. You are going to get into NERC's ColdFront resource allocation management portal using the PI user role.","title":"How to request a Principal Investigator (PI) Account?"},{"location":"get-started/get-an-allocation/","text":"What is NERC's ColdFront? NERC uses an open source resource allocation management system called ColdFront to provide a single point-of-entry for administration, reporting, and measuring scientific impact of NERC resources for PI. How to get access to NERC's ColdFront General Users who are not PIs or Managers on a project see a read-only view of the NERC's ColdFront. Whereas, once a PI Account request is granted, the PI will receive an email confirming the request approval and how to connect NERC\u2019s ColdFront. PIs or Project Owners can use NERC's ColdFront as a self-service web-portal to do the following tasks: Add or manage or archive projects Request allocations that fall under projects to NERC\u2019s resources such as clusters, cloud resources, servers, storage, and software licenses Add/remove user access to/from allocated resources who is a member of the project without requiring system administrator interaction Elevate selected users to 'manager' status, allowing them to handle some of the PI asks such as request new resource allocations, add/remove users to/from resource allocations, add project data such as grants and publications Monitor resource utilization such as storage and cloud usage Receive email notifications for expiring/renewing access to resources as well as notifications when allocations change status - i.e. activated, expired, denied Provide information such as grants, publications, and other reportable data for periodic review by center director to demonstrate need for the resources What PIs need to fill in order to request a Project? Once logged in to NERC\u2019s ColdFront, PIs can choose Projects sub-menu located under the Project menu. On clicking the \"Add a project\" button will show interface like below: PIs need to specify appropriate title, description of their research work that you will accomplish at NERC in one to two paragraphs and Field of science or research domain and then click the \"Save\" button. Once saved successfully, PIs effectively become the \"manager\" of the project, and are free to add or remove users and also request resource allocation(s) to any Projects for which they are the PI. PIs are permitted to add users to their group, request new allocations, renew expiring allocations, and provide information such as publications and grant data. PIs can maintain all their research information under one project or if they require they can separate the work into multiple projects. Very Important Make sure to select NERC (OpenStack) on Resource option and specify your expected Units of computing. Be mindful, you can extend your current resource allocations on your current project later on. Resource Allocation Quotas The amount of quota to start out a resource allocation after approval, can be specified using an integer field in the resource allocation request form as shown above. The provided unit value is computed as PIs or project managers request resource quota. The basic unit of computational resources is defined in terms of integer value that corresponds to multiple openstack resource quotas. For example, 1 Unit corresponds to: Resource Name Quota Amount x Unit Instances 1 vCPUs 2 RAM 4096 Volumes 1 Adding and removing User from the Project A user can only view projects they are on. PIs or managers can add or remove users from their respective projects by navigating to the Users section of the project. Once we click on the \"Add Users\" button, it will shows the following search interface: They can search for any users in the system that are not already part of the project by providing exact matched username or partial text of other multiple fields. The search results show details about the user account such as email address, username, first name, last name etc. as shown below: Thus, found user(s) can be selected and assigned directly to the available resource allocation(s) on the given project using this interface. While adding the users, their Role also can be selected from the dropdown options as either User or Manager. Once confirmed with selection of user(s) their roles and allocations, click on the \"Add Selected Users to Project\" button. Removing Users from the Project is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PIs or project owners can select the user(s) and then click on the \"Remove Selected Users From Project\" button. User Roles Access to ColdFront is role based so users see a read-only view of the allocation details for any allocations they are on. PIs see the same allocation details as general users and can also add project users to the allocation if they're not already on it. Even on the first time, PIs add any user to the project as the User role. Later PIs or project owners can upgrade users on their project to the 'manager' role. This allows multiple managers on the same project. This provides the user with the same access and abilities as the PI. The only things a PI can do that a manager can't, is create a new project or archive a project. All other project related tasks that a PI can do, a manager on that project can accomplish as well. General User Accounts are not able to create/update projects and request Resource Allocations. Instead, these accounts must be associated with a Project that has Resources. General User accounts that are associated with a Project have access to view their project details and use all the resources associated with the Project on NERC. General Users (not PIs or Managers) can turn off email notifications at the project level. PIs also have the 'manager' status on a project. Managers can't turn off their notifications. This ensures they continue to get allocation expiration notification emails. Adding User to Manager Role To change a user's role to 'manager' click on the edit icon next to the user's name on the Project Detail page: Then toggle the \"Role\" from User to Manager: Very Important Make sure to click the \"Update\" button to save the change. PI and Manager Allocation View PIs and managers can view important details of the allocation including start and end dates, creation and last modified dates, users on the allocation and public allocation attributes. PIs and managers can add or remove users from allocations. Adding and removing project Users to project Resource Allocation Any available users on a given project can be added to resource allocation by clicking on the \"Add Users\" button as shown below: Once Clicked it will show the following interface where PIs can select the available user(s) on the checkboxes and click on the \"Add Selected Users to Allocation\" button. Very Important The desired user must already be on the project to be added to the allocation. Removing Users from the Resource Allocation is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PIs or project owners can select the user(s) on the checkboxes and then click on the \"Remove Selected Users From Project\" button. Adding a new Resource Allocation to the project If one resource allocation is not sufficient for a project, PIs or project owners can request for another allocation(s) by clicking on the \"Request Resource Allocation\" button on the Allocations section of the project details. This will show the page where all existing users for the project will be listed on the bottom of the request form. PIs can select all or only desired user(s) to request the resource allocations to be available on NERC\u2019s OpenStack. General User View General Users who are not PIs or Managers on a project see a read-only view of the allocation details. If a user is on a project but not a particular allocation, they will not be able to see the allocation in the Project view nor will they be able to access the Allocation detail page.","title":"How to Get An Allocation"},{"location":"get-started/get-an-allocation/#what-is-nercs-coldfront","text":"NERC uses an open source resource allocation management system called ColdFront to provide a single point-of-entry for administration, reporting, and measuring scientific impact of NERC resources for PI.","title":"What is NERC's ColdFront?"},{"location":"get-started/get-an-allocation/#how-to-get-access-to-nercs-coldfront","text":"General Users who are not PIs or Managers on a project see a read-only view of the NERC's ColdFront. Whereas, once a PI Account request is granted, the PI will receive an email confirming the request approval and how to connect NERC\u2019s ColdFront. PIs or Project Owners can use NERC's ColdFront as a self-service web-portal to do the following tasks: Add or manage or archive projects Request allocations that fall under projects to NERC\u2019s resources such as clusters, cloud resources, servers, storage, and software licenses Add/remove user access to/from allocated resources who is a member of the project without requiring system administrator interaction Elevate selected users to 'manager' status, allowing them to handle some of the PI asks such as request new resource allocations, add/remove users to/from resource allocations, add project data such as grants and publications Monitor resource utilization such as storage and cloud usage Receive email notifications for expiring/renewing access to resources as well as notifications when allocations change status - i.e. activated, expired, denied Provide information such as grants, publications, and other reportable data for periodic review by center director to demonstrate need for the resources","title":"How to get access to NERC's ColdFront"},{"location":"get-started/get-an-allocation/#what-pis-need-to-fill-in-order-to-request-a-project","text":"Once logged in to NERC\u2019s ColdFront, PIs can choose Projects sub-menu located under the Project menu. On clicking the \"Add a project\" button will show interface like below: PIs need to specify appropriate title, description of their research work that you will accomplish at NERC in one to two paragraphs and Field of science or research domain and then click the \"Save\" button. Once saved successfully, PIs effectively become the \"manager\" of the project, and are free to add or remove users and also request resource allocation(s) to any Projects for which they are the PI. PIs are permitted to add users to their group, request new allocations, renew expiring allocations, and provide information such as publications and grant data. PIs can maintain all their research information under one project or if they require they can separate the work into multiple projects. Very Important Make sure to select NERC (OpenStack) on Resource option and specify your expected Units of computing. Be mindful, you can extend your current resource allocations on your current project later on.","title":"What PIs need to fill in order to request a Project?"},{"location":"get-started/get-an-allocation/#resource-allocation-quotas","text":"The amount of quota to start out a resource allocation after approval, can be specified using an integer field in the resource allocation request form as shown above. The provided unit value is computed as PIs or project managers request resource quota. The basic unit of computational resources is defined in terms of integer value that corresponds to multiple openstack resource quotas. For example, 1 Unit corresponds to: Resource Name Quota Amount x Unit Instances 1 vCPUs 2 RAM 4096 Volumes 1","title":"Resource Allocation Quotas"},{"location":"get-started/get-an-allocation/#adding-and-removing-user-from-the-project","text":"A user can only view projects they are on. PIs or managers can add or remove users from their respective projects by navigating to the Users section of the project. Once we click on the \"Add Users\" button, it will shows the following search interface: They can search for any users in the system that are not already part of the project by providing exact matched username or partial text of other multiple fields. The search results show details about the user account such as email address, username, first name, last name etc. as shown below: Thus, found user(s) can be selected and assigned directly to the available resource allocation(s) on the given project using this interface. While adding the users, their Role also can be selected from the dropdown options as either User or Manager. Once confirmed with selection of user(s) their roles and allocations, click on the \"Add Selected Users to Project\" button. Removing Users from the Project is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PIs or project owners can select the user(s) and then click on the \"Remove Selected Users From Project\" button.","title":"Adding and removing User from the Project"},{"location":"get-started/get-an-allocation/#user-roles","text":"Access to ColdFront is role based so users see a read-only view of the allocation details for any allocations they are on. PIs see the same allocation details as general users and can also add project users to the allocation if they're not already on it. Even on the first time, PIs add any user to the project as the User role. Later PIs or project owners can upgrade users on their project to the 'manager' role. This allows multiple managers on the same project. This provides the user with the same access and abilities as the PI. The only things a PI can do that a manager can't, is create a new project or archive a project. All other project related tasks that a PI can do, a manager on that project can accomplish as well. General User Accounts are not able to create/update projects and request Resource Allocations. Instead, these accounts must be associated with a Project that has Resources. General User accounts that are associated with a Project have access to view their project details and use all the resources associated with the Project on NERC. General Users (not PIs or Managers) can turn off email notifications at the project level. PIs also have the 'manager' status on a project. Managers can't turn off their notifications. This ensures they continue to get allocation expiration notification emails.","title":"User Roles"},{"location":"get-started/get-an-allocation/#adding-user-to-manager-role","text":"To change a user's role to 'manager' click on the edit icon next to the user's name on the Project Detail page: Then toggle the \"Role\" from User to Manager: Very Important Make sure to click the \"Update\" button to save the change.","title":"Adding User to Manager Role"},{"location":"get-started/get-an-allocation/#pi-and-manager-allocation-view","text":"PIs and managers can view important details of the allocation including start and end dates, creation and last modified dates, users on the allocation and public allocation attributes. PIs and managers can add or remove users from allocations.","title":"PI and Manager Allocation View"},{"location":"get-started/get-an-allocation/#adding-and-removing-project-users-to-project-resource-allocation","text":"Any available users on a given project can be added to resource allocation by clicking on the \"Add Users\" button as shown below: Once Clicked it will show the following interface where PIs can select the available user(s) on the checkboxes and click on the \"Add Selected Users to Allocation\" button. Very Important The desired user must already be on the project to be added to the allocation. Removing Users from the Resource Allocation is straightforward by just clicking on the \"Remove Users\" button. Then it shows the following interface: PIs or project owners can select the user(s) on the checkboxes and then click on the \"Remove Selected Users From Project\" button.","title":"Adding and removing project Users to project Resource Allocation"},{"location":"get-started/get-an-allocation/#adding-a-new-resource-allocation-to-the-project","text":"If one resource allocation is not sufficient for a project, PIs or project owners can request for another allocation(s) by clicking on the \"Request Resource Allocation\" button on the Allocations section of the project details. This will show the page where all existing users for the project will be listed on the bottom of the request form. PIs can select all or only desired user(s) to request the resource allocations to be available on NERC\u2019s OpenStack.","title":"Adding a new Resource Allocation to the project"},{"location":"get-started/get-an-allocation/#general-user-view","text":"General Users who are not PIs or Managers on a project see a read-only view of the allocation details. If a user is on a project but not a particular allocation, they will not be able to see the allocation in the Project view nor will they be able to access the Allocation detail page.","title":"General User View"},{"location":"openstack/","text":"OpenStack Tutorial Index If you're just starting out, we recommend starting from Access the OpenStack Dashboard and going through the tutorial in order. If you just need to review a specific step, you can find the page you need in the list below. Logging In Access the OpenStack Dashboard <<-- Start Here Dashboard Overview Setting Up A Network Set up a Private Network Create a Router Access and Security Security Groups Create a Key Pair Create & Connect to the VM Launch a VM Assign a Floating IP SSH to Cloud VM Persistent Storage Volumes OpenStack CLI OpenStack CLI Launch a VM using OpenStack CLI Python SDK Python SDK Kubernetes Kubernetes Overview i. Kubernetes Development environment Minikube Minishift Kind MicroK8s K3s 5.a. K3s with High Availibility(HA) setup 5.b. Multi-master HA K3s cluster using k3sup 5.c. Single-Node K3s Cluster using k3d 5.d. Multi-master K3s cluster setup using k3d k0s Red Hat Code Ready Containers(CRC) ii. Kubernetes Production environment Kubeadm 1.a. Bootstrapping cluster with kubeadm 1.b. Creating a HA cluster with kubeadm Kubespray Getting Help [Help & FAQs]","title":"OpenStack"},{"location":"openstack/#openstack-tutorial-index","text":"If you're just starting out, we recommend starting from Access the OpenStack Dashboard and going through the tutorial in order. If you just need to review a specific step, you can find the page you need in the list below.","title":"OpenStack Tutorial Index"},{"location":"openstack/#logging-in","text":"Access the OpenStack Dashboard <<-- Start Here Dashboard Overview","title":"Logging In"},{"location":"openstack/#setting-up-a-network","text":"Set up a Private Network Create a Router","title":"Setting Up A Network"},{"location":"openstack/#access-and-security","text":"Security Groups Create a Key Pair","title":"Access and Security"},{"location":"openstack/#create-connect-to-the-vm","text":"Launch a VM Assign a Floating IP SSH to Cloud VM","title":"Create &amp; Connect to the VM"},{"location":"openstack/#persistent-storage","text":"Volumes","title":"Persistent Storage"},{"location":"openstack/#openstack-cli","text":"OpenStack CLI Launch a VM using OpenStack CLI","title":"OpenStack CLI"},{"location":"openstack/#python-sdk","text":"Python SDK","title":"Python SDK"},{"location":"openstack/#kubernetes","text":"Kubernetes Overview","title":"Kubernetes"},{"location":"openstack/#i-kubernetes-development-environment","text":"Minikube Minishift Kind MicroK8s K3s 5.a. K3s with High Availibility(HA) setup 5.b. Multi-master HA K3s cluster using k3sup 5.c. Single-Node K3s Cluster using k3d 5.d. Multi-master K3s cluster setup using k3d k0s Red Hat Code Ready Containers(CRC)","title":"i. Kubernetes Development environment"},{"location":"openstack/#ii-kubernetes-production-environment","text":"Kubeadm 1.a. Bootstrapping cluster with kubeadm 1.b. Creating a HA cluster with kubeadm Kubespray","title":"ii. Kubernetes Production environment"},{"location":"openstack/#getting-help","text":"[Help & FAQs]","title":"Getting Help"},{"location":"openstack/access-and-security/create-a-key-pair/","text":"Create a Key-pair NOTE: If you will be using PuTTY on Windows, please read this first . Add a Key Pair For security, the VM images have password authentication disabled by default, so you will need to use an SSH key pair to log in. You can view key pairs by clicking Project, then click Compute panel and choose Key Pairs from the tabs that appears. This shows the key pairs that are available for this project. Import a Key Pair Prerequisite You need ssh installed in your system You can create a key pair on your local machine, then upload the public key to the cloud. This is the recommended method . Open a terminal and type the following commands (in this example, we have named the key cloud.key, but you can name it anything you want): cd ~/.ssh ssh-keygen -t rsa -f ~/.ssh/cloud.key -C \"label_your_key\" Example: You will be prompted to create a passphrase for the key. IMPORTANT: Do not forget the passphrase! If you do, you will be unable to use your key. This process creates two files in your .ssh folder: cloud.key # private key - don\u2019t share this with anyone, and never upload # it anywhere ever cloud.key.pub # this is your public key *Pro Tip: The -C \"label\" field is not required, but it is useful to quickly identify different public keys later. You could use your email address as the label, or a user@host tag that identifies the computer the key is for. For example, if Bob has both a laptop and a desktop computer that he will, he might use -C \"Bob@laptop\" to label the key he generates on the laptop, and -C \"Bob@desktop\" for the desktop.* On your terminal: pbcopy < ~/.ssh/cloud.key.pub #copies the contents of public key to your clipboard *Pro Tip: If pbcopy isn't working, you can locate the hidden .ssh folder, open the file in your favorite text editor, and copy it to your clipboard. Go back to the Openstack Dashboard, where you should still be on the Key Pairs tab (If not, find it under Project -> Compute -> Key Pairs) Choose \"Import Public Key\". Give the key a name in the \"Key Pair Name\" Box, choose \"SSH Key\" as the Key Type dropdown option and paste the public key that you just copied in the \"Public Key\" text box. Click \"Import Public Key\". You will see your key pair appear in the list. You can now skip ahead to Adding the key to an ssh-agent Create a Key Pair If you are having trouble creating a key pair with the instructions above, the Openstack dashboard can make one for you. Click \"Create a Key Pair\", and enter a name for the key pair. Click on \"Create a Key Pair\" button. You will be prompted to download a .pem file containing your private key. In the example, we have named the key 'cloud_key.pem', but you can name it anything. Save this file to your hard drive, for example in your Downloads folder. Copy this key inside the .ssh folder on your local machine/laptop, using the following steps: cd ~/Downloads # Navigate to the folder where you saved the .pem file mv cloud.pem ~/.ssh/ # This command will copy the key you downloaded to # your .ssh folder. cd ~/.ssh # Navigate to your .ssh folder chmod 400 cloud.pem # Change the permissions of the file To see your public key, navigate to Project -> Compute -> Key Pairs You should see your key in the list. If you click on the name of the newly added key, you will see a screen of information that includes details about your public key: The public key is the part of the key you distribute to VMs and remote servers. You may find it convenient to paste it into a file inside your .ssh folder, so you don't always need to log into the website to see it. Call the file something like cloud_key.pub to distinguish it from your private key. Important: Never share your private key with anyone, or upload it to a server! Adding your SSH key to the ssh-agent If you have many VMs, you will most likely be using one or two VMs with public IPs as a gateway to others which are not reachable from the internet. In order to be able to use your key for multiple SSH hops, do NOT copy your private key to the gateway VM! The correct method to use Agent Forwarding, which adds the key to an ssh-agent on your local machine and 'forwards' it over the SSH connection. If ssh-agent is not already running in background, you need to start the ssh-agent in the background. eval \"$(ssh-agent -s)\" > Agent pid 59566 Then, add the key to your ssh agent: cd ~/.ssh ssh-add cloud.key Identity added: cloud.key (test_user@laptop) Check that it is added with the command ssh-add -l 2048 SHA256:D0DLuODzs15j2OaZnA8I52aEeY3exRT2PCsUyAXgI24 test_user@laptop (RSA) Depending on your system, you might have to repeat these steps after you reboot or log out of your computer. You can always check if your ssh key is added by running the ssh-add -l command. A key with the default name id_rsa will be added by default at login, although you will still need to unlock it with your passphrase the first time you use it. Once the key is added, you will be able to forward it over an SSH connection, like this: ssh -A -i cloud.key <username>@<remote-host-IP> Connecting via SSH is discussed in more detail later in the tutorial ( SSH to Cloud VM ); for now, just proceed to the next step below. SSH keys with PuTTY on Windows PuTTY requires SSH keys to be in its own ppk format. To convert between OpenSSH keys used by OpenStack and PuTTY's format, you need a utility called PuTTYgen. If it was not installed when you originally installed PuTTY, you can get it here: Download PuTTY . You have 2 options for generating keys that will work with PuTTY: Generate an OpenSSH key with ssh-keygen or from the Horizon GUI using the instructions above, then use PuTTYgen to convert the private key to .ppk Generate a .ppk key with PuTTYgen, and import the provided OpenSSH public key to OpenStack using the 'Import a Key Pair' instructions above . There is a detailed walkthrough of how to use PuTTYgen here: Use SSH Keys with PuTTY on Windows","title":"Create a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#create-a-key-pair","text":"NOTE: If you will be using PuTTY on Windows, please read this first .","title":"Create a Key-pair"},{"location":"openstack/access-and-security/create-a-key-pair/#add-a-key-pair","text":"For security, the VM images have password authentication disabled by default, so you will need to use an SSH key pair to log in. You can view key pairs by clicking Project, then click Compute panel and choose Key Pairs from the tabs that appears. This shows the key pairs that are available for this project.","title":"Add a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#import-a-key-pair","text":"Prerequisite You need ssh installed in your system You can create a key pair on your local machine, then upload the public key to the cloud. This is the recommended method . Open a terminal and type the following commands (in this example, we have named the key cloud.key, but you can name it anything you want): cd ~/.ssh ssh-keygen -t rsa -f ~/.ssh/cloud.key -C \"label_your_key\" Example: You will be prompted to create a passphrase for the key. IMPORTANT: Do not forget the passphrase! If you do, you will be unable to use your key. This process creates two files in your .ssh folder: cloud.key # private key - don\u2019t share this with anyone, and never upload # it anywhere ever cloud.key.pub # this is your public key *Pro Tip: The -C \"label\" field is not required, but it is useful to quickly identify different public keys later. You could use your email address as the label, or a user@host tag that identifies the computer the key is for. For example, if Bob has both a laptop and a desktop computer that he will, he might use -C \"Bob@laptop\" to label the key he generates on the laptop, and -C \"Bob@desktop\" for the desktop.* On your terminal: pbcopy < ~/.ssh/cloud.key.pub #copies the contents of public key to your clipboard *Pro Tip: If pbcopy isn't working, you can locate the hidden .ssh folder, open the file in your favorite text editor, and copy it to your clipboard. Go back to the Openstack Dashboard, where you should still be on the Key Pairs tab (If not, find it under Project -> Compute -> Key Pairs) Choose \"Import Public Key\". Give the key a name in the \"Key Pair Name\" Box, choose \"SSH Key\" as the Key Type dropdown option and paste the public key that you just copied in the \"Public Key\" text box. Click \"Import Public Key\". You will see your key pair appear in the list. You can now skip ahead to Adding the key to an ssh-agent","title":"Import a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#create-a-key-pair_1","text":"If you are having trouble creating a key pair with the instructions above, the Openstack dashboard can make one for you. Click \"Create a Key Pair\", and enter a name for the key pair. Click on \"Create a Key Pair\" button. You will be prompted to download a .pem file containing your private key. In the example, we have named the key 'cloud_key.pem', but you can name it anything. Save this file to your hard drive, for example in your Downloads folder. Copy this key inside the .ssh folder on your local machine/laptop, using the following steps: cd ~/Downloads # Navigate to the folder where you saved the .pem file mv cloud.pem ~/.ssh/ # This command will copy the key you downloaded to # your .ssh folder. cd ~/.ssh # Navigate to your .ssh folder chmod 400 cloud.pem # Change the permissions of the file To see your public key, navigate to Project -> Compute -> Key Pairs You should see your key in the list. If you click on the name of the newly added key, you will see a screen of information that includes details about your public key: The public key is the part of the key you distribute to VMs and remote servers. You may find it convenient to paste it into a file inside your .ssh folder, so you don't always need to log into the website to see it. Call the file something like cloud_key.pub to distinguish it from your private key. Important: Never share your private key with anyone, or upload it to a server!","title":"Create a Key Pair"},{"location":"openstack/access-and-security/create-a-key-pair/#adding-your-ssh-key-to-the-ssh-agent","text":"If you have many VMs, you will most likely be using one or two VMs with public IPs as a gateway to others which are not reachable from the internet. In order to be able to use your key for multiple SSH hops, do NOT copy your private key to the gateway VM! The correct method to use Agent Forwarding, which adds the key to an ssh-agent on your local machine and 'forwards' it over the SSH connection. If ssh-agent is not already running in background, you need to start the ssh-agent in the background. eval \"$(ssh-agent -s)\" > Agent pid 59566 Then, add the key to your ssh agent: cd ~/.ssh ssh-add cloud.key Identity added: cloud.key (test_user@laptop) Check that it is added with the command ssh-add -l 2048 SHA256:D0DLuODzs15j2OaZnA8I52aEeY3exRT2PCsUyAXgI24 test_user@laptop (RSA) Depending on your system, you might have to repeat these steps after you reboot or log out of your computer. You can always check if your ssh key is added by running the ssh-add -l command. A key with the default name id_rsa will be added by default at login, although you will still need to unlock it with your passphrase the first time you use it. Once the key is added, you will be able to forward it over an SSH connection, like this: ssh -A -i cloud.key <username>@<remote-host-IP> Connecting via SSH is discussed in more detail later in the tutorial ( SSH to Cloud VM ); for now, just proceed to the next step below.","title":"Adding your SSH key to the ssh-agent"},{"location":"openstack/access-and-security/create-a-key-pair/#ssh-keys-with-putty-on-windows","text":"PuTTY requires SSH keys to be in its own ppk format. To convert between OpenSSH keys used by OpenStack and PuTTY's format, you need a utility called PuTTYgen. If it was not installed when you originally installed PuTTY, you can get it here: Download PuTTY . You have 2 options for generating keys that will work with PuTTY: Generate an OpenSSH key with ssh-keygen or from the Horizon GUI using the instructions above, then use PuTTYgen to convert the private key to .ppk Generate a .ppk key with PuTTYgen, and import the provided OpenSSH public key to OpenStack using the 'Import a Key Pair' instructions above . There is a detailed walkthrough of how to use PuTTYgen here: Use SSH Keys with PuTTY on Windows","title":"SSH keys with PuTTY on Windows"},{"location":"openstack/access-and-security/security-groups/","text":"Security Groups Before you launch an instance, you should add security group rules to enable users to ping and use SSH to connect to the instance. Security groups are sets of IP filter rules that define networking access and are applied to all instances within a project. To do so, you either add rules to the default security group Add a rule to the default security group or add a new security group with rules. You can view security groups by clicking Project, then click Network panel and choose Security Groups from the tabs that appears. You should see a \u2018default\u2019 security group. The default security group allows traffic only between members of the security group, so by default you can always connect between VMs in this group. However, it blocks all traffic from outside, including incoming SSH connections. In order to access instances via a public IP, an additional security group is needed. Security groups are very highly configurable, so you can create different security groups for different types of VMs used in your project. For example, for a VM that hosts a web page, you need a security group which allows access to ports 80 and 443. You can also limit access based on where the traffic originates, using either IP addresses or security groups to define the allowed sources. Create a new Security Group Click on \"Create Security Group\" Give your new group a name, and a brief description. You will see some existing rules: Let's create the new rule to allow SSH. Click on \"Add Rule\". You will see there are a lot of options you can configure on the Add Rule dialog box. Enter the following values: Rule: SSH Remote: CIDR CIDR: 0.0.0.0/0 Note To accept requests from a particular range of IP addresses, specify the IP address block in the CIDR box. The new rule now appears in the list. This signifies that any instances using this newly added Security Group will now have SSH port 22 open for requests from any IP address. Allowing Ping The default configuration blocks ping responses, so you will need to add an additional group and/or rule if you want your public IPs to respond to ping requests. Ping is ICMP traffic, so the easiest way to allow it is to add a new rule and choose \"ALL ICMP\" from the dropdown. In the Add Rule dialog box, enter the following values: Rule: All ICMP Direction: Ingress Remote: CIDR CIDR: 0.0.0.0/0 Instances will now accept all incoming ICMP packets.","title":"Security Groups"},{"location":"openstack/access-and-security/security-groups/#security-groups","text":"Before you launch an instance, you should add security group rules to enable users to ping and use SSH to connect to the instance. Security groups are sets of IP filter rules that define networking access and are applied to all instances within a project. To do so, you either add rules to the default security group Add a rule to the default security group or add a new security group with rules. You can view security groups by clicking Project, then click Network panel and choose Security Groups from the tabs that appears. You should see a \u2018default\u2019 security group. The default security group allows traffic only between members of the security group, so by default you can always connect between VMs in this group. However, it blocks all traffic from outside, including incoming SSH connections. In order to access instances via a public IP, an additional security group is needed. Security groups are very highly configurable, so you can create different security groups for different types of VMs used in your project. For example, for a VM that hosts a web page, you need a security group which allows access to ports 80 and 443. You can also limit access based on where the traffic originates, using either IP addresses or security groups to define the allowed sources.","title":"Security Groups"},{"location":"openstack/access-and-security/security-groups/#create-a-new-security-group","text":"Click on \"Create Security Group\" Give your new group a name, and a brief description. You will see some existing rules: Let's create the new rule to allow SSH. Click on \"Add Rule\". You will see there are a lot of options you can configure on the Add Rule dialog box. Enter the following values: Rule: SSH Remote: CIDR CIDR: 0.0.0.0/0 Note To accept requests from a particular range of IP addresses, specify the IP address block in the CIDR box. The new rule now appears in the list. This signifies that any instances using this newly added Security Group will now have SSH port 22 open for requests from any IP address.","title":"Create a new Security Group"},{"location":"openstack/access-and-security/security-groups/#allowing-ping","text":"The default configuration blocks ping responses, so you will need to add an additional group and/or rule if you want your public IPs to respond to ping requests. Ping is ICMP traffic, so the easiest way to allow it is to add a new rule and choose \"ALL ICMP\" from the dropdown. In the Add Rule dialog box, enter the following values: Rule: All ICMP Direction: Ingress Remote: CIDR CIDR: 0.0.0.0/0 Instances will now accept all incoming ICMP packets.","title":"Allowing Ping"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/","text":"Assign a Floating IP When an instance is created in OpenStack, it is automatically assigned a fixed IP address in the network to which the instance is assigned. This IP address is permanently associated with the instance until the instance is terminated. However, in addition to the fixed IP address, a floating IP address can also be attached to an instance. Unlike fixed IP addresses, floating IP addresses can have their associations modified at any time, regardless of the state of the instances involved. Floating IPs are a limited resource, so your project will have a quota based on its needs. You should only assign public IPs to VMs that need them. This procedure details the reservation of a floating IP address from an existing pool of addresses and the association of that address with a specific instance. By attaching a Floating IP to your instance, you can ssh into your vm from your local machine. Make sure you are using key forwarding as described in Create a Key Pair Allocate a Floating IP Navigate to Project -> Compute -> Instances Next to Instance Name -> Click Actions dropdown arrow (far right) -> Choose Associate Floating IP If you have some floating IPs already allocated to your project which are not yet associated with a VM, they will be available in the dropdown list on this screen. If you have no floating IPs allocated, or all your allocated IPs are in use already, the dropdown list will be empty. Click the + symbol to allocate an IP. You will see the following screen. Make sure 'provider' appears in the dropdown menu, and that you have not already met your quota of allocated IPs. In this example, the project has a quota of 50 floating IPs, but we have allocated 5 so far, so we can still allocate up to 45 IPs. Click \"Allocate IP\". You will get a green \"success\" popup in the top left that shows your public IP address. You will get a red error message instead if you attempt to exceed your project's floating IP quota. If you have not tried to exceed your quota, but you get a red error message anyway, please contact [TODO:contact_mail] for help. NOw click on \"Associate\" button. Then, a green \"success\" popup in the top left and you can see the floating IP is attached to your VM on the Instances page: Disassociate a Floating IP You may need to disassociate a Floating IP from an instance which no longer needs it, so you can assign it to one that does. Navigate to Project -> Compute -> Instances Find the instance you want to remove the IP from in the list. Click the red \"Disassociate Floating IP\" to the right. This IP will be disassociated from the instance, but it will still remain allocated to your project. Release a Floating IP You may discover that your project does not need all the floating IPs that are allocated to it. We can release a Floating IP while disassociating it just we need to check the \"Release Floating IP\" option as shown here: OR, Navigate to Project -> Network -> Floating IPs To release the floating IP address back into the floating IP pool, click the Release Floating IP option in the Actions column.","title":"Assign a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#assign-a-floating-ip","text":"When an instance is created in OpenStack, it is automatically assigned a fixed IP address in the network to which the instance is assigned. This IP address is permanently associated with the instance until the instance is terminated. However, in addition to the fixed IP address, a floating IP address can also be attached to an instance. Unlike fixed IP addresses, floating IP addresses can have their associations modified at any time, regardless of the state of the instances involved. Floating IPs are a limited resource, so your project will have a quota based on its needs. You should only assign public IPs to VMs that need them. This procedure details the reservation of a floating IP address from an existing pool of addresses and the association of that address with a specific instance. By attaching a Floating IP to your instance, you can ssh into your vm from your local machine. Make sure you are using key forwarding as described in Create a Key Pair","title":"Assign a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#allocate-a-floating-ip","text":"Navigate to Project -> Compute -> Instances Next to Instance Name -> Click Actions dropdown arrow (far right) -> Choose Associate Floating IP If you have some floating IPs already allocated to your project which are not yet associated with a VM, they will be available in the dropdown list on this screen. If you have no floating IPs allocated, or all your allocated IPs are in use already, the dropdown list will be empty. Click the + symbol to allocate an IP. You will see the following screen. Make sure 'provider' appears in the dropdown menu, and that you have not already met your quota of allocated IPs. In this example, the project has a quota of 50 floating IPs, but we have allocated 5 so far, so we can still allocate up to 45 IPs. Click \"Allocate IP\". You will get a green \"success\" popup in the top left that shows your public IP address. You will get a red error message instead if you attempt to exceed your project's floating IP quota. If you have not tried to exceed your quota, but you get a red error message anyway, please contact [TODO:contact_mail] for help. NOw click on \"Associate\" button. Then, a green \"success\" popup in the top left and you can see the floating IP is attached to your VM on the Instances page:","title":"Allocate a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#disassociate-a-floating-ip","text":"You may need to disassociate a Floating IP from an instance which no longer needs it, so you can assign it to one that does. Navigate to Project -> Compute -> Instances Find the instance you want to remove the IP from in the list. Click the red \"Disassociate Floating IP\" to the right. This IP will be disassociated from the instance, but it will still remain allocated to your project.","title":"Disassociate a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/assign-a-floating-IP/#release-a-floating-ip","text":"You may discover that your project does not need all the floating IPs that are allocated to it. We can release a Floating IP while disassociating it just we need to check the \"Release Floating IP\" option as shown here: OR, Navigate to Project -> Network -> Floating IPs To release the floating IP address back into the floating IP pool, click the Release Floating IP option in the Actions column.","title":"Release a Floating IP"},{"location":"openstack/create-and-connect-to-the-VM/flavors/","text":"Nova flavors In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server. Currently, our setup supports the following flavors Field Value disk 10 name m1.tiny ram 1024 vcpus 1 disk 10 name m1.small ram 2048 vcpus 1 disk 10 name m1.medium ram 4096 vcpus 2 disk 10 name m1.large ram 8192 vcpus 4 disk 10 name m1.xlarge ram 16384 vcpus 8 disk 10 name c1.xlarge ram 46080 vcpus 10 disk 10 name c1.2xlarge ram 92160 vcpus 20 disk 10 name c1.4xlarge ram 184320 vcpus 40 disk 10 name gpu.A100 ram 96256 vcpus 12 disk 10 name custom.4c.16g ram 16384 vcpus 4 disk 10 name custom.4c.32g ram 32768 vcpus 4 disk 10 name custom.8c.32g ram 32768 vcpus 8 disk 10 name custom.8c.64g ram 65536 vcpus 8 disk 25 name m1.s2.tiny ram 1024 vcpus 1 disk 25 name m1.s2.small ram 2048 vcpus 1 disk 25 name m1.s2.medium ram 4096 vcpus 2 disk 25 name m1.s2.large ram 8192 vcpus 4 disk 25 name m1.s2.xlarge ram 16384 vcpus 8 disk 25 name c1.s2.xlarge ram 46080 vcpus 10 disk 25 name c1.s2.2xlarge ram 92160 vcpus 20 disk 25 name c1.s2.4xlarge ram 184320 vcpus 40 disk 25 name c2.s2.xlarge ram 32768 vcpus 16 disk 25 name c2.s2.2xlarge ram 65536 vcpus 32 disk 25 name c2.s2.4xlarge ram 81920 vcpus 40 Each flavor includes enforced quotas for disk limits through maximum disk read, write, and total bytes per second and disk limits through maximum disk read, write, and total I/O operations per second. They also include enforced network bandwidth limits through inbound and outbound average. So, while launching a VM choose the flavor for your instance that fits your requirements and use-cases.","title":"Available NOVA Flavors"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#nova-flavors","text":"In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server.","title":"Nova flavors"},{"location":"openstack/create-and-connect-to-the-VM/flavors/#currently-our-setup-supports-the-following-flavors","text":"Field Value disk 10 name m1.tiny ram 1024 vcpus 1 disk 10 name m1.small ram 2048 vcpus 1 disk 10 name m1.medium ram 4096 vcpus 2 disk 10 name m1.large ram 8192 vcpus 4 disk 10 name m1.xlarge ram 16384 vcpus 8 disk 10 name c1.xlarge ram 46080 vcpus 10 disk 10 name c1.2xlarge ram 92160 vcpus 20 disk 10 name c1.4xlarge ram 184320 vcpus 40 disk 10 name gpu.A100 ram 96256 vcpus 12 disk 10 name custom.4c.16g ram 16384 vcpus 4 disk 10 name custom.4c.32g ram 32768 vcpus 4 disk 10 name custom.8c.32g ram 32768 vcpus 8 disk 10 name custom.8c.64g ram 65536 vcpus 8 disk 25 name m1.s2.tiny ram 1024 vcpus 1 disk 25 name m1.s2.small ram 2048 vcpus 1 disk 25 name m1.s2.medium ram 4096 vcpus 2 disk 25 name m1.s2.large ram 8192 vcpus 4 disk 25 name m1.s2.xlarge ram 16384 vcpus 8 disk 25 name c1.s2.xlarge ram 46080 vcpus 10 disk 25 name c1.s2.2xlarge ram 92160 vcpus 20 disk 25 name c1.s2.4xlarge ram 184320 vcpus 40 disk 25 name c2.s2.xlarge ram 32768 vcpus 16 disk 25 name c2.s2.2xlarge ram 65536 vcpus 32 disk 25 name c2.s2.4xlarge ram 81920 vcpus 40 Each flavor includes enforced quotas for disk limits through maximum disk read, write, and total bytes per second and disk limits through maximum disk read, write, and total I/O operations per second. They also include enforced network bandwidth limits through inbound and outbound average. So, while launching a VM choose the flavor for your instance that fits your requirements and use-cases.","title":"Currently, our setup supports the following flavors"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/","text":"Launch a VM Background To start a VM, we will need a base image. NERC has made several Public images available to users. Launch an Instance Navigate: Project -> Compute -> Images. Click Launch Instance next to the public image of your choice. In the example, we chose ubuntu-21.04-x86_64 . *Important: There are multiple tabs along the top of the the pop up window. Make sure you review all of them as per instructions before clicking on Launch! Otherwise, your launched VM may be inaccessible.* In the Launch Instance dialog box, specify the following values: Details tab Instance Name: Give your instance a name that assign a name to the virtual machine. NOTE: The name you assign here becomes the initial host name of the server. If the name is longer than 63 characters, the Compute service truncates it automatically to ensure dnsmasq works correctly. Availability Zone: By default, this value is set to the availability zone given by the cloud provider i.e. nova . Count: To launch multiple instances, enter a value greater than 1. The default is 1. Source tab: Double check that in the dropdown \"Select Boot Source,\" \"Image\" is selected. Note To create an image that uses the boot volume sized according to the flavor ensure that \"No\" is selected under the \"Create New Volume\" section. When you deploy a non-ephemeral instance (i.e. Creating a new volume), and indicate \"Yes\" in \"Delete Volume on Instance delete\", then when you delete the instance, the volume is also removed. This is not desired while the data of its attached volumes need to persist even instance is deleted. But this incures the Volumes quotas so ideally you can select \"Yes\" only for those instances you will not be storing persistent data. Flavor tab: Specify the size of the instance to launch. Choose m1.large from the 'Flavor' tab by clicking on the \"+\" icon. Note In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server. The default flavor is m1. small which is too small for the available base images so please do not use it. More details about available flavors and corresponding quotas details can be found here . After choosing m1.large , you should see it moved up to \"Allocated\". Networks: tab: Make sure the Private Network you just created is moved up to \"Allocated\". If not, you can click on the \"+\" icon in \"Available\". Security Groups: tab: Make sure to add the security group where you enabled SSH. Key Pair: Add the key pair you created for your local machine/laptop to use with this VM. NOTE: If you did not provide a key pair, security groups, or rules, users can access the instance only from inside the cloud through VNC. Even pinging the instance is not possible without an ICMP rule configured. Network Ports, Configuration, Server Groups, Schedular Hints, and Metadata: tab: Ignore these tabs for now. You are now ready to launch your VM - go ahead and click \"Launch Instance\"! This will initiate a instance on a compute node in the cloud. On a successful launch you would be redirected to Compute -> Instances tab and can see the VM spawning. Once your VM is successfully running you will see the Power State changes from \"No State\" to \"running\". Note You can also launch an instance from the \"Instances\" or \"Volumes\" category when you launch an instance from an instance or a volume respectively.","title":"Launch a VM"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#launch-a-vm","text":"","title":"Launch a VM"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#background","text":"To start a VM, we will need a base image. NERC has made several Public images available to users.","title":"Background"},{"location":"openstack/create-and-connect-to-the-VM/launch-a-VM/#launch-an-instance","text":"Navigate: Project -> Compute -> Images. Click Launch Instance next to the public image of your choice. In the example, we chose ubuntu-21.04-x86_64 . *Important: There are multiple tabs along the top of the the pop up window. Make sure you review all of them as per instructions before clicking on Launch! Otherwise, your launched VM may be inaccessible.* In the Launch Instance dialog box, specify the following values: Details tab Instance Name: Give your instance a name that assign a name to the virtual machine. NOTE: The name you assign here becomes the initial host name of the server. If the name is longer than 63 characters, the Compute service truncates it automatically to ensure dnsmasq works correctly. Availability Zone: By default, this value is set to the availability zone given by the cloud provider i.e. nova . Count: To launch multiple instances, enter a value greater than 1. The default is 1. Source tab: Double check that in the dropdown \"Select Boot Source,\" \"Image\" is selected. Note To create an image that uses the boot volume sized according to the flavor ensure that \"No\" is selected under the \"Create New Volume\" section. When you deploy a non-ephemeral instance (i.e. Creating a new volume), and indicate \"Yes\" in \"Delete Volume on Instance delete\", then when you delete the instance, the volume is also removed. This is not desired while the data of its attached volumes need to persist even instance is deleted. But this incures the Volumes quotas so ideally you can select \"Yes\" only for those instances you will not be storing persistent data. Flavor tab: Specify the size of the instance to launch. Choose m1.large from the 'Flavor' tab by clicking on the \"+\" icon. Note In NERC OpenStack, flavors define the compute, memory, and storage capacity of nova computing instances. In other words, a flavor is an available hardware configuration for a server. The default flavor is m1. small which is too small for the available base images so please do not use it. More details about available flavors and corresponding quotas details can be found here . After choosing m1.large , you should see it moved up to \"Allocated\". Networks: tab: Make sure the Private Network you just created is moved up to \"Allocated\". If not, you can click on the \"+\" icon in \"Available\". Security Groups: tab: Make sure to add the security group where you enabled SSH. Key Pair: Add the key pair you created for your local machine/laptop to use with this VM. NOTE: If you did not provide a key pair, security groups, or rules, users can access the instance only from inside the cloud through VNC. Even pinging the instance is not possible without an ICMP rule configured. Network Ports, Configuration, Server Groups, Schedular Hints, and Metadata: tab: Ignore these tabs for now. You are now ready to launch your VM - go ahead and click \"Launch Instance\"! This will initiate a instance on a compute node in the cloud. On a successful launch you would be redirected to Compute -> Instances tab and can see the VM spawning. Once your VM is successfully running you will see the Power State changes from \"No State\" to \"running\". Note You can also launch an instance from the \"Instances\" or \"Volumes\" category when you launch an instance from an instance or a volume respectively.","title":"Launch an Instance"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/","text":"SSH to Cloud VM Shell , or SSH , is used to administering and managing Linux workloads. Before trying to access instances from the outside world, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Assign a Floating IP to the instance in order to access it from outside world. Make sure you have added rules in the Security Groups to allow ssh to the instance. Make a note of the floating IP you have associated to your instance. In our example, the IP is 140.247.152.229 . Default usernames for all the base images are: all Ubuntu images : ubuntu all CentOS images : centos all RHEL images : ubuntu all Fedora images : fedora Our example VM was launched with the ubuntu-21.04-x86_64 base image, the user we need is 'ubuntu'. Open a Terminal window and type: ssh ubuntu@140.247.152.229 Since you have never connected to this VM before, you will be asked if you are sure you want to connect. Type yes . Note If you haven't added your key to ssh-agent, you may need to specify the private key file, like this: sh ssh -i ~/.ssh/cloud.key ubuntu@140.247.152.229 Setting a password When the VMs are launched, a strong, randomly-generated password is created for the default user, and then discarded. Once you connect to your VM, you will want to set a password in case you ever need to log in via the console in the web dashboard. For example, if your network connections aren't working right. Since you are not using it to log in over SSH or to sudo, it doesn't really matter how hard it is to type, and we recommend using a randomly-generated password. Create a random password like this: ubuntu@test-vm:~$ cat /dev/urandom | base64 | dd count=14 bs=1 T1W16HCyfZf8V514+0 records in 14+0 records out 14 bytes copied, 0.00110367 s, 12.7 kB/s The 'count' parameter controls the number of characters. The first [count] characters of the output are your randomly generated output, followed immediately by \"[count]+0\", so in the above example the password is: T1W16HCyfZf8V5 . Set the password for ubuntu using the command: ubuntu@test-vm:~$ sudo passwd ubuntu New password: Retype new password: ... password updated successfully Store the password in a secure place. Don't send it over email, post it on your wall on a sticky note, etc. Adding other people's SSH keys to the instance You were able to log into using your own SSH key. Right now Openstack only permits one key to be added at launch, so you need to add your teammates keys manually. Get your teammates' public keys. If they used ssh-keygen to create their key, this will be in a file called .pub on their machine. If they created a key via the dashboard, or imported the key created with ssh-keygen, their public key is viewable from the Key Pairs tab. Click on the key pair name. The public key starts with 'ssh-rsa' and looks something like this: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDL6O5qNZHfgFwf4vnnib2XBub7ZU6khy6z6JQl3XRJg6I6gZ +Ss6tNjz0Xgax5My0bizORcka/TJ33S36XZfzUKGsZqyEl/ax1Xnl3MfE/rgq415wKljg4 +QvDznF0OFqXjDIgL938N8G4mq/ cKKtRSMdksAvNsAreO0W7GZi24G1giap4yuG4XghAXcYxDnOSzpyP2HgqgjsPdQue919IYvgH8shr +sPa48uC5sGU5PkTb0Pk/ef1Y5pLBQZYchyMakQvxjj7hHZaT/ Lw0wIvGpPQay84plkjR2IDNb51tiEy5x163YDtrrP7RM2LJwXm+1vI8MzYmFRrXiqUyznd test_user@demo Create a file called something like 'teammates.txt' and paste in your team's public keys, one per line. Hang onto this file to save yourself from having to do all the copy/pasting every time you launch a new VM. Copy the file to the vm: [you@your-laptop ~]$ scp teammates.txt ubuntu@140.247.152.229:~ If the copy works, you will see the output: teammates.txt 100% 0 0KB/s 00:00 Append the file's contents to authorized_keys: [cloud-user@test-vm ~] #cat teammates.txt >> ~/.ssh/authorized_keys Now your teammates should also be able to log in. Make sure to use >> instead of > to avoid overwriting your own key. Adding users to the instance You may decide that each teammate should have their own user on the VM instead of everyone logging in to the default user. Once you log into the VM, you can create another user like this. Note The 'sudo_group' is different for different OS - in CentOS and Red Hat, the group is called 'wheel', while in Ubuntu, the group is called 'sudo'. $ sudo su # useradd -m <username> # passwd <username> # usermod -aG <sudo_group> <username> <-- skip this step for users who # should not have root access # su username $ cd ~ $ mkdir .ssh $ chmod 700 .ssh $ cd .ssh $ vi authorized_keys <-- paste the public key for that user in this file $ chmod 600 authorized_keys","title":"SSH to Cloud VM"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#ssh-to-cloud-vm","text":"Shell , or SSH , is used to administering and managing Linux workloads. Before trying to access instances from the outside world, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Assign a Floating IP to the instance in order to access it from outside world. Make sure you have added rules in the Security Groups to allow ssh to the instance. Make a note of the floating IP you have associated to your instance. In our example, the IP is 140.247.152.229 . Default usernames for all the base images are: all Ubuntu images : ubuntu all CentOS images : centos all RHEL images : ubuntu all Fedora images : fedora Our example VM was launched with the ubuntu-21.04-x86_64 base image, the user we need is 'ubuntu'. Open a Terminal window and type: ssh ubuntu@140.247.152.229 Since you have never connected to this VM before, you will be asked if you are sure you want to connect. Type yes . Note If you haven't added your key to ssh-agent, you may need to specify the private key file, like this: sh ssh -i ~/.ssh/cloud.key ubuntu@140.247.152.229","title":"SSH to Cloud VM"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#setting-a-password","text":"When the VMs are launched, a strong, randomly-generated password is created for the default user, and then discarded. Once you connect to your VM, you will want to set a password in case you ever need to log in via the console in the web dashboard. For example, if your network connections aren't working right. Since you are not using it to log in over SSH or to sudo, it doesn't really matter how hard it is to type, and we recommend using a randomly-generated password. Create a random password like this: ubuntu@test-vm:~$ cat /dev/urandom | base64 | dd count=14 bs=1 T1W16HCyfZf8V514+0 records in 14+0 records out 14 bytes copied, 0.00110367 s, 12.7 kB/s The 'count' parameter controls the number of characters. The first [count] characters of the output are your randomly generated output, followed immediately by \"[count]+0\", so in the above example the password is: T1W16HCyfZf8V5 . Set the password for ubuntu using the command: ubuntu@test-vm:~$ sudo passwd ubuntu New password: Retype new password: ... password updated successfully Store the password in a secure place. Don't send it over email, post it on your wall on a sticky note, etc.","title":"Setting a password"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#adding-other-peoples-ssh-keys-to-the-instance","text":"You were able to log into using your own SSH key. Right now Openstack only permits one key to be added at launch, so you need to add your teammates keys manually. Get your teammates' public keys. If they used ssh-keygen to create their key, this will be in a file called .pub on their machine. If they created a key via the dashboard, or imported the key created with ssh-keygen, their public key is viewable from the Key Pairs tab. Click on the key pair name. The public key starts with 'ssh-rsa' and looks something like this: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDL6O5qNZHfgFwf4vnnib2XBub7ZU6khy6z6JQl3XRJg6I6gZ +Ss6tNjz0Xgax5My0bizORcka/TJ33S36XZfzUKGsZqyEl/ax1Xnl3MfE/rgq415wKljg4 +QvDznF0OFqXjDIgL938N8G4mq/ cKKtRSMdksAvNsAreO0W7GZi24G1giap4yuG4XghAXcYxDnOSzpyP2HgqgjsPdQue919IYvgH8shr +sPa48uC5sGU5PkTb0Pk/ef1Y5pLBQZYchyMakQvxjj7hHZaT/ Lw0wIvGpPQay84plkjR2IDNb51tiEy5x163YDtrrP7RM2LJwXm+1vI8MzYmFRrXiqUyznd test_user@demo Create a file called something like 'teammates.txt' and paste in your team's public keys, one per line. Hang onto this file to save yourself from having to do all the copy/pasting every time you launch a new VM. Copy the file to the vm: [you@your-laptop ~]$ scp teammates.txt ubuntu@140.247.152.229:~ If the copy works, you will see the output: teammates.txt 100% 0 0KB/s 00:00 Append the file's contents to authorized_keys: [cloud-user@test-vm ~] #cat teammates.txt >> ~/.ssh/authorized_keys Now your teammates should also be able to log in. Make sure to use >> instead of > to avoid overwriting your own key.","title":"Adding other people's SSH keys to the instance"},{"location":"openstack/create-and-connect-to-the-VM/ssh-to-cloud-VM/#adding-users-to-the-instance","text":"You may decide that each teammate should have their own user on the VM instead of everyone logging in to the default user. Once you log into the VM, you can create another user like this. Note The 'sudo_group' is different for different OS - in CentOS and Red Hat, the group is called 'wheel', while in Ubuntu, the group is called 'sudo'. $ sudo su # useradd -m <username> # passwd <username> # usermod -aG <sudo_group> <username> <-- skip this step for users who # should not have root access # su username $ cd ~ $ mkdir .ssh $ chmod 700 .ssh $ cd .ssh $ vi authorized_keys <-- paste the public key for that user in this file $ chmod 600 authorized_keys","title":"Adding users to the instance"},{"location":"openstack/create-and-connect-to-the-VM/bastion-host-based-ssh/","text":"Bastion Host A bastion host is a server that provides secure access to private networks over SSH from an external network, such as the Internet. We can leverage a bastion host to record all SSH sessions established with private network instances which enables auditing and can help us in efforts to comply with regulatory requirements. The following diagram illustrates the concept of using an SSH bastion host to provide access to Linux instances running inside OpenStack cloud network. In OpenStack, users can deploy instances in a private tenant network. In order to make these instances to be accessible externally via internet, the tenant must assign each instance a floating IP address i.e., an external public IP. Nevertheless, users may still want a way to deploy instances without having to assign a floating IP address for every instance. This is useful in the context of an OpenStack project as you don't necessarily want to reserve a floating IP for all your instances. This way you can isolate certain resources so that there is only a single point of access to them and conserve floating IP addresses so that you don't need as big of a quota. Leveraging an SSH bastion host allows this sort of configuration while still enabling SSH access to the private instances. Before trying to access instances from the outside world using SSH tunneling via Bastion Host, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. You can use the same key for both the bastion host and the remote instances, or different keys; you'll just need to ensure that the keys are loaded by ssh-agent appropriately so they can be used as needed. Please read this instruction on how to add ssh-agent and load your private key using ssh-add command to access the bastion host. Verify you have an SSH agent running. This should match whatever you built your cluster with. ssh-add -l If you need to add the key to your agent: ssh-add path/to/private/key Now you can SSH into the bastion host: ssh -A <user>@<bastion-floating-IP> Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Add two Security Groups, one will be used by the Bastion host and another one will be used by any private instances. i. Bastion Host Security Group: Allow inbound SSH (optional ICMP) for this security group. Make sure you have added rules in the Security Groups to allow ssh to the bastion host. ii. Private Instances Security Group: You need to select \"Security Group\" in Remote dropdown option, and then select the \" Bastion Host Security Group \" under Security Group option as shown below: Assign a Floating IP to the Bastion host instance in order to access it from outside world. Make a note of the floating IP you have associated to your instance. While adding the Bastion host and private instance, please select appropriate Security Group as shown below: private1: bastion_host_demo: Finally, you'll want to configure the ProxyJump setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). In SSH configuration file, we can define multiple hosts by pet names, specify custom ports, hostnames, users, etc. For example, let's say that you had a remote instance named \" private1 \" and you wanted to run SSH connections through a bastion host called \" bastion \". The appropriate SSH configuration file might look something like this: Host bastion HostName 140.247.152.139 User ubuntu Host private1 Hostname 192.168.0.40 User ubuntu ProxyJump bastion ProxyJump makes it super simple to jump from one host to another totally transparently. OR, if you don't have keys loaded by ssh-add command starting ssh-agent on your local machine. you can load the private key using IdentityFile variable in SSH configuration file as shown below: Host private1 Hostname 192.168.0.40 User ubuntu IdentityFile ~/.ssh/cloud.key ProxyJump bastion Host bastion HostName 140.247.152.139 User ubuntu IdentityFile ~/.ssh/cloud.key With this configuration in place, when you type ssh private1 SSH will establish a connection to the bastion host and then through the bastion host connect to \" private1 \", using the agent added keys or specified private keys. In this sort of arrangement, SSH traffic to private servers that are not directly accessible via SSH is instead directed through a bastion host, which proxies the connection between the SSH client and the remote servers. The bastion host runs on an instance that is typically in a public subnet with attached floating public IP. Private instances are in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying instance running the bastion host. The user won't see any of this; he or she will just see a shell for \" private1 \" appear. If you dig a bit further, though (try running who on the remote node), you'll see the connections are coming from the bastion host, not the original SSH client.","title":"Bastion Host based SSH"},{"location":"openstack/create-and-connect-to-the-VM/bastion-host-based-ssh/#bastion-host","text":"A bastion host is a server that provides secure access to private networks over SSH from an external network, such as the Internet. We can leverage a bastion host to record all SSH sessions established with private network instances which enables auditing and can help us in efforts to comply with regulatory requirements. The following diagram illustrates the concept of using an SSH bastion host to provide access to Linux instances running inside OpenStack cloud network. In OpenStack, users can deploy instances in a private tenant network. In order to make these instances to be accessible externally via internet, the tenant must assign each instance a floating IP address i.e., an external public IP. Nevertheless, users may still want a way to deploy instances without having to assign a floating IP address for every instance. This is useful in the context of an OpenStack project as you don't necessarily want to reserve a floating IP for all your instances. This way you can isolate certain resources so that there is only a single point of access to them and conserve floating IP addresses so that you don't need as big of a quota. Leveraging an SSH bastion host allows this sort of configuration while still enabling SSH access to the private instances. Before trying to access instances from the outside world using SSH tunneling via Bastion Host, you need to make sure you have followed these steps: You followed the instruction in Create a Key Pair to set up a public ssh key. You can use the same key for both the bastion host and the remote instances, or different keys; you'll just need to ensure that the keys are loaded by ssh-agent appropriately so they can be used as needed. Please read this instruction on how to add ssh-agent and load your private key using ssh-add command to access the bastion host. Verify you have an SSH agent running. This should match whatever you built your cluster with. ssh-add -l If you need to add the key to your agent: ssh-add path/to/private/key Now you can SSH into the bastion host: ssh -A <user>@<bastion-floating-IP> Your public ssh-key was selected (in the Access and Security tab) while launching the instance . Add two Security Groups, one will be used by the Bastion host and another one will be used by any private instances. i. Bastion Host Security Group: Allow inbound SSH (optional ICMP) for this security group. Make sure you have added rules in the Security Groups to allow ssh to the bastion host. ii. Private Instances Security Group: You need to select \"Security Group\" in Remote dropdown option, and then select the \" Bastion Host Security Group \" under Security Group option as shown below: Assign a Floating IP to the Bastion host instance in order to access it from outside world. Make a note of the floating IP you have associated to your instance. While adding the Bastion host and private instance, please select appropriate Security Group as shown below: private1: bastion_host_demo: Finally, you'll want to configure the ProxyJump setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). In SSH configuration file, we can define multiple hosts by pet names, specify custom ports, hostnames, users, etc. For example, let's say that you had a remote instance named \" private1 \" and you wanted to run SSH connections through a bastion host called \" bastion \". The appropriate SSH configuration file might look something like this: Host bastion HostName 140.247.152.139 User ubuntu Host private1 Hostname 192.168.0.40 User ubuntu ProxyJump bastion ProxyJump makes it super simple to jump from one host to another totally transparently. OR, if you don't have keys loaded by ssh-add command starting ssh-agent on your local machine. you can load the private key using IdentityFile variable in SSH configuration file as shown below: Host private1 Hostname 192.168.0.40 User ubuntu IdentityFile ~/.ssh/cloud.key ProxyJump bastion Host bastion HostName 140.247.152.139 User ubuntu IdentityFile ~/.ssh/cloud.key With this configuration in place, when you type ssh private1 SSH will establish a connection to the bastion host and then through the bastion host connect to \" private1 \", using the agent added keys or specified private keys. In this sort of arrangement, SSH traffic to private servers that are not directly accessible via SSH is instead directed through a bastion host, which proxies the connection between the SSH client and the remote servers. The bastion host runs on an instance that is typically in a public subnet with attached floating public IP. Private instances are in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying instance running the bastion host. The user won't see any of this; he or she will just see a shell for \" private1 \" appear. If you dig a bit further, though (try running who on the remote node), you'll see the connections are coming from the bastion host, not the original SSH client.","title":"Bastion Host"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/","text":"OpenVPN OpenVPN is a full-featured SSL VPN which implements OSI layer 2 or 3 secure network extension using the industry standard SSL/TLS protocol, supports flexible client authentication methods based on certificates, smart cards, and/ or username/password credentials, and allows user or group-specific access control policies using firewall rules applied to the VPN virtual interface. OpenVPN offers a scalable client/server mode, allowing multiple clients to connect to a single OpenVPN server process over a single TCP or UDP port. Installing OpenVPN Server You can read official documentation here . You can spin up a new instance with \" ubuntu-21.04-x86_64 \" flavor, named \" openvpn_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create OpenVPN server like this: Host openvpn HostName 140.247.152.229 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the OpenVPN Server running: ssh openvpn Also note that OpenVPN must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install OpenVPN server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.ovpn file) named as \" nerc.ovpn \". Based on your client's name it will name the config file as \" .ovpn \" Copy the generated config file from \" /root/nerc.ovpn \" to \" /home/ubuntu/ nerc.ovpn \" by running: cp /root/nerc.ovpn . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.ovpn You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp openvpn:nerc.ovpn . To add a new client user Once it ends, you can run it again to add more users, remove some of them or even completely uninstall OpenVPN. For this, run the script and follow the assistant: wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.ovpn \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client. Important Note You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A OpenVPN client or compatible software is needed to connect to the OpenVPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the OpenVPN server. Windows OpenVPN source code and Windows installers can be downloaded here . The OpenVPN executable should be installed on both server and client machines since the single executable provides both client and server functions. Please see the OpenVPN client setup guide for Windows . Mac OS X The client we recommend and support for Mac OS is Tunnelblick . To install Tunnelblick, download the dmg installer file from the Tunnelblick site , mount the dmg, and drag the Tunnelblick application to Applications. Please refer to this guide for more information. Linux OpenVPN is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install openvpn On RedHat/CentOS: sudo yum install openvpn Then, to run OpenVPN using the client profile: Move the VPN client profile (configuration) file to /etc/openvpn/ : sudo mv nerc.ovpn /etc/openvpn/client.conf Restart the OpenVPN daemon (i.e., This will start OpenVPN connection and will automatically run on boot): sudo /etc/init.d/openvpn start OR, sudo systemctl enable --now openvpn@client sudo systemctl start openvpn@client Checking the status: systemctl status openvpn@client Alternatively, if you want to run OpenVPN manually each time, then run: sudo openvpn --config /etc/openvpn/client.ovpn OR, sudo openvpn --config nerc.ovpn","title":"About OpenVPN"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#openvpn","text":"OpenVPN is a full-featured SSL VPN which implements OSI layer 2 or 3 secure network extension using the industry standard SSL/TLS protocol, supports flexible client authentication methods based on certificates, smart cards, and/ or username/password credentials, and allows user or group-specific access control policies using firewall rules applied to the VPN virtual interface. OpenVPN offers a scalable client/server mode, allowing multiple clients to connect to a single OpenVPN server process over a single TCP or UDP port.","title":"OpenVPN"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#installing-openvpn-server","text":"You can read official documentation here . You can spin up a new instance with \" ubuntu-21.04-x86_64 \" flavor, named \" openvpn_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create OpenVPN server like this: Host openvpn HostName 140.247.152.229 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the OpenVPN Server running: ssh openvpn Also note that OpenVPN must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install OpenVPN server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.ovpn file) named as \" nerc.ovpn \". Based on your client's name it will name the config file as \" .ovpn \" Copy the generated config file from \" /root/nerc.ovpn \" to \" /home/ubuntu/ nerc.ovpn \" by running: cp /root/nerc.ovpn . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.ovpn You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp openvpn:nerc.ovpn .","title":"Installing OpenVPN Server"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#to-add-a-new-client-user","text":"Once it ends, you can run it again to add more users, remove some of them or even completely uninstall OpenVPN. For this, run the script and follow the assistant: wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.ovpn \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client. Important Note You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A OpenVPN client or compatible software is needed to connect to the OpenVPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the OpenVPN server.","title":"To add a new client user"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#windows","text":"OpenVPN source code and Windows installers can be downloaded here . The OpenVPN executable should be installed on both server and client machines since the single executable provides both client and server functions. Please see the OpenVPN client setup guide for Windows .","title":"Windows"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#mac-os-x","text":"The client we recommend and support for Mac OS is Tunnelblick . To install Tunnelblick, download the dmg installer file from the Tunnelblick site , mount the dmg, and drag the Tunnelblick application to Applications. Please refer to this guide for more information.","title":"Mac OS X"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/#linux","text":"OpenVPN is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install openvpn On RedHat/CentOS: sudo yum install openvpn Then, to run OpenVPN using the client profile: Move the VPN client profile (configuration) file to /etc/openvpn/ : sudo mv nerc.ovpn /etc/openvpn/client.conf Restart the OpenVPN daemon (i.e., This will start OpenVPN connection and will automatically run on boot): sudo /etc/init.d/openvpn start OR, sudo systemctl enable --now openvpn@client sudo systemctl start openvpn@client Checking the status: systemctl status openvpn@client Alternatively, if you want to run OpenVPN manually each time, then run: sudo openvpn --config /etc/openvpn/client.ovpn OR, sudo openvpn --config nerc.ovpn","title":"Linux"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/","text":"OpenVPN-GUI Official OpenVPN Windows installers include a Windows OpenVPN-GUI , which allows managing OpenVPN connections from a system tray applet. Find your client account credentials You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. Download and install OpenVPN-GUI Download the OpenVPN client installer: OpenVPN for Windows can be installed from the self-installing exe file on the OpenVPN download page . Also note that OpenVPN must be installed and run by a user who has administrative privileges (this restriction is imposed by Windows, not OpenVPN) Launch the installer and follow the prompts as directed. Clicking \" Customize \" button we can see settings and features of OpenVPN GUI client. Click \" Install Now \" to continue. Click \"Close\"button. For the newly installed OpenVPN GUI there will be no configuration profile for the client so it will show a pop up that alerts: Set up the VPN with OpenVPN GUI After you've run the Windows installer, OpenVPN is ready for use and will associate itself with files having the .ovpn extension. You can use the previously downloaded .ovpn file from your Downloads folder to setup the connection profiles. a. Either you can Right click on the OpenVPN configuration file (.ovpn) and select \" Start OpenVPN on this config file \": b. OR, you can use \"Import file\u2026\" menu to select the previously downloaded .ovpn file. Once, done it will show: c. OR, you can manually copy the config file to one of OpenVPN's configuration directories: C:\\Program Files\\OpenVPN\\config (global configs) C:\\Program Files\\OpenVPN\\config-auto (autostarted global configs) %USERPROFILE%\\OpenVPN\\config (per-user configs) Connect to a VPN server location For launching OpenVPN Connections you click on OpenVPN GUI (tray applet). OpenVPN GUI is used to launching VPN connections on demand. OpenVPN GUI is a system-tray applet, so an icon for the GUI will appear in the lower-right corner of the screen located at the taskbar notification area. Right click on the system tray icon, and if you have multiple configurations then a menu should appear showing the names of your OpenVPN configuration profiles and giving you the option to connect. If you have only one configuration then you can just click on \"Connect\" menu. When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below in your terminal to connect to the private instances: ssh ubuntu@192.168. 0.40 -A -i cloud.key Disconnect VPN server To disconnect, right click on the system tray icon, in your status bar and select Disconnect from the menu.","title":"OpenVPN GUI for Windows"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#openvpn-gui","text":"Official OpenVPN Windows installers include a Windows OpenVPN-GUI , which allows managing OpenVPN connections from a system tray applet.","title":"OpenVPN-GUI"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#find-your-client-account-credentials","text":"You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.","title":"Find your client account credentials"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#download-and-install-openvpn-gui","text":"Download the OpenVPN client installer: OpenVPN for Windows can be installed from the self-installing exe file on the OpenVPN download page . Also note that OpenVPN must be installed and run by a user who has administrative privileges (this restriction is imposed by Windows, not OpenVPN) Launch the installer and follow the prompts as directed. Clicking \" Customize \" button we can see settings and features of OpenVPN GUI client. Click \" Install Now \" to continue. Click \"Close\"button. For the newly installed OpenVPN GUI there will be no configuration profile for the client so it will show a pop up that alerts:","title":"Download and install OpenVPN-GUI"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#set-up-the-vpn-with-openvpn-gui","text":"After you've run the Windows installer, OpenVPN is ready for use and will associate itself with files having the .ovpn extension. You can use the previously downloaded .ovpn file from your Downloads folder to setup the connection profiles. a. Either you can Right click on the OpenVPN configuration file (.ovpn) and select \" Start OpenVPN on this config file \": b. OR, you can use \"Import file\u2026\" menu to select the previously downloaded .ovpn file. Once, done it will show: c. OR, you can manually copy the config file to one of OpenVPN's configuration directories: C:\\Program Files\\OpenVPN\\config (global configs) C:\\Program Files\\OpenVPN\\config-auto (autostarted global configs) %USERPROFILE%\\OpenVPN\\config (per-user configs)","title":"Set up the VPN with OpenVPN GUI"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#connect-to-a-vpn-server-location","text":"For launching OpenVPN Connections you click on OpenVPN GUI (tray applet). OpenVPN GUI is used to launching VPN connections on demand. OpenVPN GUI is a system-tray applet, so an icon for the GUI will appear in the lower-right corner of the screen located at the taskbar notification area. Right click on the system tray icon, and if you have multiple configurations then a menu should appear showing the names of your OpenVPN configuration profiles and giving you the option to connect. If you have only one configuration then you can just click on \"Connect\" menu. When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below in your terminal to connect to the private instances: ssh ubuntu@192.168. 0.40 -A -i cloud.key","title":"Connect to a VPN server location"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/openvpn_gui_for_windows/#disconnect-vpn-server","text":"To disconnect, right click on the system tray icon, in your status bar and select Disconnect from the menu.","title":"Disconnect VPN server"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/","text":"Tunnelblick Tunnelblick is a free, open-source GUI (graphical user interface) for OpenVPN on macOS and OS X: More details can be found here . Access to a VPN server \u2014 your computer is one end of the tunnel and the VPN server is the other end. Find your client account credentials You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. Download and install Tunnelblick Download Tunnelblick , a free and user-friendly app for managing OpenVPN connections on macOS. Navigate to your Downloads folder and double-click the Tunnelblick installation file (.dmg installer file) you have just downloaded. In the window that opens, double-click on the Tunnelblick icon. A new dialogue box will pop up, asking you if you are sure you want to open the app. Click Open . You will be asked to enter your device password. Enter it and click OK : Select Allow or Don't Allow for your notification preference. Once the installation is complete, you will see a pop-up notification asking you if you want to launch Tunnelblick now. (An administrator username and password will be required to secure Tunnelblick). Click Launch . Alternatively, you can click on the Tunnelblick icon in the status bar and select VPN Details... : Set up the VPN with Tunnelblick A new dialogue box will appear. Click I have configuration files . Another notification will pop-up, instructing you how to import configuration files. Click OK . Drag and drop the previously downloaded .ovpn file from your Downloads folder to the Configurations tab in Tunnelblick. OR, You can just drag and drop the provided OpenVPN configuration file (file with .ovpn extension) directly to Tunnelblick icon in status bar at the top-right corner of your screen. A pop-up will appear, asking you if you want to install the configuration profile for your current user only or for all users on your Mac. Select your preferred option. If the VPN is intended for all accounts on your Mac, select All Users . If the VPN will only be used by your current account, select Only Me . You will be asked to enter your Mac password. Then the screen reads \" Tunnelblick successfully: installed one configuration \". You can see the configuration setting is loaded and installed successfully. Connect to a VPN server location To connect to a VPN server location, click the Tunnelblick icon in status bar at the top-right corner of your screen. From the drop down menu select the server and click Connect [name of the .ovpn configuration file] .. Alternatively, you can select \" VPN Details \" from the menu and then click the \" Connect \"button: This will show the connection log on the dialog: When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below to connect to the private instances: sh ssh ubuntu@192.168.0.40 -A -i cloud.key Disconnect VPN server To disconnect, click on the Tunnelblick icon in your status bar and select Disconnect in the drop-down menu. While closing the log will be shown on popup as shown below:","title":"Tunnelblick for MacOS"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#tunnelblick","text":"Tunnelblick is a free, open-source GUI (graphical user interface) for OpenVPN on macOS and OS X: More details can be found here . Access to a VPN server \u2014 your computer is one end of the tunnel and the VPN server is the other end.","title":"Tunnelblick"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#find-your-client-account-credentials","text":"You need to contact your project administrator to get your own OpenVPN configuration file (file with .ovpn extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file.","title":"Find your client account credentials"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#download-and-install-tunnelblick","text":"Download Tunnelblick , a free and user-friendly app for managing OpenVPN connections on macOS. Navigate to your Downloads folder and double-click the Tunnelblick installation file (.dmg installer file) you have just downloaded. In the window that opens, double-click on the Tunnelblick icon. A new dialogue box will pop up, asking you if you are sure you want to open the app. Click Open . You will be asked to enter your device password. Enter it and click OK : Select Allow or Don't Allow for your notification preference. Once the installation is complete, you will see a pop-up notification asking you if you want to launch Tunnelblick now. (An administrator username and password will be required to secure Tunnelblick). Click Launch . Alternatively, you can click on the Tunnelblick icon in the status bar and select VPN Details... :","title":"Download and install Tunnelblick"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#set-up-the-vpn-with-tunnelblick","text":"A new dialogue box will appear. Click I have configuration files . Another notification will pop-up, instructing you how to import configuration files. Click OK . Drag and drop the previously downloaded .ovpn file from your Downloads folder to the Configurations tab in Tunnelblick. OR, You can just drag and drop the provided OpenVPN configuration file (file with .ovpn extension) directly to Tunnelblick icon in status bar at the top-right corner of your screen. A pop-up will appear, asking you if you want to install the configuration profile for your current user only or for all users on your Mac. Select your preferred option. If the VPN is intended for all accounts on your Mac, select All Users . If the VPN will only be used by your current account, select Only Me . You will be asked to enter your Mac password. Then the screen reads \" Tunnelblick successfully: installed one configuration \". You can see the configuration setting is loaded and installed successfully.","title":"Set up the VPN with Tunnelblick"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#connect-to-a-vpn-server-location","text":"To connect to a VPN server location, click the Tunnelblick icon in status bar at the top-right corner of your screen. From the drop down menu select the server and click Connect [name of the .ovpn configuration file] .. Alternatively, you can select \" VPN Details \" from the menu and then click the \" Connect \"button: This will show the connection log on the dialog: When you are connected to OpenVPN server successfully, you will see popup message as shown below. That's it! You are now connected to a VPN. Once you are connected to the OpenVPN server, you can run commands like shown below to connect to the private instances: sh ssh ubuntu@192.168.0.40 -A -i cloud.key","title":"Connect to a VPN server location"},{"location":"openstack/create-and-connect-to-the-VM/openvpn/tunnelblick_for_macos/#disconnect-vpn-server","text":"To disconnect, click on the Tunnelblick icon in your status bar and select Disconnect in the drop-down menu. While closing the log will be shown on popup as shown below:","title":"Disconnect VPN server"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/","text":"sshuttle sshuttle is a lightweight SSH-encrypted VPN. This is a Python based script that allows you to tunnel connections through SSH in a far more efficient way then traditional ssh proxying. Installing sshuttle Server You can spin up a new instance with \" ubuntu-21.04-x86_64 \" flavor named \" sshuttle_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create sshuttle server like this: Host sshuttle HostName 140.247.152.244 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the sshuttle Server running: ssh sshuttle Note Unlike other VPN servers, for sshuttle you don't need to install anything on the server side. As long as you have an SSH server (with python3 installed) you're good to go. To connect from a new client Install sshuttle Windows Currently there is no built in support for running sshuttle directly on Microsoft Windows. What you can do is to create a Linux VM with Vagrant (or simply Virtualbox if you like) and then try to connect via that VM. For more details read here Mac OS X Install using Homebrew : brew install sshuttle OR, via MacPorts sudo port selfupdate sudo port install sshuttle Linux sshuttle is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install sshuttle On RedHat/CentOS: sudo yum install sshuttle It is also possible to install into a virtualenv as a non-root user . From PyPI: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate pip install sshuttle Clone: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate git clone [https://github.com/sshuttle/sshuttle.git](https://github.com/sshuttle/sshuttle.git) cd sshuttle ./setup.py install How to Connect Tunnel to all networks (0.0.0.0/0): sshuttle -r ubuntu @140.247.152.244 0.0.0.0/0 OR, shorthand: sudo sshuttle -r ubuntu@140.247.152.244 0/0 If you would also like your DNS queries to be proxied through the DNS server of the server, you are connected to: sshuttle --dns -r ubuntu@140.247.152.244 0/0","title":"sshuttle"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#sshuttle","text":"sshuttle is a lightweight SSH-encrypted VPN. This is a Python based script that allows you to tunnel connections through SSH in a far more efficient way then traditional ssh proxying.","title":"sshuttle"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#installing-sshuttle-server","text":"You can spin up a new instance with \" ubuntu-21.04-x86_64 \" flavor named \" sshuttle_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create sshuttle server like this: Host sshuttle HostName 140.247.152.244 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the sshuttle Server running: ssh sshuttle Note Unlike other VPN servers, for sshuttle you don't need to install anything on the server side. As long as you have an SSH server (with python3 installed) you're good to go.","title":"Installing sshuttle Server"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#to-connect-from-a-new-client","text":"","title":"To connect from a new client"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#install-sshuttle","text":"","title":"Install sshuttle"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#windows","text":"Currently there is no built in support for running sshuttle directly on Microsoft Windows. What you can do is to create a Linux VM with Vagrant (or simply Virtualbox if you like) and then try to connect via that VM. For more details read here","title":"Windows"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#mac-os-x","text":"Install using Homebrew : brew install sshuttle OR, via MacPorts sudo port selfupdate sudo port install sshuttle","title":"Mac OS X"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#linux","text":"sshuttle is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt-get install sshuttle On RedHat/CentOS: sudo yum install sshuttle It is also possible to install into a virtualenv as a non-root user . From PyPI: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate pip install sshuttle Clone: virtualenv -p python3 /tmp/sshuttle . /tmp/sshuttle/bin/activate git clone [https://github.com/sshuttle/sshuttle.git](https://github.com/sshuttle/sshuttle.git) cd sshuttle ./setup.py install","title":"Linux"},{"location":"openstack/create-and-connect-to-the-VM/sshuttle/#how-to-connect","text":"Tunnel to all networks (0.0.0.0/0): sshuttle -r ubuntu @140.247.152.244 0.0.0.0/0 OR, shorthand: sudo sshuttle -r ubuntu@140.247.152.244 0/0 If you would also like your DNS queries to be proxied through the DNS server of the server, you are connected to: sshuttle --dns -r ubuntu@140.247.152.244 0/0","title":"How to Connect"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/","text":"WireGuard WireGuard is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. Here's what it will look like: Installing WireGuard Server You can spin up a new instance with \" ubuntu-21.04-x86_64 \" flavor, named \" wireguard_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create WireGuard server like this: Host wireguard HostName 140.247.152.188 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the WireGuard Server running: ssh wireguard Also note that WireGuard must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install WireGuard server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.conf file) named as \" nerc.conf \". Based on your client's name it will name the config file as \" .conf \" Note: For each peers the client configuration files comply with the following template: Copy the generated config file from \" /root/nerc.conf \" to \" /home/ubuntu/nerc.conf \" by running: cp /root/nerc.conf . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.conf You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp wireguard:nerc.conf . To add a new client user Once it ends, you can run it again to add more users, remove some of them or even completely uninstall WireGuard. For this, run the script and follow the assistant: wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.conf \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client. Authentication Mechanism It would be kind of pointless to have our VPN server allow anyone to connect. This is where our public & private keys come into play. Each client's **public** key needs to be added to the SERVER'S configuration file The server's **public** key added to the CLIENT'S configuration file Useful commands To view server config: wg show or, wg To activateconfig: wg-quick up /path/to/file_name.config To deactivate config: wg-quick down /path/to/file_name.config Read more: https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8 https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8 Important Note You need to contact your project administrator to get your own WireGUard configuration file (file with .conf extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A WireGuard client or compatible software is needed to connect to the WireGuard VPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the WireGuard VPN server. Windows WireGuard client can be downloaded here . The WireGuard executable should be installed on client machines. After the installation, you should see the WireGuard icon in the lower-right corner of the screen located at the taskbar notification area. Set up the VPN with WireGuard GUI Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI and either click on Add Tunnel -> Import tunnel(s) from file\u2026 OR, click on \" Import tunnel(s) from file \" button located at the center. The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. Either, Right Click on your tunnel name and select \" Edit selected tunnel\u2026 \" menu OR, click on \" Edit \" button at the lower left. Checking Block untunneled traffic (kill-switch) will make sure that all your traffic is being routed through this new VPN server. Test your connection On your Windows machine, press the \" Activate\" button. You should see a successful connection be made: After a few seconds, the status should change to Active. If the connection is routed through the VPN, it should show the IP address of the WireGuard server as the public address. If that's not the case, to troubleshoot please check the \" Log \" tab and verify and validate the client and server configuration. Clicking \" Deactivate \" button closes the VPN connection. Mac OS X I. Using HomeBrew This allows more than one Wireguard tunnel active at a time unlike the WireGuard GUI app. Install WireGuard CLI on macOS through brew: brew install wireguard-tools Copy the \" .conf \" file to \" /usr/local/etc/wireguard/ \" (or \" /etc/wireguard/ \"). You'll need to create the \" wireguard \" directory first. For your example, you will have your config file located at: \" /usr/local/etc /wireguard/mac_client.conf \" or, \" /etc/wireguard/mac_client.conf \" To activate the VPN: \"wg-quick up [ name of the conf file without including .conf extension ]\". For example, in your case, running wg-quick up mac_client - If the peer system is already configured and its interface is up, then the VPN connection should establish automatically, and you should be able to start routing traffic through the peer. Use wg-quick down mac_client to take the VPN connection down. II. Using WireGuard GUI App Download WireGuard Client from the macOS App Store You can find the official WireGuard Client app on the App Store here . Set up the VPN with WireGuard Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI by directly clicking WireGuard icon in status bar at the top-right corner of your screen. And then click on \" Import tunnel(s) from file \" menu to load your client config file. OR, Find and click the WireGUard GUI from your Launchpad and then either click on Add Tunnel -> Import tunnel(s) from file\u2026 or, just click on \" Import tunnel(s) from file \" button located at the center. Browse to the configuration file: The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. If you would like your computer to automatically connect to the WireGuard VPN server as soon as either (or both) Ethernet or Wi-Fi network adapter becomes active, check the relevant ' On-Demand ' checkboxes for \" Ethernet \" and \" Wi-Fi \". Checking Exclude private IPs will generate a list of networks which excludes the server IP address and add them to the AllowedIPs list. This setting allows you to pass all your traffic through your Wireguard VPN EXCLUDING private address ranges like 10.0.0.0/8 , 172.16.0.0/12 , and 192.168.0.0/16 . Test your connection On your Windows machine, press the \" Activate \" button. You should see a successful connection be made: After a few seconds, the status should change to Active. Clicking \" Deactivate \" button from the GUI's interface or directly clicking \" Deactivate \" menu from the WireGuard icon in status bar at the top-right corner of your screen closes the VPN connection. Linux WireGuard is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt update sudo apt-get install wireguard resolvconf -y On RedHat/CentOS: sudo yum install wireguard Then, to run WireGuard using the client profile: Move the VPN client profile (configuration) file to /etc/wireguard/ : sudo mv nerc.conf /etc/wireguard/client.conf Restart the WireGuard daemon (i.e., This will start WireGuard connection and will automatically run on boot): sudo /etc/init.d/wireguard start OR, sudo systemctl enable --now wg-quick@client sudo systemctl start wg-quick@client OR, wg-quick up /etc/wireguard/client.conf Checking the status: systemctl status wg-quick@client Alternatively, if you want to run WireGuard manually each time, then run: sudo wireguard --config /etc/wireguard/client.conf OR, sudo wireguard --config nerc.conf","title":"WireGuard"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#wireguard","text":"WireGuard is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. Here's what it will look like:","title":"WireGuard"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#installing-wireguard-server","text":"You can spin up a new instance with \" ubuntu-21.04-x86_64 \" flavor, named \" wireguard_server \" on OpenStack, with \" default \" and \" ssh_only \" Security Groups attached to it. Also, attach a Floating IP to this instance so you can ssh into it from outside. Finally, you'll want to configure the setting for the remote instances in your SSH configuration file (typically found in ~/.ssh/config ). The SSH configuration file might include entry for your newly create WireGuard server like this: Host wireguard HostName 140.247.152.188 User ubuntu IdentityFile ~/.ssh/cloud.key Then you can ssh into the WireGuard Server running: ssh wireguard Also note that WireGuard must be installed and run by a user who has administrative/root privileges. So, we need to run the command: sudo su We are using this repo to install WireGuard server on this ubuntu server. For that, run the script and follow the assistant: sh wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh You can press Enter for all default values. And, while entering a name for the first client you can give \" nerc \" as the client name, this will generate a new configuration file (.conf file) named as \" nerc.conf \". Based on your client's name it will name the config file as \" .conf \" Note: For each peers the client configuration files comply with the following template: Copy the generated config file from \" /root/nerc.conf \" to \" /home/ubuntu/nerc.conf \" by running: cp /root/nerc.conf . Update the ownership of the config file to ubuntu user and ubuntu group by running the following command: chown ubuntu:ubuntu nerc.conf You can exit from the root and ssh session all together and then copy the configuration file to your local machine by running the following script on your local machine's terminal: scp wireguard:nerc.conf .","title":"Installing WireGuard Server"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#to-add-a-new-client-user","text":"Once it ends, you can run it again to add more users, remove some of them or even completely uninstall WireGuard. For this, run the script and follow the assistant: wget https://git.io/wireguard -O wireguard-install.sh && bash wireguard-install.sh Here, you are giving client name as \" mac_client \" and that will generate a new configuration file at \" /root/mac_client.conf \". You can repeat above steps: 4 to 6 to copy this new client's configuration file and share it to the new client.","title":"To add a new client user"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#authentication-mechanism","text":"It would be kind of pointless to have our VPN server allow anyone to connect. This is where our public & private keys come into play. Each client's **public** key needs to be added to the SERVER'S configuration file The server's **public** key added to the CLIENT'S configuration file","title":"Authentication Mechanism"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#useful-commands","text":"To view server config: wg show or, wg To activateconfig: wg-quick up /path/to/file_name.config To deactivate config: wg-quick down /path/to/file_name.config Read more: https://git.zx2c4.com/wireguard-tools/about/src/man/wg.8 https://git.zx2c4.com/wireguard-tools/about/src/man/wg-quick.8 Important Note You need to contact your project administrator to get your own WireGUard configuration file (file with .conf extension). Download it and Keep it in your local machine so in next steps we can use this configuration client profile file. A WireGuard client or compatible software is needed to connect to the WireGuard VPN server. Please install one of these clients depending on your device. The client program must be configured with a client profile to connect to the WireGuard VPN server.","title":"Useful commands"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#windows","text":"WireGuard client can be downloaded here . The WireGuard executable should be installed on client machines. After the installation, you should see the WireGuard icon in the lower-right corner of the screen located at the taskbar notification area.","title":"Windows"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#set-up-the-vpn-with-wireguard-gui","text":"Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI and either click on Add Tunnel -> Import tunnel(s) from file\u2026 OR, click on \" Import tunnel(s) from file \" button located at the center. The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. Either, Right Click on your tunnel name and select \" Edit selected tunnel\u2026 \" menu OR, click on \" Edit \" button at the lower left. Checking Block untunneled traffic (kill-switch) will make sure that all your traffic is being routed through this new VPN server.","title":"Set up the VPN with WireGuard GUI"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#test-your-connection","text":"On your Windows machine, press the \" Activate\" button. You should see a successful connection be made: After a few seconds, the status should change to Active. If the connection is routed through the VPN, it should show the IP address of the WireGuard server as the public address. If that's not the case, to troubleshoot please check the \" Log \" tab and verify and validate the client and server configuration. Clicking \" Deactivate \" button closes the VPN connection.","title":"Test your connection"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#mac-os-x","text":"","title":"Mac OS X"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#i-using-homebrew","text":"This allows more than one Wireguard tunnel active at a time unlike the WireGuard GUI app. Install WireGuard CLI on macOS through brew: brew install wireguard-tools Copy the \" .conf \" file to \" /usr/local/etc/wireguard/ \" (or \" /etc/wireguard/ \"). You'll need to create the \" wireguard \" directory first. For your example, you will have your config file located at: \" /usr/local/etc /wireguard/mac_client.conf \" or, \" /etc/wireguard/mac_client.conf \" To activate the VPN: \"wg-quick up [ name of the conf file without including .conf extension ]\". For example, in your case, running wg-quick up mac_client - If the peer system is already configured and its interface is up, then the VPN connection should establish automatically, and you should be able to start routing traffic through the peer. Use wg-quick down mac_client to take the VPN connection down.","title":"I. Using HomeBrew"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#ii-using-wireguard-gui-app","text":"Download WireGuard Client from the macOS App Store You can find the official WireGuard Client app on the App Store here . Set up the VPN with WireGuard Next, we configure the VPN tunnel. This includes setting up the endpoints and exchanging the public keys. Open the WireGuard GUI by directly clicking WireGuard icon in status bar at the top-right corner of your screen. And then click on \" Import tunnel(s) from file \" menu to load your client config file. OR, Find and click the WireGUard GUI from your Launchpad and then either click on Add Tunnel -> Import tunnel(s) from file\u2026 or, just click on \" Import tunnel(s) from file \" button located at the center. Browse to the configuration file: The software automatically loads the client configuration. Also, it creates a public key for this new tunnel and displays it on the screen. If you would like your computer to automatically connect to the WireGuard VPN server as soon as either (or both) Ethernet or Wi-Fi network adapter becomes active, check the relevant ' On-Demand ' checkboxes for \" Ethernet \" and \" Wi-Fi \". Checking Exclude private IPs will generate a list of networks which excludes the server IP address and add them to the AllowedIPs list. This setting allows you to pass all your traffic through your Wireguard VPN EXCLUDING private address ranges like 10.0.0.0/8 , 172.16.0.0/12 , and 192.168.0.0/16 . Test your connection On your Windows machine, press the \" Activate \" button. You should see a successful connection be made: After a few seconds, the status should change to Active. Clicking \" Deactivate \" button from the GUI's interface or directly clicking \" Deactivate \" menu from the WireGuard icon in status bar at the top-right corner of your screen closes the VPN connection.","title":"II. Using WireGuard GUI App"},{"location":"openstack/create-and-connect-to-the-VM/wireguard/#linux","text":"WireGuard is available through the package management system on most Linux distributions. On Debian/Ubuntu: sudo apt update sudo apt-get install wireguard resolvconf -y On RedHat/CentOS: sudo yum install wireguard Then, to run WireGuard using the client profile: Move the VPN client profile (configuration) file to /etc/wireguard/ : sudo mv nerc.conf /etc/wireguard/client.conf Restart the WireGuard daemon (i.e., This will start WireGuard connection and will automatically run on boot): sudo /etc/init.d/wireguard start OR, sudo systemctl enable --now wg-quick@client sudo systemctl start wg-quick@client OR, wg-quick up /etc/wireguard/client.conf Checking the status: systemctl status wg-quick@client Alternatively, if you want to run WireGuard manually each time, then run: sudo wireguard --config /etc/wireguard/client.conf OR, sudo wireguard --config nerc.conf","title":"Linux"},{"location":"openstack/kubernetes/comparisons/","text":"Comparison Kubespray vs Kubeadm Kubeadm provides domain Knowledge of Kubernetes clusters' life cycle management, including self-hosted layouts, dynamic discovery services and so on. Had it belonged to the new operators world , it may have been named a \"Kubernetes cluster operator\". Kubespray however, does generic configuration management tasks from the \"OS operators\" ansible world, plus some initial K8s clustering (with networking plugins included) and control plane bootstrapping. Kubespray has started using kubeadm internally for cluster creation since v2.3 in order to consume life cycle management domain knowledge from it and offload generic OS configuration things from it, which hopefully benefits both sides.","title":"Comparison"},{"location":"openstack/kubernetes/comparisons/#comparison","text":"","title":"Comparison"},{"location":"openstack/kubernetes/comparisons/#kubespray-vs-kubeadm","text":"Kubeadm provides domain Knowledge of Kubernetes clusters' life cycle management, including self-hosted layouts, dynamic discovery services and so on. Had it belonged to the new operators world , it may have been named a \"Kubernetes cluster operator\". Kubespray however, does generic configuration management tasks from the \"OS operators\" ansible world, plus some initial K8s clustering (with networking plugins included) and control plane bootstrapping. Kubespray has started using kubeadm internally for cluster creation since v2.3 in order to consume life cycle management domain knowledge from it and offload generic OS configuration things from it, which hopefully benefits both sides.","title":"Kubespray vs Kubeadm"},{"location":"openstack/kubernetes/crc/","text":"CRC - Red Hat Code Ready Containers Red Hat CodeReady Containers allows you to spin up a small Red Hat OpenShift cluster on your local PC, without the need for a server, a cloud, or a team of operations people. For developers who want to get started immediately with cloud-native development, containers, and Kubernetes (as well as OpenShift), it's a simple and slick tool. It runs on macOS, Linux, and all versions of Windows 10. Minimum system requirements for CRC CodeReady Containers requires the following system resources: 4 virtual CPUs (vCPUs) 8 GB of memory 35 GB of storage space Pre-requisite We will need 1 VM to create a single node kubernetes cluster using crc . We are using following setting for this purpose: 1 Linux machine, fedora-34-x86_64, c2.s2.xlarge flavor with 16vCPUs, 32GB RAM, 50GB storage - also assign Floating IP to this VM. Prepare host for CRC Run the below command on the CRC's VM: SSH into crc machine Important Note Please run the following commands not using root user. On newly built VM download CRC using your redhat login, from: https://console.redhat.com/openshift/create/local To save transfer hassle you can, curl CRC bundle directly to the VM, using the url from a \"Download CodeReady Containers\" button in redhat console. curl \\ https://developers.redhat.com/content-gateway/rest/mirror/pub/openshift-v4/\\ clients/crc/latest/crc-linux-amd64.tar.xz \\ --output crc-linux-amd64.tar.xz -L then click \"Copy pull secret\" button from the same console page and save it to a file somewhere (for example ~fedora/pull-secret ) Setup crc binary to be accessable tar -xvf crc-linux-amd64.tar.xz mkdir -p ~/bin mv crc-linux-1.34.0-amd64/crc ~/bin/ export PATH=$PATH:$HOME/bin echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc Note This CRC version crc-linux-1.34.0-amd64 may be different when you are installing! Please update it as your are running above command. Install and configure CRC Run the install (you can choose to answer N for collecing data by RedHat when prompted on terminal). Also, CRC has trouble starting with default resources allocated to it, that is why additional steps need to be taken allocationg addtional memory to it. Additional cores added as well, but this may not be as critical to it's performance. crc setup crc config set memory 24576 crc config set cpus 12 Paste crc secret copied during previous prep step when prompted for \"? Please enter the pull secret\" by crc start terminal. crc start Make a note of user login info displayed once install is finished. Output would look like below: Started the OpenShift cluster. The server is accessible via web console at: https://console-openshift-console.apps-crc.testing Log in as administrator: Username: kubeadmin Password: ... #pragma: allowlist secret Log in as user: Username: developer Password: ... #pragma: allowlist secret Use the 'oc' command line interface: $ eval $(crc oc-env) $ oc login -u developer https://api.crc.testing:6443 Using CRC CLI Setup your environment eval $(crc oc-env) To look up CRC login credentials you can run. This will provide the oc login commands with password info for both admin and developer users. crc console --credentials To login as a regular user, run 'oc login -u developer -p developer https://api.crc.testing:6443'. To login as an admin, run 'oc login -u kubeadmin -p MTNAK-YHvuU-FIuSt-qgAxd https://api.crc.testing:6443' Using CRC web interface Install and configure HAPROXY first Switch as root: sudo su Install the package sudo dnf install haproxy policycoreutils-python-utils Update configuration cd /etc/haproxy sudo cp haproxy.cfg haproxy.cfg.orig Clean the content of /etc/haproxy/haproxy.cfg : sudo echo > /etc/haproxy/haproxy.cfg Replace /etc/haproxy/haproxy.cfg with cat <<EOF | sudo tee /etc/haproxy/haproxy.cfg global defaults log global mode http timeout connect 0 timeout client 0 timeout server 0 frontend apps bind SERVER_IP:80 bind SERVER_IP:443 option tcplog mode tcp default_backend apps backend apps mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:443 check frontend api bind SERVER_IP:6443 option tcplog mode tcp default_backend api backend api mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:6443 check EOF Plugin your servers and crc ip addresses # this may be different depending on your setup export SERVER_IP=$(hostname --ip-address |cut -d\\ -f3) export CRC_IP=$(crc ip) sudo sed -i \"s/SERVER_IP/$SERVER_IP/g\" /etc/haproxy/haproxy.cfg sudo sed -i \"s/CRC_IP/$CRC_IP/g\" haproxy.cfg sudo semanage port -a -t http_port_t -p tcp 6443 Start haproxy sudo systemctl start haproxy sudo systemctl status haproxy Ensure haproxy is in running status. Note Switch out from root to fedora user. To check the internal IP, run the crc ip command. Configure your local workstation to resolve CRC addresses Add security groups for your CRC instance to open ports 80, 443 and 6443. In our example setup, the Security Group Rules that are attached to a new Security Rule to the CRC instance has entries like this: Configure your local workstations host lookup to resolve names associated with CRC to the Public IP i.e. Floating IP address of your openstack VM instance. This can be done in several ways: 1) RH document [2] describes a dnsmasq configuration. 2) A simpler path for Linux and Mac users is just to create an entry in your /etc/hosts file or for Windows users find it at C:\\Windows\\System32\\Drivers\\etc\\hosts . Associate your CRC servers public ip retrieved from Horizon with hostnames: api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing For example, using #2 by adding an entry in your /etc/hosts file in your local machine. Your local machine's /etc/hosts may have an entry looks like this: <Your CRC Instance's Floating IP> api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing Use what's appropriate to your environment, ask for help if unsure. Point your browser https://console-openshift-console.apps-crc.testing and log in using crednetials provided by the output of crc start or crc console --credentials command. References [1] Getting Started CRC [2] Accessing CRC on a remote server","title":"CRC - Red Hat Code Ready Containers"},{"location":"openstack/kubernetes/crc/#crc-red-hat-code-ready-containers","text":"Red Hat CodeReady Containers allows you to spin up a small Red Hat OpenShift cluster on your local PC, without the need for a server, a cloud, or a team of operations people. For developers who want to get started immediately with cloud-native development, containers, and Kubernetes (as well as OpenShift), it's a simple and slick tool. It runs on macOS, Linux, and all versions of Windows 10.","title":"CRC - Red Hat Code Ready Containers"},{"location":"openstack/kubernetes/crc/#minimum-system-requirements-for-crc","text":"CodeReady Containers requires the following system resources: 4 virtual CPUs (vCPUs) 8 GB of memory 35 GB of storage space","title":"Minimum system requirements for CRC"},{"location":"openstack/kubernetes/crc/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using crc . We are using following setting for this purpose: 1 Linux machine, fedora-34-x86_64, c2.s2.xlarge flavor with 16vCPUs, 32GB RAM, 50GB storage - also assign Floating IP to this VM.","title":"Pre-requisite"},{"location":"openstack/kubernetes/crc/#prepare-host-for-crc","text":"Run the below command on the CRC's VM: SSH into crc machine Important Note Please run the following commands not using root user. On newly built VM download CRC using your redhat login, from: https://console.redhat.com/openshift/create/local To save transfer hassle you can, curl CRC bundle directly to the VM, using the url from a \"Download CodeReady Containers\" button in redhat console. curl \\ https://developers.redhat.com/content-gateway/rest/mirror/pub/openshift-v4/\\ clients/crc/latest/crc-linux-amd64.tar.xz \\ --output crc-linux-amd64.tar.xz -L then click \"Copy pull secret\" button from the same console page and save it to a file somewhere (for example ~fedora/pull-secret ) Setup crc binary to be accessable tar -xvf crc-linux-amd64.tar.xz mkdir -p ~/bin mv crc-linux-1.34.0-amd64/crc ~/bin/ export PATH=$PATH:$HOME/bin echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc Note This CRC version crc-linux-1.34.0-amd64 may be different when you are installing! Please update it as your are running above command.","title":"Prepare host for CRC"},{"location":"openstack/kubernetes/crc/#install-and-configure-crc","text":"Run the install (you can choose to answer N for collecing data by RedHat when prompted on terminal). Also, CRC has trouble starting with default resources allocated to it, that is why additional steps need to be taken allocationg addtional memory to it. Additional cores added as well, but this may not be as critical to it's performance. crc setup crc config set memory 24576 crc config set cpus 12 Paste crc secret copied during previous prep step when prompted for \"? Please enter the pull secret\" by crc start terminal. crc start Make a note of user login info displayed once install is finished. Output would look like below: Started the OpenShift cluster. The server is accessible via web console at: https://console-openshift-console.apps-crc.testing Log in as administrator: Username: kubeadmin Password: ... #pragma: allowlist secret Log in as user: Username: developer Password: ... #pragma: allowlist secret Use the 'oc' command line interface: $ eval $(crc oc-env) $ oc login -u developer https://api.crc.testing:6443","title":"Install and configure CRC"},{"location":"openstack/kubernetes/crc/#using-crc-cli","text":"Setup your environment eval $(crc oc-env) To look up CRC login credentials you can run. This will provide the oc login commands with password info for both admin and developer users. crc console --credentials To login as a regular user, run 'oc login -u developer -p developer https://api.crc.testing:6443'. To login as an admin, run 'oc login -u kubeadmin -p MTNAK-YHvuU-FIuSt-qgAxd https://api.crc.testing:6443'","title":"Using CRC CLI"},{"location":"openstack/kubernetes/crc/#using-crc-web-interface","text":"","title":"Using CRC web interface"},{"location":"openstack/kubernetes/crc/#install-and-configure-haproxy-first","text":"Switch as root: sudo su Install the package sudo dnf install haproxy policycoreutils-python-utils Update configuration cd /etc/haproxy sudo cp haproxy.cfg haproxy.cfg.orig Clean the content of /etc/haproxy/haproxy.cfg : sudo echo > /etc/haproxy/haproxy.cfg Replace /etc/haproxy/haproxy.cfg with cat <<EOF | sudo tee /etc/haproxy/haproxy.cfg global defaults log global mode http timeout connect 0 timeout client 0 timeout server 0 frontend apps bind SERVER_IP:80 bind SERVER_IP:443 option tcplog mode tcp default_backend apps backend apps mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:443 check frontend api bind SERVER_IP:6443 option tcplog mode tcp default_backend api backend api mode tcp balance roundrobin option ssl-hello-chk server webserver1 CRC_IP:6443 check EOF Plugin your servers and crc ip addresses # this may be different depending on your setup export SERVER_IP=$(hostname --ip-address |cut -d\\ -f3) export CRC_IP=$(crc ip) sudo sed -i \"s/SERVER_IP/$SERVER_IP/g\" /etc/haproxy/haproxy.cfg sudo sed -i \"s/CRC_IP/$CRC_IP/g\" haproxy.cfg sudo semanage port -a -t http_port_t -p tcp 6443 Start haproxy sudo systemctl start haproxy sudo systemctl status haproxy Ensure haproxy is in running status. Note Switch out from root to fedora user. To check the internal IP, run the crc ip command.","title":"Install and configure HAPROXY first"},{"location":"openstack/kubernetes/crc/#configure-your-local-workstation-to-resolve-crc-addresses","text":"Add security groups for your CRC instance to open ports 80, 443 and 6443. In our example setup, the Security Group Rules that are attached to a new Security Rule to the CRC instance has entries like this: Configure your local workstations host lookup to resolve names associated with CRC to the Public IP i.e. Floating IP address of your openstack VM instance. This can be done in several ways: 1) RH document [2] describes a dnsmasq configuration. 2) A simpler path for Linux and Mac users is just to create an entry in your /etc/hosts file or for Windows users find it at C:\\Windows\\System32\\Drivers\\etc\\hosts . Associate your CRC servers public ip retrieved from Horizon with hostnames: api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing For example, using #2 by adding an entry in your /etc/hosts file in your local machine. Your local machine's /etc/hosts may have an entry looks like this: <Your CRC Instance's Floating IP> api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing apps-crc.testing Use what's appropriate to your environment, ask for help if unsure. Point your browser https://console-openshift-console.apps-crc.testing and log in using crednetials provided by the output of crc start or crc console --credentials command.","title":"Configure your local workstation to resolve CRC addresses"},{"location":"openstack/kubernetes/crc/#references","text":"[1] Getting Started CRC [2] Accessing CRC on a remote server","title":"References"},{"location":"openstack/kubernetes/k0s/","text":"k0s Key Features Available as a single static binary Offers a self-hosted, isolated control plane Supports a variety of storage backends, including etcd, SQLite, MySQL (or any compatible), and PostgreSQL. Offers an Elastic control plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64 Pre-requisite We will need 1 VM to create a single node kubernetes cluster using k0s . We are using following setting for this purpose: 1 Linux machine, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.252 k0s\" >> /etc/hosts hostnamectl set-hostname k0s Install k0s on Ubuntu Run the below command on the Ubuntu VM: SSH into k0s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Download k0s: curl -sSLf https://get.k0s.sh | sudo sh Install k0s as a service: k0s install controller --single INFO[2021-10-12 01:45:52] no config file given, using defaults INFO[2021-10-12 01:45:52] creating user: etcd INFO[2021-10-12 01:46:00] creating user: kube-apiserver INFO[2021-10-12 01:46:00] creating user: konnectivity-server INFO[2021-10-12 01:46:00] creating user: kube-scheduler INFO[2021-10-12 01:46:01] Installing k0s service Start k0s as a service: k0s start Check service, logs and k0s status: k0s status Version: v1.22.2+k0s.1 Process ID: 16625 Role: controller Workloads: true Access your cluster using kubectl : k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 8m3s v1.22.2+k0s alias kubectl='k0s kubectl' kubectl get nodes -o wide kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 38s Uninstall k0s Stop the service: sudo k0s stop Execute the k0s reset command - cleans up the installed system service, data directories, containers, mounts and network namespaces. sudo k0s reset Reboot the system","title":"k0s"},{"location":"openstack/kubernetes/k0s/#k0s","text":"","title":"k0s"},{"location":"openstack/kubernetes/k0s/#key-features","text":"Available as a single static binary Offers a self-hosted, isolated control plane Supports a variety of storage backends, including etcd, SQLite, MySQL (or any compatible), and PostgreSQL. Offers an Elastic control plane Vanilla upstream Kubernetes Supports custom container runtimes (containerd is the default) Supports custom Container Network Interface (CNI) plugins (calico is the default) Supports x86_64 and arm64","title":"Key Features"},{"location":"openstack/kubernetes/k0s/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using k0s . We are using following setting for this purpose: 1 Linux machine, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.252 k0s\" >> /etc/hosts hostnamectl set-hostname k0s","title":"Pre-requisite"},{"location":"openstack/kubernetes/k0s/#install-k0s-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into k0s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Download k0s: curl -sSLf https://get.k0s.sh | sudo sh Install k0s as a service: k0s install controller --single INFO[2021-10-12 01:45:52] no config file given, using defaults INFO[2021-10-12 01:45:52] creating user: etcd INFO[2021-10-12 01:46:00] creating user: kube-apiserver INFO[2021-10-12 01:46:00] creating user: konnectivity-server INFO[2021-10-12 01:46:00] creating user: kube-scheduler INFO[2021-10-12 01:46:01] Installing k0s service Start k0s as a service: k0s start Check service, logs and k0s status: k0s status Version: v1.22.2+k0s.1 Process ID: 16625 Role: controller Workloads: true Access your cluster using kubectl : k0s kubectl get nodes NAME STATUS ROLES AGE VERSION k0s Ready <none> 8m3s v1.22.2+k0s alias kubectl='k0s kubectl' kubectl get nodes -o wide kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 38s","title":"Install k0s on Ubuntu"},{"location":"openstack/kubernetes/k0s/#uninstall-k0s","text":"Stop the service: sudo k0s stop Execute the k0s reset command - cleans up the installed system service, data directories, containers, mounts and network namespaces. sudo k0s reset Reboot the system","title":"Uninstall k0s"},{"location":"openstack/kubernetes/kind/","text":"Kind Pre-requisite We will need 1 VM to create a single node kubernetes cluster using kind . We are using following setting for this purpose: 1 Linux machine, centos-7-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 kind\" >> /etc/hosts hostnamectl set-hostname kind Install docker on CentOS7 Run the below command on the CentOS7 VM: SSH into kind machine Switch to root user: sudo su Execute the below command to initialize the cluster: yum -y install epel-release; yum -y install docker; systemctl enable --now docker; systemctl status docker docker version Install kubectl on CentOS7 curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl chmod +x /usr/bin/kubectl Test to ensure the version you installed is up-to-date: kubectl version --client Install kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64 chmod +x ./kind mv ./kind /usr/bin which kind /usr/bin/kind kind version kind v0.11.1 go1.16.4 linux/amd64 To communicate with cluster, just give the cluster name as a context in kubectl: kind create cluster --name k8s-kind-cluster1 Creating cluster \"k8s-kind-cluster1\" ... \u2713 Ensuring node image (kindest/node:v1.21.1) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-k8s-kind-cluster1\" You can now use your cluster with: kubectl cluster-info --context kind-k8s-kind-cluster1 Thanks for using kind! \ud83d\ude0a Get the cluster details: kubectl cluster-info --context kind-k8s-kind-cluster1 Kubernetes control plane is running at https://127.0.0.1:38646 CoreDNS is running at https://127.0.0.1:38646/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 5m25s kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-kind-cluster1-control-plane Ready control-plane,master 5m26s v1.21.11 Deleting a Cluster If you created a cluster with kind create cluster then deleting is equally simple: kind delete cluster","title":"Kind"},{"location":"openstack/kubernetes/kind/#kind","text":"","title":"Kind"},{"location":"openstack/kubernetes/kind/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using kind . We are using following setting for this purpose: 1 Linux machine, centos-7-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 kind\" >> /etc/hosts hostnamectl set-hostname kind","title":"Pre-requisite"},{"location":"openstack/kubernetes/kind/#install-docker-on-centos7","text":"Run the below command on the CentOS7 VM: SSH into kind machine Switch to root user: sudo su Execute the below command to initialize the cluster: yum -y install epel-release; yum -y install docker; systemctl enable --now docker; systemctl status docker docker version","title":"Install docker on CentOS7"},{"location":"openstack/kubernetes/kind/#install-kubectl-on-centos7","text":"curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" sudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl chmod +x /usr/bin/kubectl Test to ensure the version you installed is up-to-date: kubectl version --client","title":"Install kubectl on CentOS7"},{"location":"openstack/kubernetes/kind/#install-kind","text":"curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64 chmod +x ./kind mv ./kind /usr/bin which kind /usr/bin/kind kind version kind v0.11.1 go1.16.4 linux/amd64 To communicate with cluster, just give the cluster name as a context in kubectl: kind create cluster --name k8s-kind-cluster1 Creating cluster \"k8s-kind-cluster1\" ... \u2713 Ensuring node image (kindest/node:v1.21.1) \ud83d\uddbc \u2713 Preparing nodes \ud83d\udce6 \u2713 Writing configuration \ud83d\udcdc \u2713 Starting control-plane \ud83d\udd79\ufe0f \u2713 Installing CNI \ud83d\udd0c \u2713 Installing StorageClass \ud83d\udcbe Set kubectl context to \"kind-k8s-kind-cluster1\" You can now use your cluster with: kubectl cluster-info --context kind-k8s-kind-cluster1 Thanks for using kind! \ud83d\ude0a Get the cluster details: kubectl cluster-info --context kind-k8s-kind-cluster1 Kubernetes control plane is running at https://127.0.0.1:38646 CoreDNS is running at https://127.0.0.1:38646/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 5m25s kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-kind-cluster1-control-plane Ready control-plane,master 5m26s v1.21.11","title":"Install kind"},{"location":"openstack/kubernetes/kind/#deleting-a-cluster","text":"If you created a cluster with kind create cluster then deleting is equally simple: kind delete cluster","title":"Deleting a Cluster"},{"location":"openstack/kubernetes/kubernetes/","text":"Kubernetes Overview Kubernetes , commonly known as K8s is an open sourced container orchestration tool for managing containerized cloud-native workloads and services in computing, networking, and storage infrastructure. K8s can help to deploy and manage containerized applications like platforms as a service(PaaS), batch processing workers, and microservices in the cloud at scale. It reduces cloud computing costs while simplifying the operation of resilient and scalable applications. While it is possible to install and manage Kubernetes on infrastructure that you manage, it is a time-consuming and complicated process. To make provisioning and deploying clusters much easier, we have listed a number of popular platforms and tools to setup your K8s on your NERC's OpenStack Project space. Kubernetes Components & Architecture A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane or master manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance, redundancy, and high availability. Here's the diagram of a Kubernetes cluster with all the components tied together. Kubernetes Basics workflow Create a Kubernetes cluster Deploy an app Explore your app Expose your app publicly Scale up your app Update your app Development environment Minikube is a local Kubernetes cluster that focuses on making Kubernetes development and learning simple. Kubernetes may be started with just a single command if you have a Docker (or similarly comparable) container or a Virtual Machine environment. For more read this . Minishift is a tool for running OKD locally by launching a single-node OKD cluster within a virtual machine. Minishift is an open-source project forked from Minikube . Minishift is a tool that helps you run OpenShift locally by running a single-node OpenShift cluster inside a VM. For more read this . Kind is a tool for running local Kubernetes clusters utilizing Docker container \u201cnodes\u201d. It was built for Kubernetes testing, but it may also be used for local development and continuous integration. For more read this . MicroK8s is the smallest, fastest, and most conformant Kubernetes that tracks upstream releases and simplifies clustering. MicroK8s is ideal for prototyping, testing, and offline development. For more read this . K3s is a single <40MB binary, certified Kubernetes distribution developed by Rancher Labs and now a CNCF sandbox project that fully implements the Kubernetes API and is less than 40MB in size. To do so, they got rid of a lot of additional drivers that didn't need to be in the core and could easily be replaced with add-ons. For more read this . To setup a Multi-master HA K3s cluster using k3sup(pronounced ketchup ) read this . To setup a Single-Node K3s Cluster using k3d read this and if you would like to setup Multi-master K3s cluster setup using k3d read this . k0s is an all-inclusive Kubernetes distribution, configured with all of the features needed to build a Kubernetes cluster simply by copying and running an executable file on each target host. For more read this . CRC - Red Hat Code Ready Containers is a great way to experience the most recent version of OpenShift locally (CRC). CRC instals a minimum OpenShift 4.x cluster on your local machine, allowing you to create and test in a controlled environment. CRC is primarily designed for use on the workstations of programmers. For more read this . Production environment If your Kubernetes cluster has to run critical workloads, it must be configured to be resilient and higly available(HA) production-ready Kubernetes cluster. To setup production-quality cluster, you can use the following deployment tools. Kubeadm performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way. Bootstrapping cluster with kubeadm read this and if you would like to setup Multi-master cluster setup using Kubeadm read this . Kubespray helps to install a Kubernetes cluster on NERC OpenStack. Kubespray is a composition of Ansible playbooks, inventory, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. Installing Kubernetes with Kubespray read this . To choose a tool which best fits your use case, read this comparison .","title":"Kubernetes Overview"},{"location":"openstack/kubernetes/kubernetes/#kubernetes-overview","text":"Kubernetes , commonly known as K8s is an open sourced container orchestration tool for managing containerized cloud-native workloads and services in computing, networking, and storage infrastructure. K8s can help to deploy and manage containerized applications like platforms as a service(PaaS), batch processing workers, and microservices in the cloud at scale. It reduces cloud computing costs while simplifying the operation of resilient and scalable applications. While it is possible to install and manage Kubernetes on infrastructure that you manage, it is a time-consuming and complicated process. To make provisioning and deploying clusters much easier, we have listed a number of popular platforms and tools to setup your K8s on your NERC's OpenStack Project space.","title":"Kubernetes Overview"},{"location":"openstack/kubernetes/kubernetes/#kubernetes-components-architecture","text":"A Kubernetes cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane or master manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance, redundancy, and high availability. Here's the diagram of a Kubernetes cluster with all the components tied together.","title":"Kubernetes Components &amp; Architecture"},{"location":"openstack/kubernetes/kubernetes/#kubernetes-basics-workflow","text":"Create a Kubernetes cluster Deploy an app Explore your app Expose your app publicly Scale up your app Update your app","title":"Kubernetes Basics workflow"},{"location":"openstack/kubernetes/kubernetes/#development-environment","text":"Minikube is a local Kubernetes cluster that focuses on making Kubernetes development and learning simple. Kubernetes may be started with just a single command if you have a Docker (or similarly comparable) container or a Virtual Machine environment. For more read this . Minishift is a tool for running OKD locally by launching a single-node OKD cluster within a virtual machine. Minishift is an open-source project forked from Minikube . Minishift is a tool that helps you run OpenShift locally by running a single-node OpenShift cluster inside a VM. For more read this . Kind is a tool for running local Kubernetes clusters utilizing Docker container \u201cnodes\u201d. It was built for Kubernetes testing, but it may also be used for local development and continuous integration. For more read this . MicroK8s is the smallest, fastest, and most conformant Kubernetes that tracks upstream releases and simplifies clustering. MicroK8s is ideal for prototyping, testing, and offline development. For more read this . K3s is a single <40MB binary, certified Kubernetes distribution developed by Rancher Labs and now a CNCF sandbox project that fully implements the Kubernetes API and is less than 40MB in size. To do so, they got rid of a lot of additional drivers that didn't need to be in the core and could easily be replaced with add-ons. For more read this . To setup a Multi-master HA K3s cluster using k3sup(pronounced ketchup ) read this . To setup a Single-Node K3s Cluster using k3d read this and if you would like to setup Multi-master K3s cluster setup using k3d read this . k0s is an all-inclusive Kubernetes distribution, configured with all of the features needed to build a Kubernetes cluster simply by copying and running an executable file on each target host. For more read this . CRC - Red Hat Code Ready Containers is a great way to experience the most recent version of OpenShift locally (CRC). CRC instals a minimum OpenShift 4.x cluster on your local machine, allowing you to create and test in a controlled environment. CRC is primarily designed for use on the workstations of programmers. For more read this .","title":"Development environment"},{"location":"openstack/kubernetes/kubernetes/#production-environment","text":"If your Kubernetes cluster has to run critical workloads, it must be configured to be resilient and higly available(HA) production-ready Kubernetes cluster. To setup production-quality cluster, you can use the following deployment tools. Kubeadm performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way. Bootstrapping cluster with kubeadm read this and if you would like to setup Multi-master cluster setup using Kubeadm read this . Kubespray helps to install a Kubernetes cluster on NERC OpenStack. Kubespray is a composition of Ansible playbooks, inventory, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. Installing Kubernetes with Kubespray read this . To choose a tool which best fits your use case, read this comparison .","title":"Production environment"},{"location":"openstack/kubernetes/kubespray/","text":"Kubespray Pre-requisite We will need 1 control-plane(master) and 1 worker node to create a single control-plane kubernetes cluster using Kubespray . We are using following setting for this purpose: 1 Linux machine for Ansible master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage. 1 Linux machine for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to the master node. 1 Linux machines for worker, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. To allow SSH from Ansible master to all other nodes : Read more here Generate SSH key for Ansible master node using: ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:OMsKP7EmhT400AJA/KN1smKt6eTaa3QFQUiepmj8dxroot@ansible-master The key's randomart image is: +---[RSA 3072]----+ |=o.oo. | |.o... | |..= . | |=o.= ... | |o=+.=.o SE | |.+*o+. o. . | |.=== +o. . | |o+=o=.. | |++o=o. | +----[SHA256]-----+ Copy and append the content of SSH public key i.e. ~/.ssh/id_rsa.pub to other nodes's ~/.ssh/authorized_keys file. This will allow ssh <other_nodes_internal_ip> from the Ansible master node's terminal. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.224 ansible_master\" >> /etc/hosts hostnamectl set-hostname ansible_master In this step, you will update packages and disable swap on the all 3 nodes: i. 1 Ansible Master Node - ansible_master ii. 1 Kubernetes Master Node - kubspray_master iii.1 Kubernetes Worker Node - kubspray_worker1 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Configure Kubespray on ansible_master node using Ansible Playbook Run the below command on the master node i.e. master that you want to setup as control plane. SSH into ansible_master machine Switch to root user: sudo su Execute the below command to initialize the cluster: Install Python3 and upgrade pip to pip3: apt install python3-pip -y pip3 install --upgrade pip python3 -V && pip3 -V pip -V Clone the Kubespray git repository: git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray Install dependencies from requirements.txt : pip install -r requirements.txt Copy inventory/sample as inventory/mycluster cp -rfp inventory/sample inventory/mycluster Update Ansible inventory file with inventory builder This step is little trivial because we need to update hosts.yml with the nodes IP. Now we are going to declare a variable \"IPS\" for storing the IP address of other K8s nodes .i.e. kubspray_master(192.168.0.130), kubspray_worker1(192.168.0.32) declare -a IPS=(192.168.0.189 192.168.0.116) CONFIG_FILE=inventory/mycluster/hosts.yml python3 \\ contrib/inventory_builder/inventory.py ${IPS[@]} DEBUG: Adding group all DEBUG: Adding group kube_control_plane DEBUG: Adding group kube_node DEBUG: Adding group etcd DEBUG: Adding group k8s_cluster DEBUG: Adding group calico_rr DEBUG: adding host node1 to group all DEBUG: adding host node2 to group all DEBUG: adding host node1 to group etcd DEBUG: adding host node1 to group kube_control_plane DEBUG: adding host node2 to group kube_control_plane DEBUG: adding host node1 to group kube_node DEBUG: adding host node2 to group kube_node After running the above commands do verify the hosts.yml and its content: cat inventory/mycluster/hosts.yml The content of the hosts.yml file should looks like: all: hosts: node1: ansible_host: 192.168.0.76 ip: 192.168.0.76 access_ip: 192.168.0.76 node2: ansible_host: 192.168.0.176 ip: 192.168.0.176 access_ip: 192.168.0.176 children: kube_control_plane: hosts: node1: node2: kube_node: hosts: node1: node2: etcd: hosts: node1: k8s_cluster: children: kube_control_plane: kube_node: calico_rr: hosts: {} Review and change parameters under inventory/mycluster/group_vars cat inventory/mycluster/group_vars/all/all.yml cat inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml It can be useful to set the following two variables to true in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : kubeconfig_localhost (to make a copy of kubeconfig on the host that runs Ansible in { inventory_dir }/artifacts ) and kubectl_localhost (to download kubectl onto the host that runs Ansible in { bin_dir } ). Very Important As Ubuntu 20 kvm kernel doesn't have dummy module we need to modify the following two variables in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : enable_nodelocaldns: false and kube_proxy_mode: iptables which will Disable nodelocal dns cache and Kube-proxy proxyMode to iptables respectively. Deploy Kubespray with Ansible Playbook - run the playbook as root user. The option --become is required, as for example writing SSL keys in /etc/ , installing packages and interacting with various systemd daemons. Without --become the playbook will fail to run! ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml Note Running ansible playbook takes little time because it depends on the network bandwidth also. Install kubectl on Kubernetes master node .i.e. kubspray_master Install kubectl binary snap install kubectl --classic kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml Validate all cluster components and nodes are visible on all nodes Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION kubspray_master NotReady control-plane,master 21m v1.16.2 kubspray_worker1 Ready <none> 9m17s v1.16.2 Deploy A Hello Minikube Application Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=LoadBalancer --port=8080 service/hello-minikube exposed View the deployments information: kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-minikube 1/1 1 1 50s View the port information: kubectl get svc hello-minikube NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-minikube LoadBalancer 10.233.35.126 <pending> 8080:30723/TCP 40s View the web url for the service: minikube service hello-minikube --url Expose the service locally kubectl port-forward svc/hello-minikube 30723:8080 Forwarding from [::1]:30723 -> 8080 Forwarding from 127.0.0.1:30723 -> 8080 Handling connection for 30723 Handling connection for 30723 Go to browser, visit http://<Master-Floating-IP>:8080 i.e. http://140.247.152.235:8080/ to check the hello minikube default page. Clean up Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube","title":"Kubespray"},{"location":"openstack/kubernetes/kubespray/#kubespray","text":"","title":"Kubespray"},{"location":"openstack/kubernetes/kubespray/#pre-requisite","text":"We will need 1 control-plane(master) and 1 worker node to create a single control-plane kubernetes cluster using Kubespray . We are using following setting for this purpose: 1 Linux machine for Ansible master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage. 1 Linux machine for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to the master node. 1 Linux machines for worker, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. To allow SSH from Ansible master to all other nodes : Read more here Generate SSH key for Ansible master node using: ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:OMsKP7EmhT400AJA/KN1smKt6eTaa3QFQUiepmj8dxroot@ansible-master The key's randomart image is: +---[RSA 3072]----+ |=o.oo. | |.o... | |..= . | |=o.= ... | |o=+.=.o SE | |.+*o+. o. . | |.=== +o. . | |o+=o=.. | |++o=o. | +----[SHA256]-----+ Copy and append the content of SSH public key i.e. ~/.ssh/id_rsa.pub to other nodes's ~/.ssh/authorized_keys file. This will allow ssh <other_nodes_internal_ip> from the Ansible master node's terminal. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.224 ansible_master\" >> /etc/hosts hostnamectl set-hostname ansible_master In this step, you will update packages and disable swap on the all 3 nodes: i. 1 Ansible Master Node - ansible_master ii. 1 Kubernetes Master Node - kubspray_master iii.1 Kubernetes Worker Node - kubspray_worker1 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab","title":"Pre-requisite"},{"location":"openstack/kubernetes/kubespray/#configure-kubespray-on-ansible_master-node-using-ansible-playbook","text":"Run the below command on the master node i.e. master that you want to setup as control plane. SSH into ansible_master machine Switch to root user: sudo su Execute the below command to initialize the cluster: Install Python3 and upgrade pip to pip3: apt install python3-pip -y pip3 install --upgrade pip python3 -V && pip3 -V pip -V Clone the Kubespray git repository: git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray Install dependencies from requirements.txt : pip install -r requirements.txt Copy inventory/sample as inventory/mycluster cp -rfp inventory/sample inventory/mycluster Update Ansible inventory file with inventory builder This step is little trivial because we need to update hosts.yml with the nodes IP. Now we are going to declare a variable \"IPS\" for storing the IP address of other K8s nodes .i.e. kubspray_master(192.168.0.130), kubspray_worker1(192.168.0.32) declare -a IPS=(192.168.0.189 192.168.0.116) CONFIG_FILE=inventory/mycluster/hosts.yml python3 \\ contrib/inventory_builder/inventory.py ${IPS[@]} DEBUG: Adding group all DEBUG: Adding group kube_control_plane DEBUG: Adding group kube_node DEBUG: Adding group etcd DEBUG: Adding group k8s_cluster DEBUG: Adding group calico_rr DEBUG: adding host node1 to group all DEBUG: adding host node2 to group all DEBUG: adding host node1 to group etcd DEBUG: adding host node1 to group kube_control_plane DEBUG: adding host node2 to group kube_control_plane DEBUG: adding host node1 to group kube_node DEBUG: adding host node2 to group kube_node After running the above commands do verify the hosts.yml and its content: cat inventory/mycluster/hosts.yml The content of the hosts.yml file should looks like: all: hosts: node1: ansible_host: 192.168.0.76 ip: 192.168.0.76 access_ip: 192.168.0.76 node2: ansible_host: 192.168.0.176 ip: 192.168.0.176 access_ip: 192.168.0.176 children: kube_control_plane: hosts: node1: node2: kube_node: hosts: node1: node2: etcd: hosts: node1: k8s_cluster: children: kube_control_plane: kube_node: calico_rr: hosts: {} Review and change parameters under inventory/mycluster/group_vars cat inventory/mycluster/group_vars/all/all.yml cat inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml It can be useful to set the following two variables to true in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : kubeconfig_localhost (to make a copy of kubeconfig on the host that runs Ansible in { inventory_dir }/artifacts ) and kubectl_localhost (to download kubectl onto the host that runs Ansible in { bin_dir } ). Very Important As Ubuntu 20 kvm kernel doesn't have dummy module we need to modify the following two variables in inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml : enable_nodelocaldns: false and kube_proxy_mode: iptables which will Disable nodelocal dns cache and Kube-proxy proxyMode to iptables respectively. Deploy Kubespray with Ansible Playbook - run the playbook as root user. The option --become is required, as for example writing SSL keys in /etc/ , installing packages and interacting with various systemd daemons. Without --become the playbook will fail to run! ansible-playbook -i inventory/mycluster/hosts.yml --become --become-user=root cluster.yml Note Running ansible playbook takes little time because it depends on the network bandwidth also.","title":"Configure Kubespray on ansible_master node using Ansible Playbook"},{"location":"openstack/kubernetes/kubespray/#install-kubectl-on-kubernetes-master-node-ie-kubspray_master","text":"Install kubectl binary snap install kubectl --classic kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml","title":"Install kubectl on Kubernetes master node .i.e. kubspray_master"},{"location":"openstack/kubernetes/kubespray/#validate-all-cluster-components-and-nodes-are-visible-on-all-nodes","text":"Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION kubspray_master NotReady control-plane,master 21m v1.16.2 kubspray_worker1 Ready <none> 9m17s v1.16.2","title":"Validate all cluster components and nodes are visible on all nodes"},{"location":"openstack/kubernetes/kubespray/#deploy-a-hello-minikube-application","text":"Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=LoadBalancer --port=8080 service/hello-minikube exposed View the deployments information: kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE hello-minikube 1/1 1 1 50s View the port information: kubectl get svc hello-minikube NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-minikube LoadBalancer 10.233.35.126 <pending> 8080:30723/TCP 40s View the web url for the service: minikube service hello-minikube --url Expose the service locally kubectl port-forward svc/hello-minikube 30723:8080 Forwarding from [::1]:30723 -> 8080 Forwarding from 127.0.0.1:30723 -> 8080 Handling connection for 30723 Handling connection for 30723 Go to browser, visit http://<Master-Floating-IP>:8080 i.e. http://140.247.152.235:8080/ to check the hello minikube default page.","title":"Deploy A Hello Minikube Application"},{"location":"openstack/kubernetes/kubespray/#clean-up","text":"Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube","title":"Clean up"},{"location":"openstack/kubernetes/microk8s/","text":"Microk8s Pre-requisite We will need 1 VM to create a single node kubernetes cluster using microk8s . We are using following setting for this purpose: 1 Linux machine, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 microk8s\" >> /etc/hosts hostnamectl set-hostname microk8s Install MicroK8s on Ubuntu Run the below command on the Ubuntu VM: SSH into microk8s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install MicroK8s: sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Turn on the services you want: microk8s enable dashboard dns registry istio Try microk8s enable --help for a list of available services and optional features. microk8s disable <name> turns off a service. Start using Kubernetes microk8s kubectl get all --all-namespaces If you mainly use MicroK8s you can make our kubectl the default one on your command-line with alias mkctl=\"microk8s kubectl\" . Since it is a standard upstream kubectl, you can also drive other Kubernetes clusters with it by pointing to the respective kubeconfig file via the --kubeconfig argument. Access the Kubernetes dashboard UI: As we see above the kubernetes-dashboard service in the kube-system namespace has a ClusterIP of 10.152.183.73 and listens on TCP port 443. The ClusterIP is randomly assigned, so if you follow these steps on your host, make sure you check the IP adress you got. microk8s dashboard-proxy Checking if Dashboard is running. Dashboard will be available at https://127.0.0.1:10443 Use the following token to login: eyJhbGc.... OR, To access the dashboard use the default token retrieved with: token=$(microk8s kubectl -n kube-system get secret | grep default-token | \\ cut -d \" \" -f1) microk8s kubectl -n kube-system describe secret $token This will show the token to login to the Dashbord shown on the url with NodePort. Start and stop Kubernetes: Kubernetes is a collection of system services that talk to each other all the time. If you don\u2019t need them running in the background then you will save battery by stopping them. microk8s start and microk8s stop will those tasks for you. To Reset the infrastructure to a clean state: microk8s reset Deploy A Sample Nginx Application Create an alias: alias mkctl=\"microk8s kubectl\" Create a deployment, in this case Nginx : mkctl create deployment --image nginx my-nginx To access the deployment we will need to expose it: mkctl expose deployment my-nginx --port=80 --type=NodePort mkctl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx NodePort 10.152.183.41 <none> 80:31225/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31225/ to check the nginx default page. Deploy Another Application You can start by creating a microbot deployment with two pods via the kubectl cli: mkctl create deployment microbot --image=dontrebootme/microbot:v1 mkctl scale deployment microbot --replicas=2 To expose the deployment to NodePort, you need to create a service: mkctl expose deployment microbot --type=NodePort --port=80 --name=microbot-service View the port information: mkctl get svc microbot-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE microbot-service NodePort 10.152.183.8 <none> 80:31442/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31442/ to check the microbot default page.","title":"MicroK8s"},{"location":"openstack/kubernetes/microk8s/#microk8s","text":"","title":"Microk8s"},{"location":"openstack/kubernetes/microk8s/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using microk8s . We are using following setting for this purpose: 1 Linux machine, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 microk8s\" >> /etc/hosts hostnamectl set-hostname microk8s","title":"Pre-requisite"},{"location":"openstack/kubernetes/microk8s/#install-microk8s-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into microk8s machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install MicroK8s: sudo snap install microk8s --classic Check the status while Kubernetes starts microk8s status --wait-ready Turn on the services you want: microk8s enable dashboard dns registry istio Try microk8s enable --help for a list of available services and optional features. microk8s disable <name> turns off a service. Start using Kubernetes microk8s kubectl get all --all-namespaces If you mainly use MicroK8s you can make our kubectl the default one on your command-line with alias mkctl=\"microk8s kubectl\" . Since it is a standard upstream kubectl, you can also drive other Kubernetes clusters with it by pointing to the respective kubeconfig file via the --kubeconfig argument. Access the Kubernetes dashboard UI: As we see above the kubernetes-dashboard service in the kube-system namespace has a ClusterIP of 10.152.183.73 and listens on TCP port 443. The ClusterIP is randomly assigned, so if you follow these steps on your host, make sure you check the IP adress you got. microk8s dashboard-proxy Checking if Dashboard is running. Dashboard will be available at https://127.0.0.1:10443 Use the following token to login: eyJhbGc.... OR, To access the dashboard use the default token retrieved with: token=$(microk8s kubectl -n kube-system get secret | grep default-token | \\ cut -d \" \" -f1) microk8s kubectl -n kube-system describe secret $token This will show the token to login to the Dashbord shown on the url with NodePort. Start and stop Kubernetes: Kubernetes is a collection of system services that talk to each other all the time. If you don\u2019t need them running in the background then you will save battery by stopping them. microk8s start and microk8s stop will those tasks for you. To Reset the infrastructure to a clean state: microk8s reset","title":"Install MicroK8s on Ubuntu"},{"location":"openstack/kubernetes/microk8s/#deploy-a-sample-nginx-application","text":"Create an alias: alias mkctl=\"microk8s kubectl\" Create a deployment, in this case Nginx : mkctl create deployment --image nginx my-nginx To access the deployment we will need to expose it: mkctl expose deployment my-nginx --port=80 --type=NodePort mkctl get svc my-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx NodePort 10.152.183.41 <none> 80:31225/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31225/ to check the nginx default page.","title":"Deploy A Sample Nginx Application"},{"location":"openstack/kubernetes/microk8s/#deploy-another-application","text":"You can start by creating a microbot deployment with two pods via the kubectl cli: mkctl create deployment microbot --image=dontrebootme/microbot:v1 mkctl scale deployment microbot --replicas=2 To expose the deployment to NodePort, you need to create a service: mkctl expose deployment microbot --type=NodePort --port=80 --name=microbot-service View the port information: mkctl get svc microbot-service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE microbot-service NodePort 10.152.183.8 <none> 80:31442/TCP 35h Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://128.31.26.4:31442/ to check the microbot default page.","title":"Deploy Another Application"},{"location":"openstack/kubernetes/minikube/","text":"Minikube Minimum system requirements for minikube 2 GB RAM or more 2 CPU / vCPUs or more 20 GB free hard disk space or more Docker / Virtual Machine Manager \u2013 KVM & VirtualBox. Docker, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMWare are examples of container or virtual machine managers. Pre-requisite We will need 1 VM to create a single node kubernetes cluster using minikube . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 minikube\" >> /etc/hosts hostnamectl set-hostname minikube Install Minikube on Ubuntu Run the below command on the Ubuntu VM: SSH into minikube machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl , wget , and apt-transport-https apt-get update && apt-get install -y curl wget apt-transport-https Install Docker Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker OR, you can install VirtualBox Hypervisor as runtime: sudo apt install virtualbox virtualbox-ext-pack -y Install kubectl Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml Installing minikube Install minikube curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb sudo dpkg -i minikube_latest_amd64.deb OR, install minikube using wget : wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 cp minikube-linux-amd64 /usr/bin/minikube chmod +x /usr/bin/minikube Verify the Minikube installation: minikube version minikube version: v1.23.2 commit: 0a0ad764652082477c00d51d2475284b5d39ceed Install conntrack: Kubernetes 1.22.2 requires conntrack to be installed in root's path: apt-get install -y conntrack Start minikube: As we are already stated in the beginning that we would be using docker as base for minikue, so start the minikube with the docker driver, minikube start --driver=none Note To check the internal IP, run the minikube ip command. By default, Minikube uses the driver most relevant to the host OS. To use a different driver, set the --driver flag in minikube start . For example, to use Docker instead of others or none, run minikube start --driver=docker . To persistent configuration so that you to run minikube start without explicitly passing i.e. in global scope the --vm-driver docker flag each time, run: minikube config set vm-driver docker . In case you want to start minikube with customize resources and want installer to automatically select the driver then you can run following command, minikube start --addons=ingress --cpus=2 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g Output would like below: Perfect, above confirms that minikube cluster has been configured and started successfully. Run below minikube command to check status: minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Run following kubectl command to verify the cluster info and node status: kubectl cluster-info Kubernetes control plane is running at https://192.168.0.62:8443 CoreDNS is running at https://192.168.0.62:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready control-plane,master 5m v1.22.2 To see the kubectl configuration use the command: kubectl config view The output looks like: Get minikube addon details: minikube addons list The output will display like below: If you wish to enable any addons run the below minikube command, minikube addons enable <addon-name> Enable minikube dashboard addon: minikube dashboard \ud83d\udd0c Enabling dashboard ... \u25aa Using image kubernetesui/metrics-scraper:v1.0.7 \u25aa Using image kubernetesui/dashboard:v2.3.1 \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:40783/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ To view minikube dashboard url: minikube dashboard --url \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:42669/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ Expose Dashboard on NodePort instead of ClusterIP : -- Check the current port for kubernetes-dashboard : kubectl get services -n kubernetes-dashboard The output looks like below: kubectl edit service kubernetes-dashboard -n kubernetes-dashboard -- Replace type: \"ClusterIP\" to \"NodePort\": -- After saving the file: Test again: kubectl get services -n kubernetes-dashboard Now the output should look like below: So, now you can browser the K8s Dashboard, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31881 to view the Dashboard. Deploy A Sample Nginx Application Create a deployment, in this case Nginx : A Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking. The Pod in this tutorial has only one Container. A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates. Deployments are the recommended way to manage the creation and scaling of Pods. Let's check if the Kubernetes cluster is up and running: kubectl get all --all-namespaces kubectl get po -A kubectl get nodes kubectl create deployment --image nginx my-nginx To access the deployment we will need to expose it: kubectl expose deployment my-nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: OR, minikube service list |----------------------|---------------------------|--------------|-------------| | NAMESPACE | NAME | TARGET PORT | URL | |----------------------|---------------------------|--------------|-------------| | default | kubernetes | No node port | | default | my-nginx | 80 | http:.:31081| | kube-system | kube-dns | No node port | | kubernetes-dashboard | dashboard-metrics-scraper | No node port | | kubernetes-dashboard | kubernetes-dashboard | 80 | http:.:31929| |----------------------|---------------------------|--------------|-------------| OR, kubectl get svc my-nginx minikube service my-nginx --url Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from the node's Floating IP. Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31081/ to check the nginx default page. For your example, Deploy A Hello Minikube Application Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 View the port information: kubectl get svc hello-minikube minikube service hello-minikube --url Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31293/ to check the hello minikube default page. For your example, Clean up Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube Managing Minikube Cluster To stop the minikube, run minikube stop To delete the single node cluster: minikube delete To Start the minikube, run minikube start In case you want to start the minikube with higher resource like 8 GB RM and 4 CPU then execute following commands one after the another. minikube config set cpus 4 minikube config set memory 8192 minikube delete minikube start","title":"Minikube"},{"location":"openstack/kubernetes/minikube/#minikube","text":"","title":"Minikube"},{"location":"openstack/kubernetes/minikube/#minimum-system-requirements-for-minikube","text":"2 GB RAM or more 2 CPU / vCPUs or more 20 GB free hard disk space or more Docker / Virtual Machine Manager \u2013 KVM & VirtualBox. Docker, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox, or VMWare are examples of container or virtual machine managers.","title":"Minimum system requirements for minikube"},{"location":"openstack/kubernetes/minikube/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using minikube . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.62 minikube\" >> /etc/hosts hostnamectl set-hostname minikube","title":"Pre-requisite"},{"location":"openstack/kubernetes/minikube/#install-minikube-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into minikube machine Switch to root user: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl , wget , and apt-transport-https apt-get update && apt-get install -y curl wget apt-transport-https","title":"Install Minikube on Ubuntu"},{"location":"openstack/kubernetes/minikube/#install-docker","text":"Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker OR, you can install VirtualBox Hypervisor as runtime: sudo apt install virtualbox virtualbox-ext-pack -y","title":"Install Docker"},{"location":"openstack/kubernetes/minikube/#install-kubectl","text":"Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml","title":"Install kubectl"},{"location":"openstack/kubernetes/minikube/#installing-minikube","text":"Install minikube curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb sudo dpkg -i minikube_latest_amd64.deb OR, install minikube using wget : wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 cp minikube-linux-amd64 /usr/bin/minikube chmod +x /usr/bin/minikube Verify the Minikube installation: minikube version minikube version: v1.23.2 commit: 0a0ad764652082477c00d51d2475284b5d39ceed Install conntrack: Kubernetes 1.22.2 requires conntrack to be installed in root's path: apt-get install -y conntrack Start minikube: As we are already stated in the beginning that we would be using docker as base for minikue, so start the minikube with the docker driver, minikube start --driver=none Note To check the internal IP, run the minikube ip command. By default, Minikube uses the driver most relevant to the host OS. To use a different driver, set the --driver flag in minikube start . For example, to use Docker instead of others or none, run minikube start --driver=docker . To persistent configuration so that you to run minikube start without explicitly passing i.e. in global scope the --vm-driver docker flag each time, run: minikube config set vm-driver docker . In case you want to start minikube with customize resources and want installer to automatically select the driver then you can run following command, minikube start --addons=ingress --cpus=2 --cni=flannel --install-addons=true --kubernetes-version=stable --memory=6g Output would like below: Perfect, above confirms that minikube cluster has been configured and started successfully. Run below minikube command to check status: minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Run following kubectl command to verify the cluster info and node status: kubectl cluster-info Kubernetes control plane is running at https://192.168.0.62:8443 CoreDNS is running at https://192.168.0.62:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready control-plane,master 5m v1.22.2 To see the kubectl configuration use the command: kubectl config view The output looks like: Get minikube addon details: minikube addons list The output will display like below: If you wish to enable any addons run the below minikube command, minikube addons enable <addon-name> Enable minikube dashboard addon: minikube dashboard \ud83d\udd0c Enabling dashboard ... \u25aa Using image kubernetesui/metrics-scraper:v1.0.7 \u25aa Using image kubernetesui/dashboard:v2.3.1 \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:40783/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ To view minikube dashboard url: minikube dashboard --url \ud83e\udd14 Verifying dashboard health ... \ud83d\ude80 Launching proxy ... \ud83e\udd14 Verifying proxy health ... http://127.0.0.1:42669/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ Expose Dashboard on NodePort instead of ClusterIP : -- Check the current port for kubernetes-dashboard : kubectl get services -n kubernetes-dashboard The output looks like below: kubectl edit service kubernetes-dashboard -n kubernetes-dashboard -- Replace type: \"ClusterIP\" to \"NodePort\": -- After saving the file: Test again: kubectl get services -n kubernetes-dashboard Now the output should look like below: So, now you can browser the K8s Dashboard, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31881 to view the Dashboard.","title":"Installing minikube"},{"location":"openstack/kubernetes/minikube/#deploy-a-sample-nginx-application","text":"Create a deployment, in this case Nginx : A Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking. The Pod in this tutorial has only one Container. A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates. Deployments are the recommended way to manage the creation and scaling of Pods. Let's check if the Kubernetes cluster is up and running: kubectl get all --all-namespaces kubectl get po -A kubectl get nodes kubectl create deployment --image nginx my-nginx To access the deployment we will need to expose it: kubectl expose deployment my-nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: OR, minikube service list |----------------------|---------------------------|--------------|-------------| | NAMESPACE | NAME | TARGET PORT | URL | |----------------------|---------------------------|--------------|-------------| | default | kubernetes | No node port | | default | my-nginx | 80 | http:.:31081| | kube-system | kube-dns | No node port | | kubernetes-dashboard | dashboard-metrics-scraper | No node port | | kubernetes-dashboard | kubernetes-dashboard | 80 | http:.:31929| |----------------------|---------------------------|--------------|-------------| OR, kubectl get svc my-nginx minikube service my-nginx --url Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from the node's Floating IP. Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31081/ to check the nginx default page. For your example,","title":"Deploy A Sample Nginx Application"},{"location":"openstack/kubernetes/minikube/#deploy-a-hello-minikube-application","text":"Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image. kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4 kubectl expose deployment hello-minikube --type=NodePort --port=8080 View the port information: kubectl get svc hello-minikube minikube service hello-minikube --url Go to browser, visit http://<Floating-IP>:<NodePort> i.e. http://140.247.152.235:31293/ to check the hello minikube default page. For your example,","title":"Deploy A Hello Minikube Application"},{"location":"openstack/kubernetes/minikube/#clean-up","text":"Now you can clean up the app resources you created in your cluster: kubectl delete service my-nginx kubectl delete deployment my-nginx kubectl delete service hello-minikube kubectl delete deployment hello-minikube","title":"Clean up"},{"location":"openstack/kubernetes/minikube/#managing-minikube-cluster","text":"To stop the minikube, run minikube stop To delete the single node cluster: minikube delete To Start the minikube, run minikube start In case you want to start the minikube with higher resource like 8 GB RM and 4 CPU then execute following commands one after the another. minikube config set cpus 4 minikube config set memory 8192 minikube delete minikube start","title":"Managing Minikube Cluster"},{"location":"openstack/kubernetes/minishift/","text":"Minishift What Is the OKD Architecture? OKD is a platform for developing and running containerized applications. OKD has a microservices-based architecture of smaller, decoupled units that work together. It runs on top of a Kubernetes cluster, with data about the objects stored in etcd, a reliable clustered key-value store. Those services are broken down by function: REST APIs, which expose each of the core objects. Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object. For more information on the node types in the architecture overview, see Kubernetes Infrastructure . Minishift Quickstart OKD also offers a comprehensive web console and the custom OpenShift CLI (oc) interface. The interaction with OpenShift is with the command line tool oc which is copied to your host. Pre-requisite We will need 1 VM to create a single node kubernetes cluster using minishift . We are using following setting for this purpose: 1 Linux machine, ubuntu-21.04-x86_64, m1.large flavor with 4vCPU, 8GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.134 minishift\" >> /etc/hosts hostnamectl set-hostname minishift Install MiniShift on Ubuntu Run the below command on the Ubuntu VM: SSH into minishift machine Switch to root user: sudo su You will need to set up the virtualization environment before installing MiniShift. Update the repositories and packages: apt-get update && apt-get upgrade -y Install libvirt and qemu-kvm on your system: apt install qemu-kvm libvirt-daemon libvirt-daemon-system -y Create group if does not exist: addgroup libvirtd adduser $(whoami) libvirtd Add yourself to the libvirt(d) group: usermod -a -G libvirt $(whoami) OR, usermod -a -G libvirt $USER Update your current session to apply the group change: newgrp libvirtd As root, install the KVM driver binary and make it executable as follows: curl -L https://github.com/dhiltgen/docker-machine-kvm/releases/download/v0.10.0/docker-machine-driver-kvm-ubuntu16.04 -o /usr/bin/docker-machine-driver-kvm chmod +x /usr/bin/docker-machine-driver-kvm Check the status of libvirtd : systemctl is-active libvirtd If libvirtd is not active, start the libvirtd service: systemctl start libvirtd Download the archive for your operating system from the Minishift Releases page. curl -LO https://github.com/minishift/minishift/releases/download/v1.34.3/minishift-1.34.3-linux-amd64.tgz Unzip and Copy MiniShift in your path tar -zxvf minishift-1.34.3-linux-amd64.tgz --strip=1 -C/usr/bin minishift-1.34.3-linux-amd64/minishift Starting Minishift By running the command: minishift start !!!note \"Note\": cpu, and start memory, If you do not specify a disk-size 2vCPU, 4GB memory, with 20GB disk. Other customized minishift can be started following this format: minishift start --openshift-version v3.11.0 --iso-url centos --cpus 4 \\ --memory 12GB --disk-size 60GB minishift start -- Starting local OpenShift cluster using 'kvm' hypervisor... ... OpenShift server started. The server is accessible via web console at: https://192.168.42.226:8443/console You are logged in as: User: developer Password: <any value> #pragma: allowlist secret To login as administrator: oc login -u system:admin Note The IP is dynamically generated for each OpenShift cluster. To check the IP, run the minishift ip command. By default, Minishift uses the driver most relevant to the host OS. To use a different driver, set the --vm-driver flag in minishift start . For example, to use VirtualBox instead of KVM on Linux operating systems, run minishift start --vm-driver=virtualbox . To persistent configuration so that you to run minishift start without explicitly passing i.e. in global scope the --vm-driver virtualbox flag each time, run: minishift config set vm-driver virtualbox . You can run this command in a shell after starting Minishift to get the URL of the Web console: minishift console --url Use minishift oc-env to display the command you need to type into your shell in order to add the oc binary to your PATH environment variable. The output of oc-env will differ depending on OS and shell type. $ minishift oc-env export PATH=\"/root/.minishift/cache/oc/v3.11.0/linux:$PATH\" # Run this command to configure your shell: # eval $(minishift oc-env) Deploying a Sample Application OpenShift provides various sample applications, such as templates, builder applications, and quickstarts. The following steps describe how to deploy a sample Node.js application from the command line. Create a Node.js example app: oc new-app https://github.com/sclorg/nodejs-ex -l name=myapp Track the build log until the app is built and deployed: oc logs -f bc/nodejs-ex Expose a route to the service: oc expose svc/nodejs-ex Access the application: minishift openshift service nodejs-ex --in-browser To stop Minishift, use the following command: minishift stop Stopping local OpenShift cluster... Stopping \"minishift\"... Updating Minishift minishift update Uninstalling Minishift Delete the Minishift VM and any VM-specific files: minishift delete This command deletes everything in the $MINISHIFT_HOME/.minishift/machines/minishift directory. Other cached data and the persistent configuration are not removed. To completely uninstall Minishift, delete everything in the MINISHIFT_HOME directory (default ~/.minishift ) and ~/.kube : rm -rf ~/.minishift rm -rf ~/.kube With your hypervisor management tool, confirm that there are no remaining artifacts related to the Minishift VM. For example, if you use KVM, you need to run the virsh command. Connecting to the Minishift VM with SSH You can use the minishift ssh command to interact with the Minishift VM. minishift ssh -- docker ps CONTAINER IMAGE COMMAND CREATED STATUS NAMES 71fe8ff16548 openshift/origin:... \"/usr/bin/openshift s\" 1 sec ago Up 1 second origin","title":"Minishift"},{"location":"openstack/kubernetes/minishift/#minishift","text":"","title":"Minishift"},{"location":"openstack/kubernetes/minishift/#what-is-the-okd-architecture","text":"OKD is a platform for developing and running containerized applications. OKD has a microservices-based architecture of smaller, decoupled units that work together. It runs on top of a Kubernetes cluster, with data about the objects stored in etcd, a reliable clustered key-value store. Those services are broken down by function: REST APIs, which expose each of the core objects. Controllers, which read those APIs, apply changes to other objects, and report status or write back to the object. For more information on the node types in the architecture overview, see Kubernetes Infrastructure .","title":"What Is the OKD Architecture?"},{"location":"openstack/kubernetes/minishift/#minishift-quickstart","text":"OKD also offers a comprehensive web console and the custom OpenShift CLI (oc) interface. The interaction with OpenShift is with the command line tool oc which is copied to your host.","title":"Minishift Quickstart"},{"location":"openstack/kubernetes/minishift/#pre-requisite","text":"We will need 1 VM to create a single node kubernetes cluster using minishift . We are using following setting for this purpose: 1 Linux machine, ubuntu-21.04-x86_64, m1.large flavor with 4vCPU, 8GB RAM, 10GB storage - also assign Floating IP to this VM. setup Unique hostname to the machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.134 minishift\" >> /etc/hosts hostnamectl set-hostname minishift","title":"Pre-requisite"},{"location":"openstack/kubernetes/minishift/#install-minishift-on-ubuntu","text":"Run the below command on the Ubuntu VM: SSH into minishift machine Switch to root user: sudo su You will need to set up the virtualization environment before installing MiniShift. Update the repositories and packages: apt-get update && apt-get upgrade -y Install libvirt and qemu-kvm on your system: apt install qemu-kvm libvirt-daemon libvirt-daemon-system -y Create group if does not exist: addgroup libvirtd adduser $(whoami) libvirtd Add yourself to the libvirt(d) group: usermod -a -G libvirt $(whoami) OR, usermod -a -G libvirt $USER Update your current session to apply the group change: newgrp libvirtd As root, install the KVM driver binary and make it executable as follows: curl -L https://github.com/dhiltgen/docker-machine-kvm/releases/download/v0.10.0/docker-machine-driver-kvm-ubuntu16.04 -o /usr/bin/docker-machine-driver-kvm chmod +x /usr/bin/docker-machine-driver-kvm Check the status of libvirtd : systemctl is-active libvirtd If libvirtd is not active, start the libvirtd service: systemctl start libvirtd Download the archive for your operating system from the Minishift Releases page. curl -LO https://github.com/minishift/minishift/releases/download/v1.34.3/minishift-1.34.3-linux-amd64.tgz Unzip and Copy MiniShift in your path tar -zxvf minishift-1.34.3-linux-amd64.tgz --strip=1 -C/usr/bin minishift-1.34.3-linux-amd64/minishift","title":"Install MiniShift on Ubuntu"},{"location":"openstack/kubernetes/minishift/#starting-minishift","text":"By running the command: minishift start !!!note \"Note\": cpu, and start memory, If you do not specify a disk-size 2vCPU, 4GB memory, with 20GB disk. Other customized minishift can be started following this format: minishift start --openshift-version v3.11.0 --iso-url centos --cpus 4 \\ --memory 12GB --disk-size 60GB minishift start -- Starting local OpenShift cluster using 'kvm' hypervisor... ... OpenShift server started. The server is accessible via web console at: https://192.168.42.226:8443/console You are logged in as: User: developer Password: <any value> #pragma: allowlist secret To login as administrator: oc login -u system:admin Note The IP is dynamically generated for each OpenShift cluster. To check the IP, run the minishift ip command. By default, Minishift uses the driver most relevant to the host OS. To use a different driver, set the --vm-driver flag in minishift start . For example, to use VirtualBox instead of KVM on Linux operating systems, run minishift start --vm-driver=virtualbox . To persistent configuration so that you to run minishift start without explicitly passing i.e. in global scope the --vm-driver virtualbox flag each time, run: minishift config set vm-driver virtualbox . You can run this command in a shell after starting Minishift to get the URL of the Web console: minishift console --url Use minishift oc-env to display the command you need to type into your shell in order to add the oc binary to your PATH environment variable. The output of oc-env will differ depending on OS and shell type. $ minishift oc-env export PATH=\"/root/.minishift/cache/oc/v3.11.0/linux:$PATH\" # Run this command to configure your shell: # eval $(minishift oc-env)","title":"Starting Minishift"},{"location":"openstack/kubernetes/minishift/#deploying-a-sample-application","text":"OpenShift provides various sample applications, such as templates, builder applications, and quickstarts. The following steps describe how to deploy a sample Node.js application from the command line. Create a Node.js example app: oc new-app https://github.com/sclorg/nodejs-ex -l name=myapp Track the build log until the app is built and deployed: oc logs -f bc/nodejs-ex Expose a route to the service: oc expose svc/nodejs-ex Access the application: minishift openshift service nodejs-ex --in-browser To stop Minishift, use the following command: minishift stop Stopping local OpenShift cluster... Stopping \"minishift\"...","title":"Deploying a Sample Application"},{"location":"openstack/kubernetes/minishift/#updating-minishift","text":"minishift update","title":"Updating Minishift"},{"location":"openstack/kubernetes/minishift/#uninstalling-minishift","text":"Delete the Minishift VM and any VM-specific files: minishift delete This command deletes everything in the $MINISHIFT_HOME/.minishift/machines/minishift directory. Other cached data and the persistent configuration are not removed. To completely uninstall Minishift, delete everything in the MINISHIFT_HOME directory (default ~/.minishift ) and ~/.kube : rm -rf ~/.minishift rm -rf ~/.kube With your hypervisor management tool, confirm that there are no remaining artifacts related to the Minishift VM. For example, if you use KVM, you need to run the virsh command.","title":"Uninstalling Minishift"},{"location":"openstack/kubernetes/minishift/#connecting-to-the-minishift-vm-with-ssh","text":"You can use the minishift ssh command to interact with the Minishift VM. minishift ssh -- docker ps CONTAINER IMAGE COMMAND CREATED STATUS NAMES 71fe8ff16548 openshift/origin:... \"/usr/bin/openshift s\" 1 sec ago Up 1 second origin","title":"Connecting to the Minishift VM with SSH"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster-using-k3d/","text":"Set up K3s in High Availability using k3d First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the embedded DB in this HA K3s cluster setup. For which etcd is the default embedded DB. There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc . Pre-requisite Make sure you have already installed k3d following this . HA cluster with at least three control plane nodes k3d cluster create --servers 3 --image rancher/k3s:latest Here, --server 3 : specifies requests three nodes to be created with the role server and --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest Switch context to the new cluster: kubectl config use-context k3d-k3s-default You can now check what has been created from the different points of view: kubectl get nodes --output wide The output will looks like: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide The output will looks like: Scale up the cluster You can quickly simulate the addition of another control plane node to the HA cluster: k3d node create extraCPnode --role=server --image=rancher/k3s:latest INFO[0000] Adding 1 node(s) to the runtime local cluster 'k3s-default'... INFO[0000] Starting Node 'k3d-extraCPnode-0' INFO[0018] Updating loadbalancer config to include new server node(s) INFO[0018] Successfully configured loadbalancer k3d-k3s-default-serverlb! INFO[0019] Successfully created 1 node(s)! Here, extraCPnode : specifies the name for the node, --role=server : sets the role for the node to be a control plane/server, --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 31m v1.19.3+k3s2 k3d-k3s-default-server-0 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 47m v1.19.3+k3s2 OR, kubectl get nodes --output wide The output looks like below: Heavy Armored against crashes As we are working with containers, the best way to \u201ccrash\u201d a node is to literally stop the container: docker stop k3d-k3s-default-server-0 !!!note: \"Note\" The Docker and k3d commands will show the state change immediately. However, the Kubernetes (read: K8s or K3s) cluster needs a short time to see the state change to NotReady. kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 32m v1.19.3+k3s2 k3d-k3s-default-server-0 NotReady etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 48m v1.19.3+k3s2 Now it is a good time to reference again the load balancer k3d uses and how it is critical in allowing us to continue accessing the K3s cluster. While the load balancer internally switched to the next available node, from an external connectivity point of view, we still use the same IP/host. This abstraction saves us quite some efforts and it\u2019s one of the most useful features of k3d. Let\u2019s look at the state of the cluster: kubectl get all --all-namespaces The output looks like below: Everything looks right. If we look at the pods more specifically, then we will see that K3s automatically self-healed by recreating pods running on the failed node on other nodes: kubectl get pods --all-namespaces --output wide As the output can be seen: Finally, to show the power of HA and how K3s manages it, let\u2019s restart the node0 and see it being re-included into the cluster as if nothing happened: docker start k3d-k3s-default-server-0 Our cluster is stable, and all the nodes are fully operational again as shown below: Cleaning the resources k3d cluster delete","title":"Multi-master K3s cluster setup using k3d"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster-using-k3d/#set-up-k3s-in-high-availability-using-k3d","text":"First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the embedded DB in this HA K3s cluster setup. For which etcd is the default embedded DB. There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc .","title":"Set up K3s in High Availability using k3d"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster-using-k3d/#pre-requisite","text":"Make sure you have already installed k3d following this .","title":"Pre-requisite"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster-using-k3d/#ha-cluster-with-at-least-three-control-plane-nodes","text":"k3d cluster create --servers 3 --image rancher/k3s:latest Here, --server 3 : specifies requests three nodes to be created with the role server and --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest Switch context to the new cluster: kubectl config use-context k3d-k3s-default You can now check what has been created from the different points of view: kubectl get nodes --output wide The output will looks like: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide The output will looks like:","title":"HA cluster with at least three control plane nodes"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster-using-k3d/#scale-up-the-cluster","text":"You can quickly simulate the addition of another control plane node to the HA cluster: k3d node create extraCPnode --role=server --image=rancher/k3s:latest INFO[0000] Adding 1 node(s) to the runtime local cluster 'k3s-default'... INFO[0000] Starting Node 'k3d-extraCPnode-0' INFO[0018] Updating loadbalancer config to include new server node(s) INFO[0018] Successfully configured loadbalancer k3d-k3s-default-serverlb! INFO[0019] Successfully created 1 node(s)! Here, extraCPnode : specifies the name for the node, --role=server : sets the role for the node to be a control plane/server, --image rancher/k3s:latest : specifies the K3s image to be used here we are using latest kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 31m v1.19.3+k3s2 k3d-k3s-default-server-0 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 47m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 47m v1.19.3+k3s2 OR, kubectl get nodes --output wide The output looks like below:","title":"Scale up the cluster"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster-using-k3d/#heavy-armored-against-crashes","text":"As we are working with containers, the best way to \u201ccrash\u201d a node is to literally stop the container: docker stop k3d-k3s-default-server-0 !!!note: \"Note\" The Docker and k3d commands will show the state change immediately. However, the Kubernetes (read: K8s or K3s) cluster needs a short time to see the state change to NotReady. kubectl get nodes NAME STATUS ROLES AGE VERSION k3d-extracpnode-0 Ready etcd,master 32m v1.19.3+k3s2 k3d-k3s-default-server-0 NotReady etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-1 Ready etcd,master 48m v1.19.3+k3s2 k3d-k3s-default-server-2 Ready etcd,master 48m v1.19.3+k3s2 Now it is a good time to reference again the load balancer k3d uses and how it is critical in allowing us to continue accessing the K3s cluster. While the load balancer internally switched to the next available node, from an external connectivity point of view, we still use the same IP/host. This abstraction saves us quite some efforts and it\u2019s one of the most useful features of k3d. Let\u2019s look at the state of the cluster: kubectl get all --all-namespaces The output looks like below: Everything looks right. If we look at the pods more specifically, then we will see that K3s automatically self-healed by recreating pods running on the failed node on other nodes: kubectl get pods --all-namespaces --output wide As the output can be seen: Finally, to show the power of HA and how K3s manages it, let\u2019s restart the node0 and see it being re-included into the cluster as if nothing happened: docker start k3d-k3s-default-server-0 Our cluster is stable, and all the nodes are fully operational again as shown below:","title":"Heavy Armored against crashes"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster-using-k3d/#cleaning-the-resources","text":"k3d cluster delete","title":"Cleaning the resources"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/","text":"K3s with High Availability setup First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the external DB in this HA K3s cluster setup. For which MySQL is the external DB as shown here: In the diagram above, both the user running kubectl and each of the two agents connect to the TCP Load Balancer . The Load Balancer uses a list of private IP addresses to balance the traffic between the three servers . If one of the servers crashes, it is be removed from the list of IP addresses. The servers use the SQL data store to synchronize the cluster\u2019s state. Requirements i. Managed TCP Load Balancer ii. Managed MySQL service iii. Three VMs to run as K3s servers iv. Two VMs to run as K3s agents There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc . Managed TCP Load Balancer Create a load balancer using nginx : The nginx.conf located at etc/nginx/nginx.conf contains upstream that is pointing to the 3 K3s Servers on port 6443 as shown below: events {} ... stream { upstream k3s_servers { server <k3s_server1-Internal-IP>:6443; server <k3s_server2-Internal-IP>:6443; server <k3s_server3-Internal-IP>:6443; } server { listen 6443; proxy_pass k3s_servers; } } Managed MySQL service Create a MySQL database server with a new database and create a new mysql user and password with granted permission to read/write the new database. In this example, you can create: database name: <YOUR_DB_NAME> database user: <YOUR_DB_USER_NAME> database password: <YOUR_DB_USER_PASSWORD> #pragma: allowlist secret Three VMs to run as K3s servers Create 3 K3s Master VMs and perform the following steps on each of them: i. Export the datastore endpoint: export K3S_DATASTORE_ENDPOINT='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' ii. Install the K3s with setting not to deploy any pods on this server (opposite of affinity) unless critical addons and tls-san set <Loadbalancer-Internal-IP> as alternative name for that tls certificate. curl -sfL https://get.k3s.io | sh -s - server \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --tls-san <Loadbalancer-Internal-IP_or_Hostname> Verify all master nodes are visible to eachothers: sudo k3s kubectl get node Generate token from one of the K3s Master VMs: You need to extract a token form the master that will be used to join the nodes to the control plane by running following command on one of the K3s master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778 Two VMs to run as K3s agents Set the K3S_URL to point to the Loadbalancer\u2019s internal IP and set the K3S_TOKEN from the clipboard on both of the agent nodes: curl -sfL https://get.k3s.io | K3S_URL=https://<Loadbalancer-Internal-IP_or_Hostname>:6443 K3S_TOKEN=<Token_From_Master> sh - Once both Agents are running, if you run the following command on Master Server, you can see all nodes: `sudo k3s kubectl get node` Simulate a failure To simulate a failure, stop the K3s service on one or more of the K3s servers manually, then run the kubectl get nodes command: sudo systemctl stop k3s The third server will take over at this point. To restart servers manually: sudo systemctl restart k3s On your local development machine to access Kubernetes Cluster Remotely (Optional) Important Requirement Your local development machine must have installed kubectl . Copy kubernetes config to your local machine: Copy the kubeconfig file's content located at the K3s master node at /etc/rancher/k3s/k3s.yaml to your local machine's ~/.kube/config file. Before saving, please change the cluster server path from 127.0.0.1 to <Loadbalancer-Internal-IP> . This will allow your local machine to see the cluster nodes: kubectl get nodes Kubernetes Dashboard check releases for the command to use for Installation : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml Dashboard RBAC Configuration: dashboard.admin-user.yml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard dashboard.admin-user-role.yml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard Deploy the admin-user configuration: Important Note If you\u2019re doing this from your local development machine, remove sudo k3s and just use kubectl ) sudo k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml Get bearer token sudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token Start dashboard locally: sudo k3s kubectl proxy Then you can sign in at this URL using your token we got in the previous step: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ Deploying Nginx using deployment Create a deployment nginx.yaml : vi nginx.yaml Copy and paste the following conent on nginx.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 sudo k3s kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces OR, kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite","title":"K3s with High Availibility(HA) setup"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#k3s-with-high-availability-setup","text":"First, Kubernetes HA has two possible setups : embedded or external database (DB). We\u2019ll use the external DB in this HA K3s cluster setup. For which MySQL is the external DB as shown here: In the diagram above, both the user running kubectl and each of the two agents connect to the TCP Load Balancer . The Load Balancer uses a list of private IP addresses to balance the traffic between the three servers . If one of the servers crashes, it is be removed from the list of IP addresses. The servers use the SQL data store to synchronize the cluster\u2019s state.","title":"K3s with High Availability setup"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#requirements","text":"i. Managed TCP Load Balancer ii. Managed MySQL service iii. Three VMs to run as K3s servers iv. Two VMs to run as K3s agents There are some strongly recommended Kubernetes HA best practices and also there is Automated HA master deployment doc .","title":"Requirements"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#managed-tcp-load-balancer","text":"Create a load balancer using nginx : The nginx.conf located at etc/nginx/nginx.conf contains upstream that is pointing to the 3 K3s Servers on port 6443 as shown below: events {} ... stream { upstream k3s_servers { server <k3s_server1-Internal-IP>:6443; server <k3s_server2-Internal-IP>:6443; server <k3s_server3-Internal-IP>:6443; } server { listen 6443; proxy_pass k3s_servers; } }","title":"Managed TCP Load Balancer"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#managed-mysql-service","text":"Create a MySQL database server with a new database and create a new mysql user and password with granted permission to read/write the new database. In this example, you can create: database name: <YOUR_DB_NAME> database user: <YOUR_DB_USER_NAME> database password: <YOUR_DB_USER_PASSWORD> #pragma: allowlist secret","title":"Managed MySQL service"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#three-vms-to-run-as-k3s-servers","text":"Create 3 K3s Master VMs and perform the following steps on each of them: i. Export the datastore endpoint: export K3S_DATASTORE_ENDPOINT='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' ii. Install the K3s with setting not to deploy any pods on this server (opposite of affinity) unless critical addons and tls-san set <Loadbalancer-Internal-IP> as alternative name for that tls certificate. curl -sfL https://get.k3s.io | sh -s - server \\ --node-taint CriticalAddonsOnly=true:NoExecute \\ --tls-san <Loadbalancer-Internal-IP_or_Hostname> Verify all master nodes are visible to eachothers: sudo k3s kubectl get node Generate token from one of the K3s Master VMs: You need to extract a token form the master that will be used to join the nodes to the control plane by running following command on one of the K3s master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778","title":"Three VMs to run as K3s servers"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#two-vms-to-run-as-k3s-agents","text":"Set the K3S_URL to point to the Loadbalancer\u2019s internal IP and set the K3S_TOKEN from the clipboard on both of the agent nodes: curl -sfL https://get.k3s.io | K3S_URL=https://<Loadbalancer-Internal-IP_or_Hostname>:6443 K3S_TOKEN=<Token_From_Master> sh - Once both Agents are running, if you run the following command on Master Server, you can see all nodes: `sudo k3s kubectl get node`","title":"Two VMs to run as K3s agents"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#simulate-a-failure","text":"To simulate a failure, stop the K3s service on one or more of the K3s servers manually, then run the kubectl get nodes command: sudo systemctl stop k3s The third server will take over at this point. To restart servers manually: sudo systemctl restart k3s","title":"Simulate a failure"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#on-your-local-development-machine-to-access-kubernetes-cluster-remotely-optional","text":"Important Requirement Your local development machine must have installed kubectl . Copy kubernetes config to your local machine: Copy the kubeconfig file's content located at the K3s master node at /etc/rancher/k3s/k3s.yaml to your local machine's ~/.kube/config file. Before saving, please change the cluster server path from 127.0.0.1 to <Loadbalancer-Internal-IP> . This will allow your local machine to see the cluster nodes: kubectl get nodes","title":"On your local development machine to access Kubernetes Cluster Remotely (Optional)"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#kubernetes-dashboard","text":"check releases for the command to use for Installation : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml Dashboard RBAC Configuration: dashboard.admin-user.yml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard dashboard.admin-user-role.yml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard Deploy the admin-user configuration: Important Note If you\u2019re doing this from your local development machine, remove sudo k3s and just use kubectl ) sudo k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml Get bearer token sudo k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token Start dashboard locally: sudo k3s kubectl proxy Then you can sign in at this URL using your token we got in the previous step: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/","title":"Kubernetes Dashboard"},{"location":"openstack/kubernetes/k3s/k3s-ha-cluster/#deploying-nginx-using-deployment","text":"Create a deployment nginx.yaml : vi nginx.yaml Copy and paste the following conent on nginx.yaml : apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 sudo k3s kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces OR, kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite","title":"Deploying Nginx using deployment"},{"location":"openstack/kubernetes/k3s/k3s-using-k3d/","text":"Setup K3s cluster Using k3d One of the most popular and second method of creating k3s cluster is by using k3d . By the name itself it suggests, K3s-in-docker , is a wrapper around K3s \u2013 Lightweight Kubernetes that runs it in docker. Please refer to this link to get brief insights of this wonderful tool. It provides a seamless experience working with K3s cluster management with some straight forward commands. k3d is efficient enough to create and manage K3s single node and well as K3s High Availability clusters just with few commands. !!!note: \"Note\" For using k3d you must have docker installed in your system Install Docker Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Install kubectl Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml Installing k3d k3d Installation: The below command will install the k3d, in your system using the installation script. wget -q -O - https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash OR, curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash To verify the installation, please run the following command: k3d version k3d version v5.0.0 k3s version v1.21.5-k3s1 (default) After the successful installation, you are ready to create your cluster using k3d and run K3s in docker within seconds. Getting Started: Now let's directly jump into creating our K3s cluster using k3d . Create k3d Cluster: sh k3d cluster create k3d-demo-cluster This single command spawns a K3s cluster with two containers: A Kubernetes control-plane node(server) and a load balancer(serverlb) in front of it. It puts both of them in a dedicated Docker network and exposes the Kubernetes API on a randomly chosen free port on the Docker host. It also creates a named Docker volume in the background as a preparation for image imports. You can also look for advance syntax for cluster creation: sh k3d cluster create mycluster --api-port 127.0.0.1:6445 --servers 3 \\ --agents 2 --volume '/home/me/mycode:/code@agent[*]' --port '8080:80@loadbalancer' Here, the above single command spawns a K3s cluster with six containers: i. load balancer ii. 3 servers (control-plane nodes) iii. 2 agents (formerly worker nodes) With the --api-port 127.0.0.1:6445 , you tell k3d to map the Kubernetes API Port (6443 internally) to 127.0.0.1/localhost \u2019s port 6445 . That means that you will have this connection string in your Kubeconfig: server: https://127.0.0.1:6445 to connect to this cluster. This port will be mapped from the load balancer to your host system. From there, requests will be proxied to your server nodes, effectively simulating a production setup, where server nodes also can go down and you would want to failover to another server. The --volume /home/me/mycode:/code@agent[*] bind mounts your local directory /home/me/mycode to the path /code inside all ( [*] of your agent nodes). Replace * with an index (here: 0 or 1) to only mount it into one of them. The specification telling k3d which nodes it should mount the volume to is called \u201cnode filter\u201d and it\u2019s also used for other flags, like the --port flag for port mappings. That said, --port '8080:80@loadbalancer' maps your local host\u2019s port 8080 to port 80 on the load balancer (serverlb), which can be used to forward HTTP ingress traffic to your cluster. For example, you can now deploy a web app into the cluster (Deployment), which is exposed (Service) externally via an Ingress such as myapp.k3d.localhost . Then (provided that everything is set up to resolve that domain to your local host IP), you can point your browser to http://myapp.k3d.localhost:8080 to access your app. Traffic then flows from your host through the Docker bridge interface to the load balancer. From there, it\u2019s proxied to the cluster, where it passes via Ingress and Service to your application Pod. !!!note: \"Note\" You have to have some mechanism set up to route to resolve myapp.k3d.localhost to your local host IP ( 127.0.0.1 ). The most common way is using entries of the form 127.0.0.1 myapp.k3d.localhost in your /etc/hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts on Windows). However, this does not allow for wildcard entries ( *.localhost ), so it may become a bit cumbersome after a while, so you may want to have a look at tools like dnsmasq (MacOS/UNIX) or Acrylic (Windows) to ease the burden. Getting the cluster\u2019s kubeconfig: Get the new cluster\u2019s connection details merged into your default kubeconfig (usually specified using the KUBECONFIG environment variable or the default path $HOME/.kube/config ) and directly switch to the new context: ```sh k3d kubeconfig merge k3d-demo-cluster --kubeconfig-switch-context /root/.k3d/kubeconfig-k3d-demo-cluster.yaml ``` Checking the nodes running on k3d cluster: ```sh k3d node list NAME ROLE CLUSTER STATUS k3d-k3d-demo-cluster-server-0 server k3d-demo-cluster running k3d-k3d-demo-cluster-serverlb loadbalancer k3d-demo-cluster running ``` You can see here two nodes. The (very) smart implementation here is that while the cluster is running on its node k3d-k3s-default-server-0 , there is another \u201cnode\u201d that acts as the load balancer i.e. k3d-k3d-demo-cluster-serverlb . Firing Kubectl commands that allows you to run commands against Kubernetes: i. The below command will list down the nodes available in our cluster: sh kubectl get nodes -o wide OR, sh kubectl get nodes --output wide The output will looks like: ii. To look at what\u2019s inside the K3s cluster (pods, services, deployments, etc.): sh kubectl get all --all-namespaces The output will looks like: We can see that, in addition of the Kubernetes service, K3s deploys DNS, metrics and ingress (traefik) services when we use the defaults. iii. List the active k3d clusters: ```sh k3d cluster list NAME SERVERS AGENTS LOADBALANCER k3d-demo-cluster 1/1 0/0 true ``` iv. Check the cluster connectivity: ```sh kubectl cluster-info Kubernetes control plane is running at https://0.0.0.0:44921 CoreDNS is running at https://0.0.0.0:44921/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://0.0.0.0:44921/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. ``` Check the active containers: docker ps Now as you can observe, the cluster is up and running and we can play around the cluster, you can create and deploy your applications over the cluster. Deleting Cluster: k3d cluster delete k3d-demo-cluster INFO[0000] Deleting cluster 'k3d-demo-cluster' INFO[0000] Deleted k3d-k3d-demo-cluster-serverlb INFO[0001] Deleted k3d-k3d-demo-cluster-server-0 INFO[0001] Deleting cluster network 'k3d-k3d-demo-cluster' INFO[0001] Deleting image volume 'k3d-k3d-demo-cluster-images' INFO[0001] Removing cluster details from default kubeconfig... INFO[0001] Removing standalone kubeconfig file (if there is one)... INFO[0001] Successfully deleted cluster k3d-demo-cluster! You can also create a k3d High Availability cluster and add as many nodes you want within seconds.","title":"Single-Node K3s Cluster using k3d"},{"location":"openstack/kubernetes/k3s/k3s-using-k3d/#setup-k3s-cluster-using-k3d","text":"One of the most popular and second method of creating k3s cluster is by using k3d . By the name itself it suggests, K3s-in-docker , is a wrapper around K3s \u2013 Lightweight Kubernetes that runs it in docker. Please refer to this link to get brief insights of this wonderful tool. It provides a seamless experience working with K3s cluster management with some straight forward commands. k3d is efficient enough to create and manage K3s single node and well as K3s High Availability clusters just with few commands. !!!note: \"Note\" For using k3d you must have docker installed in your system","title":"Setup K3s cluster Using k3d"},{"location":"openstack/kubernetes/k3s/k3s-using-k3d/#install-docker","text":"Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker","title":"Install Docker"},{"location":"openstack/kubernetes/k3s/k3s-using-k3d/#install-kubectl","text":"Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic kubectl 1.22.2 from Canonical\u2713 installed Now verify the kubectl version: kubectl version -o yaml","title":"Install kubectl"},{"location":"openstack/kubernetes/k3s/k3s-using-k3d/#installing-k3d","text":"k3d Installation: The below command will install the k3d, in your system using the installation script. wget -q -O - https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash OR, curl -s https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash To verify the installation, please run the following command: k3d version k3d version v5.0.0 k3s version v1.21.5-k3s1 (default) After the successful installation, you are ready to create your cluster using k3d and run K3s in docker within seconds. Getting Started: Now let's directly jump into creating our K3s cluster using k3d . Create k3d Cluster: sh k3d cluster create k3d-demo-cluster This single command spawns a K3s cluster with two containers: A Kubernetes control-plane node(server) and a load balancer(serverlb) in front of it. It puts both of them in a dedicated Docker network and exposes the Kubernetes API on a randomly chosen free port on the Docker host. It also creates a named Docker volume in the background as a preparation for image imports. You can also look for advance syntax for cluster creation: sh k3d cluster create mycluster --api-port 127.0.0.1:6445 --servers 3 \\ --agents 2 --volume '/home/me/mycode:/code@agent[*]' --port '8080:80@loadbalancer' Here, the above single command spawns a K3s cluster with six containers: i. load balancer ii. 3 servers (control-plane nodes) iii. 2 agents (formerly worker nodes) With the --api-port 127.0.0.1:6445 , you tell k3d to map the Kubernetes API Port (6443 internally) to 127.0.0.1/localhost \u2019s port 6445 . That means that you will have this connection string in your Kubeconfig: server: https://127.0.0.1:6445 to connect to this cluster. This port will be mapped from the load balancer to your host system. From there, requests will be proxied to your server nodes, effectively simulating a production setup, where server nodes also can go down and you would want to failover to another server. The --volume /home/me/mycode:/code@agent[*] bind mounts your local directory /home/me/mycode to the path /code inside all ( [*] of your agent nodes). Replace * with an index (here: 0 or 1) to only mount it into one of them. The specification telling k3d which nodes it should mount the volume to is called \u201cnode filter\u201d and it\u2019s also used for other flags, like the --port flag for port mappings. That said, --port '8080:80@loadbalancer' maps your local host\u2019s port 8080 to port 80 on the load balancer (serverlb), which can be used to forward HTTP ingress traffic to your cluster. For example, you can now deploy a web app into the cluster (Deployment), which is exposed (Service) externally via an Ingress such as myapp.k3d.localhost . Then (provided that everything is set up to resolve that domain to your local host IP), you can point your browser to http://myapp.k3d.localhost:8080 to access your app. Traffic then flows from your host through the Docker bridge interface to the load balancer. From there, it\u2019s proxied to the cluster, where it passes via Ingress and Service to your application Pod. !!!note: \"Note\" You have to have some mechanism set up to route to resolve myapp.k3d.localhost to your local host IP ( 127.0.0.1 ). The most common way is using entries of the form 127.0.0.1 myapp.k3d.localhost in your /etc/hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts on Windows). However, this does not allow for wildcard entries ( *.localhost ), so it may become a bit cumbersome after a while, so you may want to have a look at tools like dnsmasq (MacOS/UNIX) or Acrylic (Windows) to ease the burden. Getting the cluster\u2019s kubeconfig: Get the new cluster\u2019s connection details merged into your default kubeconfig (usually specified using the KUBECONFIG environment variable or the default path $HOME/.kube/config ) and directly switch to the new context: ```sh k3d kubeconfig merge k3d-demo-cluster --kubeconfig-switch-context /root/.k3d/kubeconfig-k3d-demo-cluster.yaml ``` Checking the nodes running on k3d cluster: ```sh k3d node list NAME ROLE CLUSTER STATUS k3d-k3d-demo-cluster-server-0 server k3d-demo-cluster running k3d-k3d-demo-cluster-serverlb loadbalancer k3d-demo-cluster running ``` You can see here two nodes. The (very) smart implementation here is that while the cluster is running on its node k3d-k3s-default-server-0 , there is another \u201cnode\u201d that acts as the load balancer i.e. k3d-k3d-demo-cluster-serverlb . Firing Kubectl commands that allows you to run commands against Kubernetes: i. The below command will list down the nodes available in our cluster: sh kubectl get nodes -o wide OR, sh kubectl get nodes --output wide The output will looks like: ii. To look at what\u2019s inside the K3s cluster (pods, services, deployments, etc.): sh kubectl get all --all-namespaces The output will looks like: We can see that, in addition of the Kubernetes service, K3s deploys DNS, metrics and ingress (traefik) services when we use the defaults. iii. List the active k3d clusters: ```sh k3d cluster list NAME SERVERS AGENTS LOADBALANCER k3d-demo-cluster 1/1 0/0 true ``` iv. Check the cluster connectivity: ```sh kubectl cluster-info Kubernetes control plane is running at https://0.0.0.0:44921 CoreDNS is running at https://0.0.0.0:44921/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://0.0.0.0:44921/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. ``` Check the active containers: docker ps Now as you can observe, the cluster is up and running and we can play around the cluster, you can create and deploy your applications over the cluster. Deleting Cluster: k3d cluster delete k3d-demo-cluster INFO[0000] Deleting cluster 'k3d-demo-cluster' INFO[0000] Deleted k3d-k3d-demo-cluster-serverlb INFO[0001] Deleted k3d-k3d-demo-cluster-server-0 INFO[0001] Deleting cluster network 'k3d-k3d-demo-cluster' INFO[0001] Deleting image volume 'k3d-k3d-demo-cluster-images' INFO[0001] Removing cluster details from default kubeconfig... INFO[0001] Removing standalone kubeconfig file (if there is one)... INFO[0001] Successfully deleted cluster k3d-demo-cluster! You can also create a k3d High Availability cluster and add as many nodes you want within seconds.","title":"Installing k3d"},{"location":"openstack/kubernetes/k3s/k3s-using-k3sup/","text":"K3s cluster setup using k3sup k3sup (pronounced ketchup ) is a popular open source tool to install K3s over SSH. Bootstrap the cluster The two most important commands in k3sup are: i. install: install K3s to a new server and create a join token for the cluster ii. join: fetch the join token from a server, then use it to install K3s to an agent Download k3sup curl -sLS https://get.k3sup.dev | sh sudo install k3sup /usr/bin/ k3sup --help Other options for install : --cluster - start this server in clustering mode using embedded etcd (embedded HA) --skip-install - if you already have k3s installed, you can just run this command to get the kubeconfig --ssh-key - specify a specific path for the SSH key for remote login --local-path - default is ./kubeconfig - set the file where you want to save your cluster's kubeconfig . By default this file will be overwritten. --merge - Merge config into existing file instead of overwriting (e.g. to add config to the default kubectl config, use --local-path ~/.kube/config --merge ). --context - default is default - set the name of the kubeconfig context. --ssh-port - default is 22, but you can specify an alternative port i.e. 2222 --k3s-extra-args - Optional extra arguments to pass to k3s installer, wrapped in quotes, i.e. --k3s-extra-args '--no-deploy traefik' or --k3s-extra-args '--docker' . For multiple args combine then within single quotes --k3s-extra-args --no-deploy traefik --docker . --k3s-version - set the specific version of k3s, i.e. v0.9.1 --ipsec - Enforces the optional extra argument for k3s: --flannel-backend option: ipsec --print-command - Prints out the command, sent over SSH to the remote computer --datastore - used to pass a SQL connection-string to the --datastore-endpoint flag of k3s. See even more install options by running k3sup install --help . On Master Node: export SERVER_IP=<Master-Internal-IP> export USER=root k3sup install --ip $SERVER_IP --user $USER On Agent Node: Next join one or more agents to the cluster: export AGENT_IP=<Agent-Internal-IP> export SERVER_IP=<Master-Internal-IP> export USER=root k3sup join --ip $AGENT_IP --server-ip $SERVER_IP --user $USER Create a multi-master (HA) setup with external SQL export LB_IP='<Loadbalancer-Internal-IP_or_Hostname>' export DATASTORE='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' export CHANNEL=latest Before continuing, check that your environment variables are still populated from earlier, and if not, trace back and populate them. echo $LB_IP echo $DATASTORE echo $CHANNEL k3sup install --user root --ip $SERVER1 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER2 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER3 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup join --user root --server-ip $LB_IP --ip $AGENT1 \\ --k3s-channel $CHANNEL \\ --print-command k3sup join --user root --server-ip $LB_IP --ip $AGENT2 \\ --k3s-channel $CHANNEL \\ --print-command There will be a kubeconfig file created in the current working directory with the IP address of the LoadBalancer set for kubectl to use. Check the nodes have joined: export KUBECONFIG=`pwd`/kubeconfig kubectl get node","title":"Multi-master HA K3s cluster using k3sup"},{"location":"openstack/kubernetes/k3s/k3s-using-k3sup/#k3s-cluster-setup-using-k3sup","text":"k3sup (pronounced ketchup ) is a popular open source tool to install K3s over SSH. Bootstrap the cluster The two most important commands in k3sup are: i. install: install K3s to a new server and create a join token for the cluster ii. join: fetch the join token from a server, then use it to install K3s to an agent","title":"K3s cluster setup using k3sup"},{"location":"openstack/kubernetes/k3s/k3s-using-k3sup/#download-k3sup","text":"curl -sLS https://get.k3sup.dev | sh sudo install k3sup /usr/bin/ k3sup --help Other options for install : --cluster - start this server in clustering mode using embedded etcd (embedded HA) --skip-install - if you already have k3s installed, you can just run this command to get the kubeconfig --ssh-key - specify a specific path for the SSH key for remote login --local-path - default is ./kubeconfig - set the file where you want to save your cluster's kubeconfig . By default this file will be overwritten. --merge - Merge config into existing file instead of overwriting (e.g. to add config to the default kubectl config, use --local-path ~/.kube/config --merge ). --context - default is default - set the name of the kubeconfig context. --ssh-port - default is 22, but you can specify an alternative port i.e. 2222 --k3s-extra-args - Optional extra arguments to pass to k3s installer, wrapped in quotes, i.e. --k3s-extra-args '--no-deploy traefik' or --k3s-extra-args '--docker' . For multiple args combine then within single quotes --k3s-extra-args --no-deploy traefik --docker . --k3s-version - set the specific version of k3s, i.e. v0.9.1 --ipsec - Enforces the optional extra argument for k3s: --flannel-backend option: ipsec --print-command - Prints out the command, sent over SSH to the remote computer --datastore - used to pass a SQL connection-string to the --datastore-endpoint flag of k3s. See even more install options by running k3sup install --help . On Master Node: export SERVER_IP=<Master-Internal-IP> export USER=root k3sup install --ip $SERVER_IP --user $USER On Agent Node: Next join one or more agents to the cluster: export AGENT_IP=<Agent-Internal-IP> export SERVER_IP=<Master-Internal-IP> export USER=root k3sup join --ip $AGENT_IP --server-ip $SERVER_IP --user $USER","title":"Download k3sup"},{"location":"openstack/kubernetes/k3s/k3s-using-k3sup/#create-a-multi-master-ha-setup-with-external-sql","text":"export LB_IP='<Loadbalancer-Internal-IP_or_Hostname>' export DATASTORE='mysql://<YOUR_DB_USER_NAME>:<YOUR_DB_USER_PASSWORD>@tcp(<MySQL-Server-Internal-IP>:3306)/<YOUR_DB_NAME>' export CHANNEL=latest Before continuing, check that your environment variables are still populated from earlier, and if not, trace back and populate them. echo $LB_IP echo $DATASTORE echo $CHANNEL k3sup install --user root --ip $SERVER1 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER2 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup install --user root --ip $SERVER3 \\ --k3s-channel $CHANNEL \\ --print-command \\ --datastore='${DATASTORE}' \\ --tls-san $LB_IP k3sup join --user root --server-ip $LB_IP --ip $AGENT1 \\ --k3s-channel $CHANNEL \\ --print-command k3sup join --user root --server-ip $LB_IP --ip $AGENT2 \\ --k3s-channel $CHANNEL \\ --print-command There will be a kubeconfig file created in the current working directory with the IP address of the LoadBalancer set for kubectl to use. Check the nodes have joined: export KUBECONFIG=`pwd`/kubeconfig kubectl get node","title":"Create a multi-master (HA) setup with external SQL"},{"location":"openstack/kubernetes/k3s/k3s/","text":"K3s Features Lightweight certified K8s distro Built for production operations 40MB binary, 250MB memeory consumption Single process w/ integrated K8s master, Kubelet, and containerd Supports not only etcd to hold the cluster state, but also SQLite (for single-node, simpler setups) or external DBs like MySQL and PostgreSQL Open source project Components and architecure High-Availability K3s Server with an External DB: or, For this kind of high availability k3s setup read this . Pre-requisite We will need 1 control-plane(master) and 2 worker nodes to create a single control-plane kubernetes cluster using k3s . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to the master node. 2 Linux machines for worker, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Networking The K3s server needs port 6443 to be accessible by all nodes. The nodes need to be able to reach other nodes over UDP port 8472 when Flannel VXLAN overlay networking is used. The node should not listen on any other port. K3s uses reverse tunneling such that the nodes make outbound connections to the server and all kubelet traffic runs through that tunnel. However, if you do not use Flannel and provide your own custom CNI, then port 8472 is not needed by K3s. If you wish to utilize the metrics server , you will need to open port 10250 on each node. If you plan on achieving high availability with embedded etcd , server nodes must be accessible to each other on ports 2379 and 2380 . Create 1 security group with appropriate Inbound Rules for K3s Server Nodes that will be used by all 3 nodes: Important Note The VXLAN overlay networking port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disables access to port 8472 . setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.235 k3s-master\" >> /etc/hosts hostnamectl set-hostname k3s-master In this step, you will install kubelet and kubeadm on the below nodes k3s-master k3s-worker1 k3s-worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Install Docker Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Configure K3s to bootstrap the cluster on master node Run the below command on the master node i.e. k3s-master that you want to setup as control plane. SSH into k3s-master machine Switch to root user: sudo su Execute the below command to initialize the cluster: curl -sfL https://get.k3s.io | sh -s - --kubelet-arg 'cgroup-driver=systemd' \\ --node-taint CriticalAddonsOnly=true:NoExecute --docker OR, If you don't want to setup the K3s cluster without using docker as the container runtime, then just run without supplying the --docker argument. curl -sfL https://get.k3s.io | sh - More Installation Options After running this installation: The K3s service will be configured to automatically restart after node reboots or if the process crashes or is killed Additional utilities will be installed, including kubectl , crictl , ctr , k3s-killall.sh , and k3s-uninstall.sh A kubeconfig file will be written to /etc/rancher/k3s/k3s.yaml and the kubectl installed by K3s will automatically use it. To check if the service installed successfully, you can use: systemctl status k3s The output looks like: OR, k3s --version kubectl version Note If you want to taint the node i.e. not to deploy pods on this node after installation then run: kubectl taint nodes <master_node_name> k3s-controlplane=true:NoExecure i.e. kubectl taint nodes k3s-master k3s-controlplane=true:NoExecure You can check if the master node is working by: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 37s v1.21.5+k3s2 kubectl config get-clusters NAME default kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:6443 CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get namespaces NAME STATUS AGE default Active 27m kube-system Active 27m kube-public Active 27m kube-node-lease Active 27m kubectl get endpoints -n kube-system NAME ENDPOINTS AGE kube-dns 10.42.0.4:53,10.42.0.4:53,10.42.0.4:9153 27m metrics-server 10.42.0.3:443 27m rancher.io-local-path <none> 27m kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE helm-install-traefik-crd-ql7j2 0/1 Pending 0 32m helm-install-traefik-mr65j 0/1 Pending 0 32m coredns-7448499f4d-x57z7 1/1 Running 0 32m metrics-server-86cbb8457f-cg2fs 1/1 Running 0 32m local-path-provisioner-5ff76fc89d-kdfcl 1/1 Running 0 32m You need to extract a token form the master that will be used to join the nodes to the master. On the master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778 Configure K3s on worker nodes to join the cluster Run the below command on both of the worker nodes i.e. k3s-worker1 and k3s-worker2 that you want to join the cluster. SSH into k3s-worker1 and k3s-worker1 machine Switch to root user: sudo su Execute the below command to join the cluster using the token obtained from the master node: To install K3s on worker nodes and add them to the cluster, run the installation script with the K3S_URL and K3S_TOKEN environment variables. Here is an example showing how to join a worker node: curl -sfL https://get.k3s.io | K3S_URL=https://<Master_IP>:6443 \\ K3S_TOKEN=<Join_Token> sh - Where is the Internal IP of the master node and is the token obtained from the master node. For example, curl -sfL https://get.k3s.io | K3S_URL=https://192.168.0.154:6443 \\ K3S_TOKEN=K1019827f88b77cc5e1dce04d692d445c1015a578dafdc56aca829b2f 501df9359a::server:1bf0d61c85c6dac6d5a0081da55f44ba sh - You can verify if the k3s-agent on both of the worker nodes is running by: systemctl status k3s-agent The output looks like: To verify that our nodes have successfully been added to the cluster, run the following command on master node: k3s kubectl get nodes OR, k3s kubectl get nodes -o wide Your output should look like: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-worker1 Ready <none> 5m16s v1.21.5+k3s2 k3s-worker2 Ready <none> 5m5s v1.21.5+k3s2 k3s-master Ready control-plane,master 9m33s v1.21.5+k3s2 This shows that we have successfully setup our K3s cluster ready to deploy applications to it. Deploying Nginx using deployment Create a deployment nginx.yaml on master node vi nginx.yaml The nginx.yaml looks like this: apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite Note Instead of apply manually any new deployment yaml, you can just copy the yaml file to the /var/lib/rancher/k3s/server/manifests/ folder i.e. sudo cp nginx.yaml /var/lib/rancher/k3s/server/manifests/. . This will automatically deploy the newly copied deployment on your cluster. Deploy Addons to K3s K3s is a lightweight kubernetes tool that doesn\u2019t come packaged with all the tools but you can install them separately. Install Helm Commandline tool on K3s: i. Download the latest version of Helm commandline tool using wget from this page . wget https://get.helm.sh/helm-v3.7.0-linux-amd64.tar.gz ii. Unpack it: tar -zxvf helm-v3.7.0-linux-amd64.tar.gz iii. Find the helm binary in the unpacked directory, and move it to its desired destination mv linux-amd64/helm /usr/bin/helm chmod +x /usr/bin/helm OR, Using Snap: snap install helm --classic OR, Using Apt (Debian/Ubuntu): curl https://baltocdn.com/helm/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Verify the Helm installation: helm version version.BuildInfo{Version:\"v3.7.0\", GitCommit:\"eeac83883cb4014fe60267ec63735 70374ce770b\", GitTreeState:\"clean\", GoVersion:\"go1.16.8\"} Add the helm chart repository to allow installation of applications using helm: helm repo add stable https://charts.helm.sh/stable helm repo update Deploy A Sample Nginx Application using Helm Nginx can be used as a web proxy to expose ingress web traffic routes in and out of the cluster. You can install \"nginx web-proxy\" using Helm: export KUBECONFIG=/etc/rancher/k3s/k3s.yaml helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo list helm repo update helm install stable ingress-nginx/ingress-nginx --namespace kube-system \\ --set defaultBackend.enabled=false --set controller.publishService.enabled=true We can test if the application has been installed by: k3s kubectl get pods -n kube-system -l app=nginx-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx.. 1/1 Running 0 19m 10.42.1.5 k3s-worker1 <none> <none> We have successfully deployed nginx web-proxy on k3s. Go to browser, visit http://<Master-Floating-IP> i.e. http://128.31.25.246 to check the nginx default page. Upgrade K3s Using the Installation Script To upgrade K3s from an older version you can re-run the installation script using the same flags, for example: curl -sfL https://get.k3s.io | sh - This will upgrade to a newer version in the stable channel by default. If you want to upgrade to a newer version in a specific channel (such as latest) you can specify the channel: curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh - If you want to upgrade to a specific version you can run the following command: curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh - From non root user's terminal to install the latest version, you do not need to pass INSTALL_K3S_VERSION that by default loads the Latest version . curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--write-kubeconfig-mode 644\" \\ sh - Note For more about on \"How to use flags and environment variables\" read this . Restarting K3s Restarting K3s is supported by the installation script for systemd and OpenRC . Using systemd : To restart servers manually: sudo systemctl restart k3s To restart agents manually: sudo systemctl restart k3s-agent Using OpenRC : To restart servers manually: sudo service k3s restart To restart agents manually: sudo service k3s-agent restart Uninstalling If you installed K3s with the help of the install.sh script, an uninstall script is generated during installation. The script is created on your master node at /usr/bin/k3s-uninstall.sh or as k3s-agent-uninstall.sh on your worker nodes. To remove K3s on the worker nodes, execute: sudo /usr/bin/k3s-agent-uninstall.sh sudo rm -rf /var/lib/rancher To remove k3s on the master node, execute: sudo /usr/bin/k3s-uninstall.sh sudo rm -rf /var/lib/rancher","title":"K3s"},{"location":"openstack/kubernetes/k3s/k3s/#k3s","text":"","title":"K3s"},{"location":"openstack/kubernetes/k3s/k3s/#features","text":"Lightweight certified K8s distro Built for production operations 40MB binary, 250MB memeory consumption Single process w/ integrated K8s master, Kubelet, and containerd Supports not only etcd to hold the cluster state, but also SQLite (for single-node, simpler setups) or external DBs like MySQL and PostgreSQL Open source project","title":"Features"},{"location":"openstack/kubernetes/k3s/k3s/#components-and-architecure","text":"High-Availability K3s Server with an External DB: or, For this kind of high availability k3s setup read this .","title":"Components and architecure"},{"location":"openstack/kubernetes/k3s/k3s/#pre-requisite","text":"We will need 1 control-plane(master) and 2 worker nodes to create a single control-plane kubernetes cluster using k3s . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to the master node. 2 Linux machines for worker, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs.","title":"Pre-requisite"},{"location":"openstack/kubernetes/k3s/k3s/#networking","text":"The K3s server needs port 6443 to be accessible by all nodes. The nodes need to be able to reach other nodes over UDP port 8472 when Flannel VXLAN overlay networking is used. The node should not listen on any other port. K3s uses reverse tunneling such that the nodes make outbound connections to the server and all kubelet traffic runs through that tunnel. However, if you do not use Flannel and provide your own custom CNI, then port 8472 is not needed by K3s. If you wish to utilize the metrics server , you will need to open port 10250 on each node. If you plan on achieving high availability with embedded etcd , server nodes must be accessible to each other on ports 2379 and 2380 . Create 1 security group with appropriate Inbound Rules for K3s Server Nodes that will be used by all 3 nodes: Important Note The VXLAN overlay networking port on nodes should not be exposed to the world as it opens up your cluster network to be accessed by anyone. Run your nodes behind a firewall/security group that disables access to port 8472 . setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.235 k3s-master\" >> /etc/hosts hostnamectl set-hostname k3s-master In this step, you will install kubelet and kubeadm on the below nodes k3s-master k3s-worker1 k3s-worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl","title":"Networking"},{"location":"openstack/kubernetes/k3s/k3s/#install-docker","text":"Install container runtime - docker apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker","title":"Install Docker"},{"location":"openstack/kubernetes/k3s/k3s/#configure-k3s-to-bootstrap-the-cluster-on-master-node","text":"Run the below command on the master node i.e. k3s-master that you want to setup as control plane. SSH into k3s-master machine Switch to root user: sudo su Execute the below command to initialize the cluster: curl -sfL https://get.k3s.io | sh -s - --kubelet-arg 'cgroup-driver=systemd' \\ --node-taint CriticalAddonsOnly=true:NoExecute --docker OR, If you don't want to setup the K3s cluster without using docker as the container runtime, then just run without supplying the --docker argument. curl -sfL https://get.k3s.io | sh - More Installation Options After running this installation: The K3s service will be configured to automatically restart after node reboots or if the process crashes or is killed Additional utilities will be installed, including kubectl , crictl , ctr , k3s-killall.sh , and k3s-uninstall.sh A kubeconfig file will be written to /etc/rancher/k3s/k3s.yaml and the kubectl installed by K3s will automatically use it. To check if the service installed successfully, you can use: systemctl status k3s The output looks like: OR, k3s --version kubectl version Note If you want to taint the node i.e. not to deploy pods on this node after installation then run: kubectl taint nodes <master_node_name> k3s-controlplane=true:NoExecure i.e. kubectl taint nodes k3s-master k3s-controlplane=true:NoExecure You can check if the master node is working by: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-master Ready control-plane,master 37s v1.21.5+k3s2 kubectl config get-clusters NAME default kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:6443 CoreDNS is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get namespaces NAME STATUS AGE default Active 27m kube-system Active 27m kube-public Active 27m kube-node-lease Active 27m kubectl get endpoints -n kube-system NAME ENDPOINTS AGE kube-dns 10.42.0.4:53,10.42.0.4:53,10.42.0.4:9153 27m metrics-server 10.42.0.3:443 27m rancher.io-local-path <none> 27m kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE helm-install-traefik-crd-ql7j2 0/1 Pending 0 32m helm-install-traefik-mr65j 0/1 Pending 0 32m coredns-7448499f4d-x57z7 1/1 Running 0 32m metrics-server-86cbb8457f-cg2fs 1/1 Running 0 32m local-path-provisioner-5ff76fc89d-kdfcl 1/1 Running 0 32m You need to extract a token form the master that will be used to join the nodes to the master. On the master node: sudo cat /var/lib/rancher/k3s/server/node-token You will then obtain a token that looks like: K1097aace305b0c1077fc854547f34a598d23330ff047ddeed8beb3c428b38a1ca7::server:6cc9fbb6c5c9de96f37fb14b5535c778","title":"Configure K3s to bootstrap the cluster on master node"},{"location":"openstack/kubernetes/k3s/k3s/#configure-k3s-on-worker-nodes-to-join-the-cluster","text":"Run the below command on both of the worker nodes i.e. k3s-worker1 and k3s-worker2 that you want to join the cluster. SSH into k3s-worker1 and k3s-worker1 machine Switch to root user: sudo su Execute the below command to join the cluster using the token obtained from the master node: To install K3s on worker nodes and add them to the cluster, run the installation script with the K3S_URL and K3S_TOKEN environment variables. Here is an example showing how to join a worker node: curl -sfL https://get.k3s.io | K3S_URL=https://<Master_IP>:6443 \\ K3S_TOKEN=<Join_Token> sh - Where is the Internal IP of the master node and is the token obtained from the master node. For example, curl -sfL https://get.k3s.io | K3S_URL=https://192.168.0.154:6443 \\ K3S_TOKEN=K1019827f88b77cc5e1dce04d692d445c1015a578dafdc56aca829b2f 501df9359a::server:1bf0d61c85c6dac6d5a0081da55f44ba sh - You can verify if the k3s-agent on both of the worker nodes is running by: systemctl status k3s-agent The output looks like: To verify that our nodes have successfully been added to the cluster, run the following command on master node: k3s kubectl get nodes OR, k3s kubectl get nodes -o wide Your output should look like: k3s kubectl get nodes NAME STATUS ROLES AGE VERSION k3s-worker1 Ready <none> 5m16s v1.21.5+k3s2 k3s-worker2 Ready <none> 5m5s v1.21.5+k3s2 k3s-master Ready control-plane,master 9m33s v1.21.5+k3s2 This shows that we have successfully setup our K3s cluster ready to deploy applications to it.","title":"Configure K3s on worker nodes to join the cluster"},{"location":"openstack/kubernetes/k3s/k3s/#deploying-nginx-using-deployment","text":"Create a deployment nginx.yaml on master node vi nginx.yaml The nginx.yaml looks like this: apiVersion: apps/v1 kind: Deployment metadata: name: mysite labels: app: mysite spec: replicas: 1 selector: matchLabels: app: mysite template: metadata: labels: app : mysite spec: containers: - name : mysite image: nginx ports: - containerPort: 80 kubectl apply -f nginx.yaml Verify the nginx pod is on Running state: sudo k3s kubectl get pods --all-namespaces Scale the pods to available agents: sudo k3s kubectl scale --replicas=2 deploy/mysite View all deployment status: sudo k3s kubectl get deploy mysite NAME READY UP-TO-DATE AVAILABLE AGE mysite 2/2 2 2 85s Delete the nginx deployment and pod: sudo k3s kubectl delete -f nginx.yaml OR, sudo k3s kubectl delete deploy mysite Note Instead of apply manually any new deployment yaml, you can just copy the yaml file to the /var/lib/rancher/k3s/server/manifests/ folder i.e. sudo cp nginx.yaml /var/lib/rancher/k3s/server/manifests/. . This will automatically deploy the newly copied deployment on your cluster.","title":"Deploying Nginx using deployment"},{"location":"openstack/kubernetes/k3s/k3s/#deploy-addons-to-k3s","text":"K3s is a lightweight kubernetes tool that doesn\u2019t come packaged with all the tools but you can install them separately. Install Helm Commandline tool on K3s: i. Download the latest version of Helm commandline tool using wget from this page . wget https://get.helm.sh/helm-v3.7.0-linux-amd64.tar.gz ii. Unpack it: tar -zxvf helm-v3.7.0-linux-amd64.tar.gz iii. Find the helm binary in the unpacked directory, and move it to its desired destination mv linux-amd64/helm /usr/bin/helm chmod +x /usr/bin/helm OR, Using Snap: snap install helm --classic OR, Using Apt (Debian/Ubuntu): curl https://baltocdn.com/helm/signing.asc | sudo apt-key add - sudo apt-get install apt-transport-https --yes echo \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm Verify the Helm installation: helm version version.BuildInfo{Version:\"v3.7.0\", GitCommit:\"eeac83883cb4014fe60267ec63735 70374ce770b\", GitTreeState:\"clean\", GoVersion:\"go1.16.8\"} Add the helm chart repository to allow installation of applications using helm: helm repo add stable https://charts.helm.sh/stable helm repo update","title":"Deploy Addons to K3s"},{"location":"openstack/kubernetes/k3s/k3s/#deploy-a-sample-nginx-application-using-helm","text":"Nginx can be used as a web proxy to expose ingress web traffic routes in and out of the cluster. You can install \"nginx web-proxy\" using Helm: export KUBECONFIG=/etc/rancher/k3s/k3s.yaml helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo list helm repo update helm install stable ingress-nginx/ingress-nginx --namespace kube-system \\ --set defaultBackend.enabled=false --set controller.publishService.enabled=true We can test if the application has been installed by: k3s kubectl get pods -n kube-system -l app=nginx-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx.. 1/1 Running 0 19m 10.42.1.5 k3s-worker1 <none> <none> We have successfully deployed nginx web-proxy on k3s. Go to browser, visit http://<Master-Floating-IP> i.e. http://128.31.25.246 to check the nginx default page.","title":"Deploy A Sample Nginx Application using Helm"},{"location":"openstack/kubernetes/k3s/k3s/#upgrade-k3s-using-the-installation-script","text":"To upgrade K3s from an older version you can re-run the installation script using the same flags, for example: curl -sfL https://get.k3s.io | sh - This will upgrade to a newer version in the stable channel by default. If you want to upgrade to a newer version in a specific channel (such as latest) you can specify the channel: curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh - If you want to upgrade to a specific version you can run the following command: curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh - From non root user's terminal to install the latest version, you do not need to pass INSTALL_K3S_VERSION that by default loads the Latest version . curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"--write-kubeconfig-mode 644\" \\ sh - Note For more about on \"How to use flags and environment variables\" read this .","title":"Upgrade K3s Using the Installation Script"},{"location":"openstack/kubernetes/k3s/k3s/#restarting-k3s","text":"Restarting K3s is supported by the installation script for systemd and OpenRC . Using systemd : To restart servers manually: sudo systemctl restart k3s To restart agents manually: sudo systemctl restart k3s-agent Using OpenRC : To restart servers manually: sudo service k3s restart To restart agents manually: sudo service k3s-agent restart","title":"Restarting K3s"},{"location":"openstack/kubernetes/k3s/k3s/#uninstalling","text":"If you installed K3s with the help of the install.sh script, an uninstall script is generated during installation. The script is created on your master node at /usr/bin/k3s-uninstall.sh or as k3s-agent-uninstall.sh on your worker nodes. To remove K3s on the worker nodes, execute: sudo /usr/bin/k3s-agent-uninstall.sh sudo rm -rf /var/lib/rancher To remove k3s on the master node, execute: sudo /usr/bin/k3s-uninstall.sh sudo rm -rf /var/lib/rancher","title":"Uninstalling"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/","text":"Highly Available Kubernetes Cluster using kubeadm Objectives Install a multi control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components Components and architecure This shows components and architecture of a highly-available, production-grade Kubernetes cluster. You can learn about each component from Kubernetes Componets . Pre-requisite You will need 2 control-plane(master node) and 2 worker nodes to create a multi-master kubernetes cluster using kubeadm . You are going to use the following set up for this purpose: 2 Linux machines for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage 2 Linux machines for worker, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage - also assign Floating IPs to both of the worker nodes. 1 Linux machine for loadbalancer, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 loadbalancer\" >> /etc/hosts hostnamectl set-hostname loadbalancer Steps Prepare the Loadbalancer node to communicate with the two master nodes' apiservers on their IPs via port 6443. Do following in all the nodes except the Loadbalancer node: Disable swap. Install kubelet and kubeadm . Install container runtime - you will be using Docker . Initiate kubeadm control plane configuration on one of the master nodes. Save the new master and worker node join commands with the token. Join the second master node to the control plane using the join command. Join the worker nodes to the control plane using the join command. Configure kubeconfig( $HOME/.kube/config ) on loadbalancer node. Install kubectl on Loadbalancer node. Install CNI network plugin i.e. Flannel on Loadbalancer node. Validate all cluster components and nodes are visible on Loadbalancer node. Deploy a sample app and validate the app from Loadbalancer node. Setting up loadbalancer You will use HAPROXY as the primary loadbalancer, but you can use any other options as well. This node will be not part of the K8s cluster but will be outside of the cluster and interacts with the cluster using ports. You have 2 master nodes. Which means the user can connect to either of the 2 apiservers. The loadbalancer will be used to loadbalance between the 2 apiservers. Login to the loadbalancer node Switch as root - sudo su Update your repository and your system sudo apt-get update && sudo apt-get upgrade -y Install haproxy sudo apt-get install haproxy -y Edit haproxy configuration vi /etc/haproxy/haproxy.cfg Add the below lines to create a frontend configuration for loadbalancer - frontend fe-apiserver bind 0.0.0.0:6443 mode tcp option tcplog default_backend be-apiserver Add the below lines to create a backend configuration for master1 and master2 nodes at port 6443 . Note 6443 is the default port of kube-apiserver backend be-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 #<!-- markdownlint-disable --> server master1 10.138.0.15:6443 check server master2 10.138.0.16:6443 check Here - master1 and master2 are the hostnames of the master nodes and 10.138.0.15 and 10.138.0.16 are the corresponding internal IP addresses. Restart and Verify haproxy systemctl restart haproxy systemctl status haproxy Ensure haproxy config file is correctly formatted: haproxy -c -q -V -f /etc/haproxy/haproxy.cfg Ensure haproxy is in running status. Run nc command as below: nc -v localhost 6443 Connection to localhost 6443 port [tcp/*] succeeded! Note If you see failures for master1 and master2 connectivity, you can ignore them for time being as you have not yet installed anything on the servers. Install kubeadm, kubelet and docker on master and worker nodes kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master1 master2 worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 4 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet and kubeadm apt-get update apt-get install -y kubelet kubeadm apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm Install Docker on master and worker nodes Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1 Configure kubeadm to bootstrap the cluster You will start off by initializing only one master node. For this purpose, you choose master1 to initialize our first control plane but you can also do the same in master2 . SSH into master1 machine Switch to root user: sudo su Execute the below command to initialize the cluster: kubeadm init --control-plane-endpoint \"LOAD_BALANCER_IP_OR_HOSTNAME:LOAD_BALANCER_PORT\" --upload-certs --pod-network-cidr=10.244.0.0/16 Here, you can use either the IP address or the hostname of the loadbalancer in place of . You have not enabled the hostname of the server, i.e. loadbalancer as the LOAD_BALANCER_IP_OR_HOSTNAME that is visible from the master1 node. so instead of using not resolvable hostnames across your network, you will be using the IP address of the Loadbalancer server. The is the front end configuration port defined in HAPROXY configuration. For this, you have kept the port as 6443 which is the default apiserver port. Important Note --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with loadbalancer node with internal IP: 192.168.0.167 look like below: kubeadm init --control-plane-endpoint \"192.168.0.167:6443\" --upload-certs --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 3 major tasks: Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf Setup a new control plane (master) i.e. master2 by running following command on master2 node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e1 5ee37ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> SSH into master2 Switch to root user: sudo su Check the command provided by the output of master1 : You can now use the below command to add another control-plane node(master) to the control plane: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Execute the kubeadm join command for control plane on master2 Your output should look like: This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane (master) label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. Now that you have initialized both the masters - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master1 to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Configure kubeconfig on loadbalancer node Now that you have configured the master and the worker nodes, its now time to configure Kubeconfig ( .kube ) on the loadbalancer node. It is completely up to you if you want to use the loadbalancer node to setup kubeconfig. kubeconfig can also be setup externally on a separate machine which has access to loadbalancer node. For the purpose of this demo you will use loadbalancer node to host kubeconfig and kubectl . SSH into loadbalancer node Switch to root user: sudo su Create a directory: .kube at $HOME of root user mkdir -p $HOME/.kube SCP configuration file from any one master node to loadbalancer node scp master1:/etc/kubernetes/admin.conf $HOME/.kube/config Important Note If you havent setup ssh connection between master node and loadbalancer, you can manually copy content of the file /etc/kubernetes/admin.conf from master1 node and then past it to $HOME/.kube/config file on the loadbalancer node. Ensure that the kubeconfig file path is $HOME/.kube/config on the loadbalancer node. Provide appropriate ownership to the copied file chown $(id -u):$(id -g) $HOME/.kube/config Install kubectl Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,master 21m v1.16.2 master2 NotReady control-plane,master 15m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2 Install CNI network plugin CNI overview Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@loadbalancer:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m Supported CNI options To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the Loadbalancer node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready control-plane,master 22m v1.16.2 master2 Ready control-plane,master 17m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2 Deploy A Sample Nginx Application Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP. To check which worker node is serving nginx , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. For your example, Deploy A K8s Dashboard You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into loadbalancer node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port on the running node. The assigned NodePort can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard. Watch Demo Video Here\u2019s a recorded demo video on how to setup HA K8s cluster using kubeadm . Clean Up To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster How to Remove the node? Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"Creating a HA cluster with kubeadm"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#highly-available-kubernetes-cluster-using-kubeadm","text":"","title":"Highly Available Kubernetes Cluster using kubeadm"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#objectives","text":"Install a multi control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components","title":"Objectives"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#components-and-architecure","text":"This shows components and architecture of a highly-available, production-grade Kubernetes cluster. You can learn about each component from Kubernetes Componets .","title":"Components and architecure"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#pre-requisite","text":"You will need 2 control-plane(master node) and 2 worker nodes to create a multi-master kubernetes cluster using kubeadm . You are going to use the following set up for this purpose: 2 Linux machines for master, ubuntu-21.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage 2 Linux machines for worker, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage - also assign Floating IPs to both of the worker nodes. 1 Linux machine for loadbalancer, ubuntu-21.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 loadbalancer\" >> /etc/hosts hostnamectl set-hostname loadbalancer","title":"Pre-requisite"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#steps","text":"Prepare the Loadbalancer node to communicate with the two master nodes' apiservers on their IPs via port 6443. Do following in all the nodes except the Loadbalancer node: Disable swap. Install kubelet and kubeadm . Install container runtime - you will be using Docker . Initiate kubeadm control plane configuration on one of the master nodes. Save the new master and worker node join commands with the token. Join the second master node to the control plane using the join command. Join the worker nodes to the control plane using the join command. Configure kubeconfig( $HOME/.kube/config ) on loadbalancer node. Install kubectl on Loadbalancer node. Install CNI network plugin i.e. Flannel on Loadbalancer node. Validate all cluster components and nodes are visible on Loadbalancer node. Deploy a sample app and validate the app from Loadbalancer node.","title":"Steps"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#setting-up-loadbalancer","text":"You will use HAPROXY as the primary loadbalancer, but you can use any other options as well. This node will be not part of the K8s cluster but will be outside of the cluster and interacts with the cluster using ports. You have 2 master nodes. Which means the user can connect to either of the 2 apiservers. The loadbalancer will be used to loadbalance between the 2 apiservers. Login to the loadbalancer node Switch as root - sudo su Update your repository and your system sudo apt-get update && sudo apt-get upgrade -y Install haproxy sudo apt-get install haproxy -y Edit haproxy configuration vi /etc/haproxy/haproxy.cfg Add the below lines to create a frontend configuration for loadbalancer - frontend fe-apiserver bind 0.0.0.0:6443 mode tcp option tcplog default_backend be-apiserver Add the below lines to create a backend configuration for master1 and master2 nodes at port 6443 . Note 6443 is the default port of kube-apiserver backend be-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 #<!-- markdownlint-disable --> server master1 10.138.0.15:6443 check server master2 10.138.0.16:6443 check Here - master1 and master2 are the hostnames of the master nodes and 10.138.0.15 and 10.138.0.16 are the corresponding internal IP addresses. Restart and Verify haproxy systemctl restart haproxy systemctl status haproxy Ensure haproxy config file is correctly formatted: haproxy -c -q -V -f /etc/haproxy/haproxy.cfg Ensure haproxy is in running status. Run nc command as below: nc -v localhost 6443 Connection to localhost 6443 port [tcp/*] succeeded! Note If you see failures for master1 and master2 connectivity, you can ignore them for time being as you have not yet installed anything on the servers.","title":"Setting up loadbalancer"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-kubeadm-kubelet-and-docker-on-master-and-worker-nodes","text":"kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master1 master2 worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 4 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet and kubeadm apt-get update apt-get install -y kubelet kubeadm apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm","title":"Install kubeadm, kubelet and docker on master and worker nodes"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-docker-on-master-and-worker-nodes","text":"Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1","title":"Install Docker on master and worker nodes"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#configure-kubeadm-to-bootstrap-the-cluster","text":"You will start off by initializing only one master node. For this purpose, you choose master1 to initialize our first control plane but you can also do the same in master2 . SSH into master1 machine Switch to root user: sudo su Execute the below command to initialize the cluster: kubeadm init --control-plane-endpoint \"LOAD_BALANCER_IP_OR_HOSTNAME:LOAD_BALANCER_PORT\" --upload-certs --pod-network-cidr=10.244.0.0/16 Here, you can use either the IP address or the hostname of the loadbalancer in place of . You have not enabled the hostname of the server, i.e. loadbalancer as the LOAD_BALANCER_IP_OR_HOSTNAME that is visible from the master1 node. so instead of using not resolvable hostnames across your network, you will be using the IP address of the Loadbalancer server. The is the front end configuration port defined in HAPROXY configuration. For this, you have kept the port as 6443 which is the default apiserver port. Important Note --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with loadbalancer node with internal IP: 192.168.0.167 look like below: kubeadm init --control-plane-endpoint \"192.168.0.167:6443\" --upload-certs --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 3 major tasks: Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf Setup a new control plane (master) i.e. master2 by running following command on master2 node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e1 5ee37ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> SSH into master2 Switch to root user: sudo su Check the command provided by the output of master1 : You can now use the below command to add another control-plane node(master) to the control plane: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee3 7ab9834567333b939458a5bfb5 \\ --control-plane --certificate-key 824d9a0e173a810416b4bca7038fb33b616108c17abcbc5eaef8651f11e3d146 Execute the kubeadm join command for control plane on master2 Your output should look like: This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane (master) label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. Now that you have initialized both the masters - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master1 to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details.","title":"Configure kubeadm to bootstrap the cluster"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#configure-kubeconfig-on-loadbalancer-node","text":"Now that you have configured the master and the worker nodes, its now time to configure Kubeconfig ( .kube ) on the loadbalancer node. It is completely up to you if you want to use the loadbalancer node to setup kubeconfig. kubeconfig can also be setup externally on a separate machine which has access to loadbalancer node. For the purpose of this demo you will use loadbalancer node to host kubeconfig and kubectl . SSH into loadbalancer node Switch to root user: sudo su Create a directory: .kube at $HOME of root user mkdir -p $HOME/.kube SCP configuration file from any one master node to loadbalancer node scp master1:/etc/kubernetes/admin.conf $HOME/.kube/config Important Note If you havent setup ssh connection between master node and loadbalancer, you can manually copy content of the file /etc/kubernetes/admin.conf from master1 node and then past it to $HOME/.kube/config file on the loadbalancer node. Ensure that the kubeconfig file path is $HOME/.kube/config on the loadbalancer node. Provide appropriate ownership to the copied file chown $(id -u):$(id -g) $HOME/.kube/config","title":"Configure kubeconfig on loadbalancer node"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-kubectl","text":"Install kubectl binary \u2022 kubectl : the command line util to talk to your cluster. snap install kubectl --classic Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master1 NotReady control-plane,master 21m v1.16.2 master2 NotReady control-plane,master 15m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2","title":"Install kubectl"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#install-cni-network-plugin","text":"","title":"Install CNI network plugin"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#cni-overview","text":"Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@loadbalancer:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m","title":"CNI overview"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#supported-cni-options","text":"To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the Loadbalancer node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master1 Ready control-plane,master 22m v1.16.2 master2 Ready control-plane,master 17m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2","title":"Supported CNI options"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#deploy-a-sample-nginx-application","text":"Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from either of the worker nodes' Floating IP. To check which worker node is serving nginx , you can check NODE column running the following command: kubectl get pods --all-namespaces --output wide OR, kubectl get pods -A -o wide Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. For your example,","title":"Deploy A Sample Nginx Application"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#deploy-a-k8s-dashboard","text":"You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into loadbalancer node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port on the running node. The assigned NodePort can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system Go to browser, visit http://<Worker-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard.","title":"Deploy A K8s Dashboard"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#watch-demo-video","text":"Here\u2019s a recorded demo video on how to setup HA K8s cluster using kubeadm .","title":"Watch Demo Video"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#clean-up","text":"To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster","title":"Clean Up"},{"location":"openstack/kubernetes/kubeadm/HA-clusters-with-kubeadm/#how-to-remove-the-node","text":"Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"How to Remove the node?"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/","text":"Creating a Single Master cluster with kubeadm Objectives Install a single control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components Components and architecure You can learn about each component from Kubernetes Componets . Pre-requisite We will need 1 control-plane(master) and 2 worker node to create a single control-plane kubernetes cluster using kubeadm . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-20.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to the master node. 2 Linux machines for worker, ubuntu-20.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 master\" >> /etc/hosts hostnamectl set-hostname master Steps Disable swap on all nodes. Install kubeadm , kubelet , and kubectl on all the nodes. Install container runtime on all nodes- you will be using Docker . Initiate kubeadm control plane configuration on the master node. Save the worker node join command with the token. Install CNI network plugin i.e. Flannel on master node. Join the worker node to the master node (control plane) using the join command. Validate all cluster components and nodes are visible on master node. Deploy a sample app and validate the app from master node. Install kubeadm, kubelet and docker on master and worker nodes kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet, kubeadm, and kubectl apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm kubectl Install Docker Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1 Configure kubeadm to bootstrap the cluster on master node Run the below command on the master node i.e. master that you want to setup as control plane. SSH into master machine Switch to root user: sudo su Execute the below command to initialize the cluster: export MASTER_IP=<Master-Internal-IP> kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Important Note Please make sure you replace the correct IP of the node with <Master-Internal-IP> which is the Internal IP of master node. --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with mster node with internal IP: 192.168.0.167 look like below: For example, export MASTER_IP=192.168.0.167 kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 2 major tasks: Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> Now that you have initialized the master - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Validate all cluster components and nodes are visible on all nodes Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady control-plane,master 21m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2 Install CNI network plugin CNI overview Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@master:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m Supported CNI options To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the master node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 22m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2 Watch Recoded Video Here\u2019s a quick recorded demo video upto this point where we successfully setup single master K8s cluster using Kubeadm. Deploy A Sample Nginx Application Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from the master node's Floating IP. Go to browser, visit http://<Master-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. For your example, Deploy A K8s Dashboard You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into master node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port from the master node. The assigned NodePort on the master node can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system Go to browser, visit http://<Master-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard. Watch Demo Video Here\u2019s a recorded demo video on how to deploy applications on top of setup single master K8s cluster as explained above. Clean Up To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster How to Remove the node? Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"Bootstrapping cluster with kubeadm"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#creating-a-single-master-cluster-with-kubeadm","text":"","title":"Creating a Single Master cluster with kubeadm"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#objectives","text":"Install a single control-plane(master) Kubernetes cluster Install a Pod network on the cluster so that your Pods can talk to each other Deploy and test a sample app Deploy K8s Dashboard to view all cluster's components","title":"Objectives"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#components-and-architecure","text":"You can learn about each component from Kubernetes Componets .","title":"Components and architecure"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#pre-requisite","text":"We will need 1 control-plane(master) and 2 worker node to create a single control-plane kubernetes cluster using kubeadm . We are using following setting for this purpose: 1 Linux machine for master, ubuntu-20.04-x86_64, m1.medium flavor with 2vCPU, 4GB RAM, 10GB storage - also assign Floating IP to the master node. 2 Linux machines for worker, ubuntu-20.04-x86_64, m1.small flavor with 1vCPU, 2GB RAM, 10GB storage. ssh access to all machines: Read more here on how to setup SSH to your remote VMs. Create 2 security groups with appropriate ports and protocols : i. To be used by the master nodes: ii. To be used by the worker nodes: - setup Unique hostname to each machine using the following command: echo \"<node_internal_IP> <host_name>\" >> /etc/hosts hostnamectl set-hostname <host_name> For example, echo \"192.168.0.167 master\" >> /etc/hosts hostnamectl set-hostname master","title":"Pre-requisite"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#steps","text":"Disable swap on all nodes. Install kubeadm , kubelet , and kubectl on all the nodes. Install container runtime on all nodes- you will be using Docker . Initiate kubeadm control plane configuration on the master node. Save the worker node join command with the token. Install CNI network plugin i.e. Flannel on master node. Join the worker node to the master node (control plane) using the join command. Validate all cluster components and nodes are visible on master node. Deploy a sample app and validate the app from master node.","title":"Steps"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-kubeadm-kubelet-and-docker-on-master-and-worker-nodes","text":"kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. You will install these packages on all of your machines: \u2022 kubeadm : the command to bootstrap the cluster. \u2022 kubelet : the component that runs on all of the machines in your cluster and does things like starting pods and containers. \u2022 kubectl : the command line util to talk to your cluster. In this step, you will install kubelet and kubeadm on the below nodes master worker1 worker2 The below steps will be performed on all the above mentioned nodes: SSH into all the 3 machines Switch as root: sudo su Update the repositories and packages: apt-get update && apt-get upgrade -y Turn off swap swapoff -a sed -i '/ swap / s/^/#/' /etc/fstab Install curl and apt-transport-https apt-get update && apt-get install -y apt-transport-https curl Download the Google Cloud public signing key and add key to verify releases curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - add kubernetes apt repo cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install kubelet, kubeadm, and kubectl apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold is used so that these packages will not be updated/removed automatically apt-mark hold kubelet kubeadm kubectl","title":"Install kubeadm, kubelet and docker on master and worker nodes"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-docker","text":"Install container runtime - docker sudo apt-get install docker.io -y Configure the Docker daemon, in particular to use systemd for the management of the container\u2019s cgroups cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF systemctl enable --now docker usermod -aG docker ubuntu systemctl daemon-reload systemctl restart docker Ensure net.bridge.bridge-nf-call-iptables is set to 1 in your sysctl config For more Read this . sysctl net.bridge.bridge-nf-call-iptables=1","title":"Install Docker"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#configure-kubeadm-to-bootstrap-the-cluster-on-master-node","text":"Run the below command on the master node i.e. master that you want to setup as control plane. SSH into master machine Switch to root user: sudo su Execute the below command to initialize the cluster: export MASTER_IP=<Master-Internal-IP> kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Important Note Please make sure you replace the correct IP of the node with <Master-Internal-IP> which is the Internal IP of master node. --pod-network-cidr value depends upon what CNI plugin you going to use so need to be very careful while setting this CIDR values. In our case, you are going to use Flannel CNI network plugin so you will use: --pod-network-cidr=10.244.0.0/16 . If you are opted to use Calico CNI network plugin then you need to use: --pod-network-cidr=192.168.0.0/16 and if you are opted to use Weave Net no need to pass this parameter. For example, our Flannel CNI network plugin based kubeadm init command with mster node with internal IP: 192.168.0.167 look like below: For example, export MASTER_IP=192.168.0.167 kubeadm config images pull kubeadm init --apiserver-advertise-address=${MASTER_IP} --pod-network-cidr=10.244.0.0/16 Save the output in some secure file for future use. This will show an unique token to join the control plane. The output from kubeadm init should looks like below: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 The output consists of 2 major tasks: Setup kubeconfig using on current master node: As you are running as root user so you need to run the following command: export KUBECONFIG=/etc/kubernetes/admin.conf Join worker nodes running following command on individual workder nodes: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Important Note Your output will be different than what is provided here. While performing the rest of the demo, ensure that you are executing the command provided by your output and dont copy and paste from here. If you do not have the token, you can get it by running the following command on the control-plane node: kubeadm token list The output is similar to this: TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p... 23h 2018-06-12 authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token If you missed the join command, execute the following command kubeadm token create --print-join-command in the master node to recreate the token with the join command. root@master:~$ kubeadm token create --print-join-command kubeadm join 10.2.0.4:6443 --token xyzeyi.wxer3eg9vj8hcpp2 \\ --discovery-token-ca-cert-hash sha256:ccfc92b2a31b002c3151cdbab77ff4dc32ef13b213fa3a9876e126831c76f7fa By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control-plane node: kubeadm token create The output is similar to this: 5didvk.d09sbcov8ph2amjw We can use this new token to join: $ kubeadm join <master-ip>:<master-port> --token <token> \\ --discovery-token-ca-cert-hash sha256:<hash> Now that you have initialized the master - you can now work on bootstrapping the worker nodes. SSH into worker1 and worker2 Switch to root user on both the machines: sudo su Check the output given by the init command on master to join worker node: kubeadm join 192.168.0.167:6443 --token cnslau.kd5fjt96jeuzymzb \\ --discovery-token-ca-cert-hash sha256:871ab3f050bc9790c977daee9e44cf52e15ee37ab9834567333b939458a5bfb5 Execute the above command on both the nodes: Your output should look like: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details.","title":"Configure kubeadm to bootstrap the cluster on master node"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#validate-all-cluster-components-and-nodes-are-visible-on-all-nodes","text":"Verify the cluster kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady control-plane,master 21m v1.16.2 worker1 Ready <none> 9m17s v1.16.2 worker2 Ready <none> 9m25s v1.16.2","title":"Validate all cluster components and nodes are visible on all nodes"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#install-cni-network-plugin","text":"","title":"Install CNI network plugin"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#cni-overview","text":"Managing a network where containers can interoperate efficiently is very important. Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster. This relatively simple specification makes it easy for Kubernetes to interact with a wide range of CNI-based software solutions. Using this CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file if applicable. You must deploy a CNI based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed. To verify you can run this command: kubectl get po -n kube-system : You should see the following output. You will see the two coredns-* pods in a pending state. It is the expected behavior. Once we install the network plugin, it will be in a Running state. Output Example: root@master:~$ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-558bd4d5db-5jktc 0/1 Pending 0 10m coredns-558bd4d5db-xdc5x 0/1 Pending 0 10m etcd-master1 1/1 Running 0 11m kube-apiserver-master1 1/1 Running 0 11m kube-controller-manager-master1 1/1 Running 0 11m kube-proxy-5jfh5 1/1 Running 0 10m kube-scheduler-master1 1/1 Running 0 11m","title":"CNI overview"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#supported-cni-options","text":"To read more about the currently supported base CNI solutions for Kubernetes read here and also read this . The below command can be run on the master node to install the CNI plugin: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml As you had passed --pod-network-cidr=10.244.0.0/16 with kubeadm init so this should work for Flannel CNI. Using Other CNI Options For Calico CNI plugin to work correctly, you need to pass --pod-network-cidr=192.168.0.0/16 with kubeadm init and then you can run: kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml For Weave Net CNI plugin to work correctly, you don't need to pass --pod-network-cidr with kubeadm init and then you can run: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" Dual Network: It is highly recommended to follow an internal/external network layout for your cluster, as showed in this diagram: To enable this just give two different names to the internal and external interface, according to your distro of choiche naming scheme: external_interface: eth0 internal_interface: eth1 Also you can decide here what CIDR should your cluster use cluster_cidr: 10.43.0.0/16 service_cidr: 10.44.0.0/16 Once you successfully installed the Flannel CNI component to your cluster. You can now verify your HA cluster running: kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 22m v1.16.2 worker1 Ready <none> 10m v1.16.2 worker2 Ready <none> 10m v1.16.2","title":"Supported CNI options"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#watch-recoded-video","text":"Here\u2019s a quick recorded demo video upto this point where we successfully setup single master K8s cluster using Kubeadm.","title":"Watch Recoded Video"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#deploy-a-sample-nginx-application","text":"Now that we have all the components to make the cluster and applications work, let\u2019s deploy a sample Nginx application and see if we can access it over a NodePort that has port range of 30000-32767 . The below command can be run on: kubectl run nginx --image=nginx --port=80 kubectl expose pod nginx --port=80 --type=NodePort To check which NodePort is opened and running the Nginx run: kubectl get svc The output will show: Once the deployment is up, you should be able to access the Nginx home page on the allocated NodePort from the master node's Floating IP. Go to browser, visit http://<Master-Floating-IP>:<NodePort> i.e. http://128.31.25.246:32713 to check the nginx default page. For your example,","title":"Deploy A Sample Nginx Application"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#deploy-a-k8s-dashboard","text":"You will going to setup K8dash/Skooner to view a dashboard that shows all your K8s cluster components. SSH into master node Switch to root user: sudo su Apply available deployment by running the following command: kubectl apply -f https://raw.githubusercontent.com/skooner-k8s/skooner/master/kubernetes-skooner-nodeport.yaml This will map Skooner port 4654 to a randomly selected port from the master node. The assigned NodePort on the master node can be found running: kubectl get svc --namespace=kube-system OR, kubectl get po,svc -n kube-system Go to browser, visit http://<Master-Floating-IP>:<NodePort> i.e. http://128.31.25.246:30495 to check the skooner dashboard page. Setup the Service Account Token to access the Skooner Dashboard: The first (and easiest) option is to create a dedicated service account. Run the following commands: Create the service account in the current namespace (we assume default) kubectl create serviceaccount skooner-sa Give that service account root on the cluster kubectl create clusterrolebinding skooner-sa --clusterrole=cluster-admin --serviceaccount=default:skooner-sa Find the secret that was created to hold the token for the SA kubectl get secrets Show the contents of the secret to extract the token kubectl describe secret skooner-sa-token-xxxxx Copy the token value from the secret and enter it into the login screen to access the dashboard.","title":"Deploy A K8s Dashboard"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#watch-demo-video","text":"Here\u2019s a recorded demo video on how to deploy applications on top of setup single master K8s cluster as explained above.","title":"Watch Demo Video"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#clean-up","text":"To view the Cluster info: kubectl cluster-info To delete your local references to the cluster: kubectl config delete-cluster","title":"Clean Up"},{"location":"openstack/kubernetes/kubeadm/single-master-clusters-with-kubeadm/#how-to-remove-the-node","text":"Talking to the control-plane node with the appropriate credentials, run: kubectl drain <node name> --delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm: kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command: ipvsadm -C Now remove the node: kubectl delete node <node name> If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.","title":"How to Remove the node?"},{"location":"openstack/logging-in/access-the-openstack-dashboard/","text":"Access the OpenStack Dashboard The OpenStack Dashboard which is a web-based graphical interface, code named Horizon, is located at https://stack.nerc.mghpcc.org . The NERC Authentication supports CILogon using Keycloak for gateway authentication and authorization that provides federated login via your institution accounts and it is the recommended authentication method. Select \"OpenID Connect\" as shown here: and then this will redirect you to MSS Keycloak login page where you will see user login or sign in option using \"CILogon\". CILogon facilitates secure access to CyberInfrastructure (CI). Click \"cilogon\". Next, you will redirected to CILogon welcome page as shown below: MSS Keycloak will requests access to the following information: Your CILogon user identifier Your name Your email address Your username and affiliation from your identity provider which are required in order to allow access your account to NERC's OpenStack dashboard. From Select an Identity Provider dropdown option, please select your institution's name. If you would like to remember your selected institution name for future logins please check the \"Remember this selection\" checkbox this will bypass the CILogon welcome page on subsequent visits and proceed directly to the selected insitution's identity provider(IdP). Click \"Log On\". This will redirect to your respective institutional login page where you need to enter your institutional credentials. Important Note The NERC does not see or have access to your institutional account credentials and neither store them rather it just point to your selected insitution's identity provider and redirects back once authenticated. Once you successfully authenticated you should see an overview of the resources like Compute (instances, VCPUs, RAM, etc.), Volume and Network. You can also see usage summary for provided date range.","title":"Access the OpenStack Dashboard"},{"location":"openstack/logging-in/access-the-openstack-dashboard/#access-the-openstack-dashboard","text":"The OpenStack Dashboard which is a web-based graphical interface, code named Horizon, is located at https://stack.nerc.mghpcc.org . The NERC Authentication supports CILogon using Keycloak for gateway authentication and authorization that provides federated login via your institution accounts and it is the recommended authentication method. Select \"OpenID Connect\" as shown here: and then this will redirect you to MSS Keycloak login page where you will see user login or sign in option using \"CILogon\". CILogon facilitates secure access to CyberInfrastructure (CI). Click \"cilogon\". Next, you will redirected to CILogon welcome page as shown below: MSS Keycloak will requests access to the following information: Your CILogon user identifier Your name Your email address Your username and affiliation from your identity provider which are required in order to allow access your account to NERC's OpenStack dashboard. From Select an Identity Provider dropdown option, please select your institution's name. If you would like to remember your selected institution name for future logins please check the \"Remember this selection\" checkbox this will bypass the CILogon welcome page on subsequent visits and proceed directly to the selected insitution's identity provider(IdP). Click \"Log On\". This will redirect to your respective institutional login page where you need to enter your institutional credentials. Important Note The NERC does not see or have access to your institutional account credentials and neither store them rather it just point to your selected insitution's identity provider and redirects back once authenticated. Once you successfully authenticated you should see an overview of the resources like Compute (instances, VCPUs, RAM, etc.), Volume and Network. You can also see usage summary for provided date range.","title":"Access the OpenStack Dashboard"},{"location":"openstack/logging-in/dashboard-overview/","text":"Dashboard Overview When you logged-in, you will redirected to the Compute panel which is under the Project tab. In the top bar, you can see the two small tabs: \"Project\" and \"Identity\". Beneath that you can see six panels in larger print: \"Project\", \"Compute\", \"Volumes\", \"Network\", \"Orchestration\", and \"Object Store\". Project Panel Navigate: Project -> Project API Access: View API endpoints. Compute Panel Navigate: Project -> Compute Overview: View reports for the project. Instances: View, launch, create a snapshot from, stop, pause, or reboot instances, or connect to them through VNC. Images: View images and instance snapshots created by project users, plus any images that are publicly available. Create, edit, and delete images, and launch instances from images and snapshots. Key Pairs: View, create, edit, import, and delete key pairs. Server Groups: View, create, edit, and delete server groups. Volume Panel Navigate: Project -> Volume Volumes: View, create, edit, delete volumes, and accept volume trnasfer. Backups: View, create, edit, and delete backups. Snapshots: View, create, edit, and delete volume snapshots. Groups: View, create, edit, and delete groups. Group Snapshots: View, create, edit, and delete group snapshots. Network Panel Navigate: Project -> Network Network Topology: View the network topology. Networks: Create and manage public and private networks. Routers: Create and manage routers. Security Groups: View, create, edit, and delete security groups and security group rules.. Load Balancers: View, create, edit, and delete load balancers. Floating IPs: Allocate an IP address to or release it from a project. Trunks: View, create, edit, and delete trunk. Orchestration Panel Navigate: Project->Orchestration Stacks: Use the REST API to orchestrate multiple composite cloud applications. Resource Types: view various resources types and their details. Template Versions: view different heat templates. Template Generator: GUI to generate and save template using drag and drop resources. Object Store Panel Navigate: Project->Object Store Containers: Create and manage containers and objects. In future you would use this tab to create Swift object storage for your projects on a need basis.","title":"Dashboard Overview"},{"location":"openstack/logging-in/dashboard-overview/#dashboard-overview","text":"When you logged-in, you will redirected to the Compute panel which is under the Project tab. In the top bar, you can see the two small tabs: \"Project\" and \"Identity\". Beneath that you can see six panels in larger print: \"Project\", \"Compute\", \"Volumes\", \"Network\", \"Orchestration\", and \"Object Store\".","title":"Dashboard Overview"},{"location":"openstack/logging-in/dashboard-overview/#project-panel","text":"Navigate: Project -> Project API Access: View API endpoints.","title":"Project Panel"},{"location":"openstack/logging-in/dashboard-overview/#compute-panel","text":"Navigate: Project -> Compute Overview: View reports for the project. Instances: View, launch, create a snapshot from, stop, pause, or reboot instances, or connect to them through VNC. Images: View images and instance snapshots created by project users, plus any images that are publicly available. Create, edit, and delete images, and launch instances from images and snapshots. Key Pairs: View, create, edit, import, and delete key pairs. Server Groups: View, create, edit, and delete server groups.","title":"Compute Panel"},{"location":"openstack/logging-in/dashboard-overview/#volume-panel","text":"Navigate: Project -> Volume Volumes: View, create, edit, delete volumes, and accept volume trnasfer. Backups: View, create, edit, and delete backups. Snapshots: View, create, edit, and delete volume snapshots. Groups: View, create, edit, and delete groups. Group Snapshots: View, create, edit, and delete group snapshots.","title":"Volume Panel"},{"location":"openstack/logging-in/dashboard-overview/#network-panel","text":"Navigate: Project -> Network Network Topology: View the network topology. Networks: Create and manage public and private networks. Routers: Create and manage routers. Security Groups: View, create, edit, and delete security groups and security group rules.. Load Balancers: View, create, edit, and delete load balancers. Floating IPs: Allocate an IP address to or release it from a project. Trunks: View, create, edit, and delete trunk.","title":"Network Panel"},{"location":"openstack/logging-in/dashboard-overview/#orchestration-panel","text":"Navigate: Project->Orchestration Stacks: Use the REST API to orchestrate multiple composite cloud applications. Resource Types: view various resources types and their details. Template Versions: view different heat templates. Template Generator: GUI to generate and save template using drag and drop resources.","title":"Orchestration Panel"},{"location":"openstack/logging-in/dashboard-overview/#object-store-panel","text":"Navigate: Project->Object Store Containers: Create and manage containers and objects. In future you would use this tab to create Swift object storage for your projects on a need basis.","title":"Object Store Panel"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/","text":"Launch a VM using OpenStack CLI First find the following details using openstack command, we would required these details during the creation of virtual machine. Flavor Image Network Security Group Key Name Get the flavor list using below openstack command: [user@laptop ~]$ openstack flavor list +--------------------------------------+---------------------+--------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+---------------------+--------+------+-----------+-------+-----------+ | 010668a2-a228-4d9f-814b-33e76bb79ca6 | c1.2xlarge | 92160 | 10 | 0 | 20 | True | | 042c3d1d-5031-48a9-91d8-2748ba2ea892 | c1.xlarge | 46080 | 10 | 0 | 10 | True | | 04f533da-df73-4a07-bfd4-845d92e2236b | m1.s2.xlarge | 16384 | 25 | 0 | 8 | True | | 2540eb5f-b39e-43aa-b9c4-bda7138f69b7 | c1.s2.4xlarge | 184320 | 25 | 0 | 40 | True | | 39dc1cc9-8931-49c4-a6ae-fd56f85ded6f | c2.s2.xlarge | 32768 | 25 | 0 | 16 | True | | 45aa2929-c2a7-49ff-b876-71f9cea3083a | m1.s2.medium | 4096 | 25 | 0 | 2 | True | | 46489535-3bbe-445b-83dc-c24c11c13017 | gpu.A100 | 96256 | 10 | 0 | 12 | True | | 57f95588-c3ae-41ee-95d7-b8b5b15814ab | m1.xlarge | 16384 | 10 | 0 | 8 | True | | 58377d5b-528f-4107-b5a4-9c840c76e2f9 | vm.2cpu.8ram | 8192 | 10 | 0 | 2 | True | | 5add4073-0822-4939-b34e-a7bdfadd6157 | custom.8c.32g | 32768 | 10 | 0 | 8 | True | | 5f098b1a-a14c-4c85-be39-4c2af6b0b9c3 | vm.24cpu.64ram.1gpu | 65536 | 50 | 0 | 24 | True | | 6e33d0e1-e604-4a99-9440-366f0c15b1f0 | m1.s2.small | 2048 | 25 | 0 | 1 | True | | 8c01d1e5-a29e-4b15-bff9-3738ac55fef7 | custom.8c.64g | 65536 | 10 | 0 | 8 | True | | 8d42c77c-3c4c-4417-991d-ab6bc39f7efb | m1.s2.large | 8192 | 25 | 0 | 4 | True | | 8f41b20c-14b5-4442-987a-1b3081d75364 | m1.small | 2048 | 10 | 0 | 1 | True | | aa182672-a501-46a2-9aa1-c41b23695960 | m1.tiny | 1024 | 10 | 0 | 1 | True | | af4109aa-9571-471c-b8ad-aacce027d1fc | m1.large | 8192 | 10 | 0 | 4 | True | | b602f97d-7870-4392-9cce-608fe8c23e30 | custom.4c.16g | 16384 | 10 | 0 | 4 | True | | b6ac3856-d984-432e-a91e-ddff853e82c8 | c2.s2.2xlarge | 65536 | 25 | 0 | 32 | True | | b7add244-7187-49c8-9cd3-1783bff483d7 | m1.s2.tiny | 1024 | 25 | 0 | 1 | True | | bb4a3292-cdf0-429a-8cbb-05540da7db6c | c2.s2.4xlarge | 81920 | 25 | 0 | 40 | True | | bd7caf51-8d5e-4019-b8ca-7af36d043ef6 | c1.4xlarge | 184320 | 10 | 0 | 40 | True | | befdf5fa-8f8e-44ee-b945-2f2cf269970d | c1.s2.2xlarge | 92160 | 25 | 0 | 20 | True | | cad489ad-3460-4a46-a867-926bdfcb2add | m1.medium | 4096 | 10 | 0 | 2 | True | | e9b78693-9c59-4528-a2b5-47d037c76670 | custom.4c.32g | 32768 | 10 | 0 | 4 | True | | fbedbef9-89dd-4549-86b1-7d34504211f2 | c1.s2.xlarge | 46080 | 25 | 0 | 10 | True | +--------------------------------------+---------------------+--------+------+-----------+-------+-----------+ Get the image name and its ID, [user@laptop ~]$ openstack image list | grep centos | f43b9e94-2862-4edc-8844-4a4e348a2d49 | centos-7-x86_64 | active | | 482f489c-d8db-47be-8f55-53096fb37c07 | centos-8.4-x86_64 | active | Get Private Virtual network details, which will be attached to the VM: [user@laptop ~]$ openstack network list +--------------------------------------+-----------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-----------------+--------------------------------------+ | 7a9efc40-4624-429d-98b7-1364ab72d8b9 | provider | b7045d5e-892b-4d8c-8837-728f62bd8fe2 | | 9aa6c35a-4fce-4a75-b167-8cfe7fb1f2d1 | harvard-network | c95c3c17-fbc1-4fb7-bc2c-d463f2c4774f | +--------------------------------------+-----------------+--------------------------------------+ Find the Security Group: [user@laptop ~]$ openstack security group list +--------------------------------------+----------+------------------------+----------------------------------+------+ | ID | Name | Description | Project | Tags | +--------------------------------------+----------+------------------------+----------------------------------+------+ | 3ca248ac-56ac-4e5f-a57c-777ed74bbd7c | default | Default security group | f01df1439b3141f8b76e68a3b58ef74a | [] | | 5cdc5f33-78fc-4af8-bf25-60b8d4e5db2a | ssh_only | Enable SSH access. | f01df1439b3141f8b76e68a3b58ef74a | [] | +--------------------------------------+----------+------------------------+----------------------------------+------+ Find the Key pair, in my case you can choose your own, [user@laptop ~]$ openstack keypair list | grep -i cloud_key | cloud_key | d5:ab:dc:1f:e5:08:44:7f:a6:21:47:23:85:32:cc:04 | ssh | Note Above details will be different for you based on your project and env. Launch an instance from an Image Now we have all the details, let\u2019s create a virtual machine using \"openstack server create\" command Syntax : $ openstack server create --flavor {Flavor-Name-Or-Flavor-ID } \\ --image {Image-Name-Or-Image-ID} \\ --nic net-id={Network-ID} \\ --user-data USER-DATA-FILE \\ --security-group {Security_Group_ID} \\ --key-name {Keypair-Name} \\ --property KEY=VALUE \\ <Instance_Name> NOTE: If you boot an instance with an \" Instance_Name \" greater than 63 characters , Compute truncates it automatically when turning it into a hostname to ensure the correct functionality of dnsmasq . Optionally, you can provide a key name for access control and a security group for security. You can also include metadata key and value pairs: --key-name {Keypair-Name} . For example, you can add a description for your server by providing the --property description=\"My Server\" parameter. You can pass user data in a local file at instance launch by using the --user-data USER-DATA-FILE parameter. If you do not provide a key pair, you will be unable to access the instance. You can also place arbitrary local files into the instance file system at creation time by using the --file <dest-filename=source-filename> parameter. You can store up to five files. For example, if you have a special authorized keys file named special_authorized_keysfile that you want to put on the instance rather than using the regular SSH key injection, you can add the \u2013file option as shown in the following example. --file /root/.ssh/authorized_keys=special_authorized_keysfile To create a VM in Specific \" Availability Zone and compute Host \" specify --availability-zone {Availbility-Zone-Name}:{Compute-Host} in above syntax. Example: [user@laptop ~]$ openstack server create --flavor m1.medium \\ --image centos-8.4-x86_64 \\ --nic net-id=e0be93b8-728b-4d4d-a272-7d672b2560a6 \\ --security-group default \\ --key-name cloud_key \\ --property description=\"My Server\" \\ test_vm_using_cli NOTE: To get more help on \"openstack server create\" command , use: [user@laptop ~]$ openstack -h server create Detailed syntax: openstack server create (--image <image> | --volume <volume>) --flavor <flavor> [--security-group <security-group>] [--key-name <key-name>] [--property <key=value>] [--file <dest-filename=source-filename>] [--user-data <user-data>] [--availability-zone <zone-name>] [--block-device-mapping <dev-name=mapping>] [--nic <net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none>] [--network <network>] [--port <port>] [--hint <key=value>] [--config-drive <config-drive-volume>|True] [--min <count>] [--max <count>] [--wait] <server-name> NOTE: Similarly we can lauch a VM using \"Volume\". Now Verify the test vm status using below commands: [user@laptop ~]$ openstack server list | grep test_vm_using_cli OR, [user@laptop ~]$ openstack server show test_vm_using_cli Associating a Floating IP to VM To Associate a floating IP to VM, first get the unused floating IP using the following command: [user@laptop ~]$ openstack floating ip list | grep None | head -2 | 071f08ac-cd10-4b89-aee4-856ead8e3ead | 169.144.107.154 | None | None | | 1baf4232-9cb7-4a44-8684-c604fa50ff60 | 169.144.107.184 | None | None | Now Associate the first IP to the server using following command: [user@laptop ~]$ openstack server add floating ip test_vm_using_cli 169.144.107.154 Use the following command to verify whether floating IP is assigned to the VM or not: [user@laptop ~]$ openstack server list | grep test_vm_using_cli | 056c0937-6222-4f49-8405-235b20d173dd | test_vm_using_cli | ACTIVE | ... nternal=192.168.15.62, 169.144.107.154 | Remove existing floating ip from the VM openstack server remove floating ip <INSTANCE_NAME_OR_ID> <FLOATING_IP_ADDRESS> Get all available security group in your project $ openstack security group list +--------------------------------------+----------+------------------------+----------------------------------+------+ | 3ca248ac-56ac-4e5f-a57c-777ed74bbd7c | default | Default security group | f01df1439b3141f8b76e68a3b58ef74a | [] | | 5cdc5f33-78fc-4af8-bf25-60b8d4e5db2a | ssh_only | Enable SSH access. | f01df1439b3141f8b76e68a3b58ef74a | [] | +--------------------------------------+----------+------------------------+----------------------------------+------+ Add existing security group to the VM openstack server add security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server add security group test_vm_using_cli ssh_only Remove existing security group from the VM openstack server remove security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server remove security group test_vm_using_cli ssh_only Alternatively , you can use the openstack port unset command to remove the group from a port: openstack port unset --security-group <SECURITY_GROUP> <PORT> Adding volume to the VM $ openstack server add volume [--device <device>] <INSTANCE_NAME_OR_ID> <VOLUME_NAME_OR_ID> Remove existing volume from the VM openstack server remove volume <INSTANCE_NAME_OR_ID> <volume> Deleting Virtual Machine from Command Line [user@laptop ~]$ openstack server delete test_vm_using_cli","title":"Launch a VM using OpenStack CLI"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#launch-a-vm-using-openstack-cli","text":"First find the following details using openstack command, we would required these details during the creation of virtual machine. Flavor Image Network Security Group Key Name Get the flavor list using below openstack command: [user@laptop ~]$ openstack flavor list +--------------------------------------+---------------------+--------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+---------------------+--------+------+-----------+-------+-----------+ | 010668a2-a228-4d9f-814b-33e76bb79ca6 | c1.2xlarge | 92160 | 10 | 0 | 20 | True | | 042c3d1d-5031-48a9-91d8-2748ba2ea892 | c1.xlarge | 46080 | 10 | 0 | 10 | True | | 04f533da-df73-4a07-bfd4-845d92e2236b | m1.s2.xlarge | 16384 | 25 | 0 | 8 | True | | 2540eb5f-b39e-43aa-b9c4-bda7138f69b7 | c1.s2.4xlarge | 184320 | 25 | 0 | 40 | True | | 39dc1cc9-8931-49c4-a6ae-fd56f85ded6f | c2.s2.xlarge | 32768 | 25 | 0 | 16 | True | | 45aa2929-c2a7-49ff-b876-71f9cea3083a | m1.s2.medium | 4096 | 25 | 0 | 2 | True | | 46489535-3bbe-445b-83dc-c24c11c13017 | gpu.A100 | 96256 | 10 | 0 | 12 | True | | 57f95588-c3ae-41ee-95d7-b8b5b15814ab | m1.xlarge | 16384 | 10 | 0 | 8 | True | | 58377d5b-528f-4107-b5a4-9c840c76e2f9 | vm.2cpu.8ram | 8192 | 10 | 0 | 2 | True | | 5add4073-0822-4939-b34e-a7bdfadd6157 | custom.8c.32g | 32768 | 10 | 0 | 8 | True | | 5f098b1a-a14c-4c85-be39-4c2af6b0b9c3 | vm.24cpu.64ram.1gpu | 65536 | 50 | 0 | 24 | True | | 6e33d0e1-e604-4a99-9440-366f0c15b1f0 | m1.s2.small | 2048 | 25 | 0 | 1 | True | | 8c01d1e5-a29e-4b15-bff9-3738ac55fef7 | custom.8c.64g | 65536 | 10 | 0 | 8 | True | | 8d42c77c-3c4c-4417-991d-ab6bc39f7efb | m1.s2.large | 8192 | 25 | 0 | 4 | True | | 8f41b20c-14b5-4442-987a-1b3081d75364 | m1.small | 2048 | 10 | 0 | 1 | True | | aa182672-a501-46a2-9aa1-c41b23695960 | m1.tiny | 1024 | 10 | 0 | 1 | True | | af4109aa-9571-471c-b8ad-aacce027d1fc | m1.large | 8192 | 10 | 0 | 4 | True | | b602f97d-7870-4392-9cce-608fe8c23e30 | custom.4c.16g | 16384 | 10 | 0 | 4 | True | | b6ac3856-d984-432e-a91e-ddff853e82c8 | c2.s2.2xlarge | 65536 | 25 | 0 | 32 | True | | b7add244-7187-49c8-9cd3-1783bff483d7 | m1.s2.tiny | 1024 | 25 | 0 | 1 | True | | bb4a3292-cdf0-429a-8cbb-05540da7db6c | c2.s2.4xlarge | 81920 | 25 | 0 | 40 | True | | bd7caf51-8d5e-4019-b8ca-7af36d043ef6 | c1.4xlarge | 184320 | 10 | 0 | 40 | True | | befdf5fa-8f8e-44ee-b945-2f2cf269970d | c1.s2.2xlarge | 92160 | 25 | 0 | 20 | True | | cad489ad-3460-4a46-a867-926bdfcb2add | m1.medium | 4096 | 10 | 0 | 2 | True | | e9b78693-9c59-4528-a2b5-47d037c76670 | custom.4c.32g | 32768 | 10 | 0 | 4 | True | | fbedbef9-89dd-4549-86b1-7d34504211f2 | c1.s2.xlarge | 46080 | 25 | 0 | 10 | True | +--------------------------------------+---------------------+--------+------+-----------+-------+-----------+ Get the image name and its ID, [user@laptop ~]$ openstack image list | grep centos | f43b9e94-2862-4edc-8844-4a4e348a2d49 | centos-7-x86_64 | active | | 482f489c-d8db-47be-8f55-53096fb37c07 | centos-8.4-x86_64 | active | Get Private Virtual network details, which will be attached to the VM: [user@laptop ~]$ openstack network list +--------------------------------------+-----------------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+-----------------+--------------------------------------+ | 7a9efc40-4624-429d-98b7-1364ab72d8b9 | provider | b7045d5e-892b-4d8c-8837-728f62bd8fe2 | | 9aa6c35a-4fce-4a75-b167-8cfe7fb1f2d1 | harvard-network | c95c3c17-fbc1-4fb7-bc2c-d463f2c4774f | +--------------------------------------+-----------------+--------------------------------------+ Find the Security Group: [user@laptop ~]$ openstack security group list +--------------------------------------+----------+------------------------+----------------------------------+------+ | ID | Name | Description | Project | Tags | +--------------------------------------+----------+------------------------+----------------------------------+------+ | 3ca248ac-56ac-4e5f-a57c-777ed74bbd7c | default | Default security group | f01df1439b3141f8b76e68a3b58ef74a | [] | | 5cdc5f33-78fc-4af8-bf25-60b8d4e5db2a | ssh_only | Enable SSH access. | f01df1439b3141f8b76e68a3b58ef74a | [] | +--------------------------------------+----------+------------------------+----------------------------------+------+ Find the Key pair, in my case you can choose your own, [user@laptop ~]$ openstack keypair list | grep -i cloud_key | cloud_key | d5:ab:dc:1f:e5:08:44:7f:a6:21:47:23:85:32:cc:04 | ssh | Note Above details will be different for you based on your project and env.","title":"Launch a VM using OpenStack CLI"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#launch-an-instance-from-an-image","text":"Now we have all the details, let\u2019s create a virtual machine using \"openstack server create\" command Syntax : $ openstack server create --flavor {Flavor-Name-Or-Flavor-ID } \\ --image {Image-Name-Or-Image-ID} \\ --nic net-id={Network-ID} \\ --user-data USER-DATA-FILE \\ --security-group {Security_Group_ID} \\ --key-name {Keypair-Name} \\ --property KEY=VALUE \\ <Instance_Name> NOTE: If you boot an instance with an \" Instance_Name \" greater than 63 characters , Compute truncates it automatically when turning it into a hostname to ensure the correct functionality of dnsmasq . Optionally, you can provide a key name for access control and a security group for security. You can also include metadata key and value pairs: --key-name {Keypair-Name} . For example, you can add a description for your server by providing the --property description=\"My Server\" parameter. You can pass user data in a local file at instance launch by using the --user-data USER-DATA-FILE parameter. If you do not provide a key pair, you will be unable to access the instance. You can also place arbitrary local files into the instance file system at creation time by using the --file <dest-filename=source-filename> parameter. You can store up to five files. For example, if you have a special authorized keys file named special_authorized_keysfile that you want to put on the instance rather than using the regular SSH key injection, you can add the \u2013file option as shown in the following example. --file /root/.ssh/authorized_keys=special_authorized_keysfile To create a VM in Specific \" Availability Zone and compute Host \" specify --availability-zone {Availbility-Zone-Name}:{Compute-Host} in above syntax. Example: [user@laptop ~]$ openstack server create --flavor m1.medium \\ --image centos-8.4-x86_64 \\ --nic net-id=e0be93b8-728b-4d4d-a272-7d672b2560a6 \\ --security-group default \\ --key-name cloud_key \\ --property description=\"My Server\" \\ test_vm_using_cli NOTE: To get more help on \"openstack server create\" command , use: [user@laptop ~]$ openstack -h server create Detailed syntax: openstack server create (--image <image> | --volume <volume>) --flavor <flavor> [--security-group <security-group>] [--key-name <key-name>] [--property <key=value>] [--file <dest-filename=source-filename>] [--user-data <user-data>] [--availability-zone <zone-name>] [--block-device-mapping <dev-name=mapping>] [--nic <net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none>] [--network <network>] [--port <port>] [--hint <key=value>] [--config-drive <config-drive-volume>|True] [--min <count>] [--max <count>] [--wait] <server-name> NOTE: Similarly we can lauch a VM using \"Volume\". Now Verify the test vm status using below commands: [user@laptop ~]$ openstack server list | grep test_vm_using_cli OR, [user@laptop ~]$ openstack server show test_vm_using_cli","title":"Launch an instance from an Image"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#associating-a-floating-ip-to-vm","text":"To Associate a floating IP to VM, first get the unused floating IP using the following command: [user@laptop ~]$ openstack floating ip list | grep None | head -2 | 071f08ac-cd10-4b89-aee4-856ead8e3ead | 169.144.107.154 | None | None | | 1baf4232-9cb7-4a44-8684-c604fa50ff60 | 169.144.107.184 | None | None | Now Associate the first IP to the server using following command: [user@laptop ~]$ openstack server add floating ip test_vm_using_cli 169.144.107.154 Use the following command to verify whether floating IP is assigned to the VM or not: [user@laptop ~]$ openstack server list | grep test_vm_using_cli | 056c0937-6222-4f49-8405-235b20d173dd | test_vm_using_cli | ACTIVE | ... nternal=192.168.15.62, 169.144.107.154 |","title":"Associating a Floating IP to VM"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-floating-ip-from-the-vm","text":"openstack server remove floating ip <INSTANCE_NAME_OR_ID> <FLOATING_IP_ADDRESS>","title":"Remove existing floating ip from the VM"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#get-all-available-security-group-in-your-project","text":"$ openstack security group list +--------------------------------------+----------+------------------------+----------------------------------+------+ | 3ca248ac-56ac-4e5f-a57c-777ed74bbd7c | default | Default security group | f01df1439b3141f8b76e68a3b58ef74a | [] | | 5cdc5f33-78fc-4af8-bf25-60b8d4e5db2a | ssh_only | Enable SSH access. | f01df1439b3141f8b76e68a3b58ef74a | [] | +--------------------------------------+----------+------------------------+----------------------------------+------+","title":"Get all available security group in your project"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#add-existing-security-group-to-the-vm","text":"openstack server add security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server add security group test_vm_using_cli ssh_only","title":"Add existing security group to the VM"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-security-group-from-the-vm","text":"openstack server remove security group <INSTANCE_NAME_OR_ID> <SECURITY_GROUP> Example: openstack server remove security group test_vm_using_cli ssh_only Alternatively , you can use the openstack port unset command to remove the group from a port: openstack port unset --security-group <SECURITY_GROUP> <PORT>","title":"Remove existing security group from the VM"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#adding-volume-to-the-vm","text":"$ openstack server add volume [--device <device>] <INSTANCE_NAME_OR_ID> <VOLUME_NAME_OR_ID>","title":"Adding volume to the VM"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#remove-existing-volume-from-the-vm","text":"openstack server remove volume <INSTANCE_NAME_OR_ID> <volume>","title":"Remove existing volume from the VM"},{"location":"openstack/openstack-cli/launch-a-VM-using-openstack-CLI/#deleting-virtual-machine-from-command-line","text":"[user@laptop ~]$ openstack server delete test_vm_using_cli","title":"Deleting Virtual Machine from Command Line"},{"location":"openstack/openstack-cli/openstack-CLI/","text":"OpenStack CLI References OpenStack Command Line Client(CLI) Cheat Sheet The OpenStack CLI is designed for interactive use. OpenStackClient (aka OSC) is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure. OpenStackClient is primarily configured using command line options and environment variables. Most of those settings can also be placed into a configuration file to simplify managing multiple cloud configurations. Most global options have a corresponding environment variable that may also be used to set the value. If both are present, the command-line option takes priority. It's also possible to call it from a bash script or similar, but typically it is too slow for heavy scripting use. Command Line setup To use the CLI, you must create an application credentials and set the appropriate environment variables. Navigate to Identity > Application Credentials Click on \"Create Application Credential\" button and provide a Name and Roles for the application credential. All other fields are optional and leaving the \"Secret\" field empty will set it to autogenerate (recommended). NOTE: Please note that an application credential is only valid for a single project, and to access multiple projects you need to create an application credential for each. You can switch projects by clicking on the project name at the top right corner and choosing from the dropdown under \"Project\". After clicking \"Create Application Credential\" button, the ID and Secret will be displayed and you will be prompted to Download openrc file or to Download clouds.yaml . Both of these are different methods of configuring the client for CLI access. Configuration The CLI is configured via environment variables and command-line options as listed in Authentication . Configuration Files OpenStack RC File Find the file (by default it will be named the same as the application credential name with the suffix -openrc.sh where project is the name of your OpenStack project). Source the file: [user@laptop ~]$ source app-cred-<Credential_Name>-openrc.sh NOTE: When you source the file, environment variables are set for your current shell. The variables enable the OpenStack client commands to communicate with the OpenStack services that run in the cloud. This just stores your entry into the environment variable - there's no validation at this stage. You can inspect the downloaded file to retrieve the ID and Secret if necessary and see what other environment variables are set. clouds.yaml clouds.yaml is a configuration file that contains everything needed to connect to one or more clouds. It may contain private information and is generally considered private to a user. For more information on configuring the OpenStackClient with clouds.yaml please see the OpenStack documentation Install the OpenStack command-line clients For more information on configuring the OpenStackClient please see the OpenStack documentation OpenStack Hello World To test that you have everything configured, try out some commands. The following command lists all the images available to your project: [user@laptop ~]$ openstack image list +--------------------------------------+---------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------+--------+ | f43b9e94-2862-4edc-8844-4a4e348a2d49 | centos-7-x86_64 | active | | 482f489c-d8db-47be-8f55-53096fb37c07 | centos-8.4-x86_64 | active | | d7a71175-bb1b-412c-a6d3-83e5e1ad0646 | debian-10-x86_64 | active | | 2a8e36a3-31ea-4c6d-adce-5c18101c6be2 | fedora-34-x86_64 | active | | ef40d46e-88f9-4099-9d1d-c6c85297f042 | ubuntu-21.04-x86_64 | active | | a44f1bac-35c8-4abd-8669-a6c6ae7ef2e1 | ubuntu-21.04-x86_64 | active | +--------------------------------------+---------------------+--------+ If you have launched some instances already, the following command shows a list of your project's instances: [user@laptop ~]$ openstack server list +--------------------------------------+-----------+--------+------------------------------------------------+--------------------------+---------------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+-----------+--------+------------------------------------------------+--------------------------+---------------------+ | 1c96ba49-a20f-4c88-bbcf-93e2364365f5 | vm-test | ACTIVE | harvard-network=140.247.152.235, 192.168.0.23 | N/A (booted from volume) | m1.medium | | dd0d8053-ab88-4d4f-b5bc-97e7e2fe035a | gpu-test | ACTIVE | harvard-network=140.247.152.195, 192.168.0.227 | | vm.24cpu.64ram.1gpu | +--------------------------------------+-----------+--------+------------------------------------------------+--------------------------+---------------------+ If you don't have any instances, you will get the error list index out of range , which is why we didn't suggest this command for your first test: [user@laptop ~]$ openstack server list list index out of range If you see this error: [user@laptop ~]$ openstack server list The request you have made requires authentication. (HTTP 401) (Request-ID: req-6a827bf3-d5e8-47f2-984c-b6edeeb2f7fb) Then your environment variables are likely not configured correctly. The most common reason is that you made a typo when entering your password. Try sourcing the OpenStack RC file again and retyping it. You can type openstack -h to see a list of available commands. Note This includes some admin-only commands. If you try one of these by mistake, you might see this output: [user@laptop ~]$ openstack user list You are not authorized to perform the requested action: identity:list_users. (HTTP 403) (Request-ID: req-cafe1e5c-8a71-44ab-bd21-0e0f25414062) Depending on your needs for API interaction, this might be sufficient. If you just occasionally want to run 1 or 2 of these commands from your terminal, you can do it manually or write a quick bash script that makes use of this CLI. However, this isn't a very optimized way to do complex interactions with OpenStack. For that, you want to write scripts that interact with the python SDK bindings directly. Pro Tip: If you find yourself fiddling extensively with awk and grep to extract things like project IDs from the CLI output, it's time to move on to using the client libraries or the RESTful API directly in your scripts.","title":"OpenStack CLI"},{"location":"openstack/openstack-cli/openstack-CLI/#openstack-cli","text":"","title":"OpenStack CLI"},{"location":"openstack/openstack-cli/openstack-CLI/#references","text":"OpenStack Command Line Client(CLI) Cheat Sheet The OpenStack CLI is designed for interactive use. OpenStackClient (aka OSC) is a command-line client for OpenStack that brings the command set for Compute, Identity, Image, Object Storage and Block Storage APIs together in a single shell with a uniform command structure. OpenStackClient is primarily configured using command line options and environment variables. Most of those settings can also be placed into a configuration file to simplify managing multiple cloud configurations. Most global options have a corresponding environment variable that may also be used to set the value. If both are present, the command-line option takes priority. It's also possible to call it from a bash script or similar, but typically it is too slow for heavy scripting use.","title":"References"},{"location":"openstack/openstack-cli/openstack-CLI/#command-line-setup","text":"To use the CLI, you must create an application credentials and set the appropriate environment variables. Navigate to Identity > Application Credentials Click on \"Create Application Credential\" button and provide a Name and Roles for the application credential. All other fields are optional and leaving the \"Secret\" field empty will set it to autogenerate (recommended). NOTE: Please note that an application credential is only valid for a single project, and to access multiple projects you need to create an application credential for each. You can switch projects by clicking on the project name at the top right corner and choosing from the dropdown under \"Project\". After clicking \"Create Application Credential\" button, the ID and Secret will be displayed and you will be prompted to Download openrc file or to Download clouds.yaml . Both of these are different methods of configuring the client for CLI access.","title":"Command Line setup"},{"location":"openstack/openstack-cli/openstack-CLI/#configuration","text":"The CLI is configured via environment variables and command-line options as listed in Authentication .","title":"Configuration"},{"location":"openstack/openstack-cli/openstack-CLI/#configuration-files","text":"","title":"Configuration Files"},{"location":"openstack/openstack-cli/openstack-CLI/#openstack-rc-file","text":"Find the file (by default it will be named the same as the application credential name with the suffix -openrc.sh where project is the name of your OpenStack project). Source the file: [user@laptop ~]$ source app-cred-<Credential_Name>-openrc.sh NOTE: When you source the file, environment variables are set for your current shell. The variables enable the OpenStack client commands to communicate with the OpenStack services that run in the cloud. This just stores your entry into the environment variable - there's no validation at this stage. You can inspect the downloaded file to retrieve the ID and Secret if necessary and see what other environment variables are set.","title":"OpenStack RC File"},{"location":"openstack/openstack-cli/openstack-CLI/#cloudsyaml","text":"clouds.yaml is a configuration file that contains everything needed to connect to one or more clouds. It may contain private information and is generally considered private to a user. For more information on configuring the OpenStackClient with clouds.yaml please see the OpenStack documentation","title":"clouds.yaml"},{"location":"openstack/openstack-cli/openstack-CLI/#install-the-openstack-command-line-clients","text":"For more information on configuring the OpenStackClient please see the OpenStack documentation","title":"Install the OpenStack command-line clients"},{"location":"openstack/openstack-cli/openstack-CLI/#openstack-hello-world","text":"To test that you have everything configured, try out some commands. The following command lists all the images available to your project: [user@laptop ~]$ openstack image list +--------------------------------------+---------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------+--------+ | f43b9e94-2862-4edc-8844-4a4e348a2d49 | centos-7-x86_64 | active | | 482f489c-d8db-47be-8f55-53096fb37c07 | centos-8.4-x86_64 | active | | d7a71175-bb1b-412c-a6d3-83e5e1ad0646 | debian-10-x86_64 | active | | 2a8e36a3-31ea-4c6d-adce-5c18101c6be2 | fedora-34-x86_64 | active | | ef40d46e-88f9-4099-9d1d-c6c85297f042 | ubuntu-21.04-x86_64 | active | | a44f1bac-35c8-4abd-8669-a6c6ae7ef2e1 | ubuntu-21.04-x86_64 | active | +--------------------------------------+---------------------+--------+ If you have launched some instances already, the following command shows a list of your project's instances: [user@laptop ~]$ openstack server list +--------------------------------------+-----------+--------+------------------------------------------------+--------------------------+---------------------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+-----------+--------+------------------------------------------------+--------------------------+---------------------+ | 1c96ba49-a20f-4c88-bbcf-93e2364365f5 | vm-test | ACTIVE | harvard-network=140.247.152.235, 192.168.0.23 | N/A (booted from volume) | m1.medium | | dd0d8053-ab88-4d4f-b5bc-97e7e2fe035a | gpu-test | ACTIVE | harvard-network=140.247.152.195, 192.168.0.227 | | vm.24cpu.64ram.1gpu | +--------------------------------------+-----------+--------+------------------------------------------------+--------------------------+---------------------+ If you don't have any instances, you will get the error list index out of range , which is why we didn't suggest this command for your first test: [user@laptop ~]$ openstack server list list index out of range If you see this error: [user@laptop ~]$ openstack server list The request you have made requires authentication. (HTTP 401) (Request-ID: req-6a827bf3-d5e8-47f2-984c-b6edeeb2f7fb) Then your environment variables are likely not configured correctly. The most common reason is that you made a typo when entering your password. Try sourcing the OpenStack RC file again and retyping it. You can type openstack -h to see a list of available commands. Note This includes some admin-only commands. If you try one of these by mistake, you might see this output: [user@laptop ~]$ openstack user list You are not authorized to perform the requested action: identity:list_users. (HTTP 403) (Request-ID: req-cafe1e5c-8a71-44ab-bd21-0e0f25414062) Depending on your needs for API interaction, this might be sufficient. If you just occasionally want to run 1 or 2 of these commands from your terminal, you can do it manually or write a quick bash script that makes use of this CLI. However, this isn't a very optimized way to do complex interactions with OpenStack. For that, you want to write scripts that interact with the python SDK bindings directly. Pro Tip: If you find yourself fiddling extensively with awk and grep to extract things like project IDs from the CLI output, it's time to move on to using the client libraries or the RESTful API directly in your scripts.","title":"OpenStack Hello World"},{"location":"openstack/persistent-storage/volumes/","text":"Persistent Storage Volumes A volume is a detachable block storage device, similar to a USB hard drive. You can attach a volume to only one instance. Volumes are the Block Storage devices that you attach to instances to enable persistent storage. Users can attach a volume to a running instance or detach a volume and attach it to another instance at any time. Ownership of volumes can be transferred to another project. Some uses for volumes: Persistent data storage for ephemeral instances. Transfer of data between projects Bootable image where disk changes persist Mounting the disk of one instance to another for troubleshooting Navigate to Project -> Volumes -> Volumes. Create an empty volume An empty volume is like an unformatted USB stick. We'll attach it to an instance, create a filesystem on it, and mount it to the instance. Click \"Create Volume\". In the Create Volume dialog box, give your volume a name. The description field is optional. Choose \"empty volume\" from the Source dropdown. This will create a volume that is like an unformatted hard disk. Choose a size (In GiB) for your volume. Leave Type and Availibility Zone as it as. Only admin to the NERC OpenStack will be able to manage volume types. Click \"Create Volume\" button. In a few moments, the newly created volume will appear in the Volumes list with the Status \"Available\". Attach the volume to an instance In the Actions column, click the dropdown and select \"Manage Attachments\". From the menu, choose the instance you want to connect the volume to from Attach to Instance, and click \"Attach Volume\". The volume now has a status of \"In Use\" and the Attached To column shows which instance it is attached to, and what device name it has. This will be something like /dev/vdb but it can vary depending on the state of your instance, and whether you have attached volumes before. Make note of the device name of your volume. Format and mount the volume SSH to your instance. You should now see the volume as an additional disk in the output of sudo fdisk -l or lsblk . # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk We see the volume here as the disk 'vdb', which matches the /dev/vdb/ we noted in the Attached To column. In this case it's easy to spot because there is only one additional disk attached to the instance, but it's important to keep track of the device name, especially if you have multiple volumes attached. Also, a given volume might not get the same device name the second time you attach it to an instance. Create a filesystem on the volume and mount it - in the example we create an ext4 filesystem: # mkfs.ext4 /dev/vdb # mkdir /mnt/test_volume # mount /dev/vdb /mnt/test_volume The volume is now available at the mount point: # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk /mnt/test_volume If you place data in the directory /mnt/test_volume , detach the volume, and mount it to another instance, the second instance will have access to the data. Detach a volume To detach a mounted volume by going back to Manage Attachments and choosing Detach Volume. Once it is successfully detached, you can use Manage Attachments to attach it to another instance if desired. Delete volumes When you delete an instance, the data of its attached volumes is not destroyed. Navigate to Project -> Volumes -> Volumes. Select the volume or volumes that you want to delete. Click \"Delete Volumes\" button. In the Confirm Delete Volumes window, click the Delete Volumes button to confirm the action. Create Volume from Image You can create a volume from an existing image. If the image is bootable, you can use the volume to launch an instance. Click \"Create Volume\". This time, in the Create Volume dialog box, for Volume Source, choose 'Image'. From the 'Use Image as a Source' dropdown, choose the image you'd like to use. To use this volume to launch an instance, you can choose Boot From Volume in the Instance \"Select Boot Source\" dropdown when creating an instance, then select your volume from the Volumes list. Make sure 'Delete Volume on Instance Delete' is selected \"No\" if you want the volume to persist even after the instance is terminated. Note Only one instance at a time can be booted from a given volume.","title":"Volumes"},{"location":"openstack/persistent-storage/volumes/#persistent-storage","text":"","title":"Persistent Storage"},{"location":"openstack/persistent-storage/volumes/#volumes","text":"A volume is a detachable block storage device, similar to a USB hard drive. You can attach a volume to only one instance. Volumes are the Block Storage devices that you attach to instances to enable persistent storage. Users can attach a volume to a running instance or detach a volume and attach it to another instance at any time. Ownership of volumes can be transferred to another project. Some uses for volumes: Persistent data storage for ephemeral instances. Transfer of data between projects Bootable image where disk changes persist Mounting the disk of one instance to another for troubleshooting Navigate to Project -> Volumes -> Volumes.","title":"Volumes"},{"location":"openstack/persistent-storage/volumes/#create-an-empty-volume","text":"An empty volume is like an unformatted USB stick. We'll attach it to an instance, create a filesystem on it, and mount it to the instance. Click \"Create Volume\". In the Create Volume dialog box, give your volume a name. The description field is optional. Choose \"empty volume\" from the Source dropdown. This will create a volume that is like an unformatted hard disk. Choose a size (In GiB) for your volume. Leave Type and Availibility Zone as it as. Only admin to the NERC OpenStack will be able to manage volume types. Click \"Create Volume\" button. In a few moments, the newly created volume will appear in the Volumes list with the Status \"Available\".","title":"Create an empty volume"},{"location":"openstack/persistent-storage/volumes/#attach-the-volume-to-an-instance","text":"In the Actions column, click the dropdown and select \"Manage Attachments\". From the menu, choose the instance you want to connect the volume to from Attach to Instance, and click \"Attach Volume\". The volume now has a status of \"In Use\" and the Attached To column shows which instance it is attached to, and what device name it has. This will be something like /dev/vdb but it can vary depending on the state of your instance, and whether you have attached volumes before. Make note of the device name of your volume.","title":"Attach the volume to an instance"},{"location":"openstack/persistent-storage/volumes/#format-and-mount-the-volume","text":"SSH to your instance. You should now see the volume as an additional disk in the output of sudo fdisk -l or lsblk . # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk We see the volume here as the disk 'vdb', which matches the /dev/vdb/ we noted in the Attached To column. In this case it's easy to spot because there is only one additional disk attached to the instance, but it's important to keep track of the device name, especially if you have multiple volumes attached. Also, a given volume might not get the same device name the second time you attach it to an instance. Create a filesystem on the volume and mount it - in the example we create an ext4 filesystem: # mkfs.ext4 /dev/vdb # mkdir /mnt/test_volume # mount /dev/vdb /mnt/test_volume The volume is now available at the mount point: # lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT ... vda 254:0 0 10G 0 disk \u251c\u2500vda1 254:1 0 9.9G 0 part / \u251c\u2500vda14 254:14 0 4M 0 part \u2514\u2500vda15 254:15 0 106M 0 part /boot/efi vdb 254:16 0 1G 0 disk /mnt/test_volume If you place data in the directory /mnt/test_volume , detach the volume, and mount it to another instance, the second instance will have access to the data.","title":"Format and mount the volume"},{"location":"openstack/persistent-storage/volumes/#detach-a-volume","text":"To detach a mounted volume by going back to Manage Attachments and choosing Detach Volume. Once it is successfully detached, you can use Manage Attachments to attach it to another instance if desired.","title":"Detach a volume"},{"location":"openstack/persistent-storage/volumes/#delete-volumes","text":"When you delete an instance, the data of its attached volumes is not destroyed. Navigate to Project -> Volumes -> Volumes. Select the volume or volumes that you want to delete. Click \"Delete Volumes\" button. In the Confirm Delete Volumes window, click the Delete Volumes button to confirm the action.","title":"Delete volumes"},{"location":"openstack/persistent-storage/volumes/#create-volume-from-image","text":"You can create a volume from an existing image. If the image is bootable, you can use the volume to launch an instance. Click \"Create Volume\". This time, in the Create Volume dialog box, for Volume Source, choose 'Image'. From the 'Use Image as a Source' dropdown, choose the image you'd like to use. To use this volume to launch an instance, you can choose Boot From Volume in the Instance \"Select Boot Source\" dropdown when creating an instance, then select your volume from the Volumes list. Make sure 'Delete Volume on Instance Delete' is selected \"No\" if you want the volume to persist even after the instance is terminated. Note Only one instance at a time can be booted from a given volume.","title":"Create Volume from Image"},{"location":"openstack/python-sdk/python-SDK/","text":"References Python SDK page at PyPi OpenStack Python SDK User Guide From the Python SDK page at Pypi: Definition openstacksdk is a client library for building applications to work with OpenStack clouds. The project aims to provide a consistent and complete set of interactions with OpenStack's many services, along with complete documentation, examples, and tools. If you need to plug OpenStack into existing scripts using another language, there are a variety of other SDKs at various levels of active development. A list of known SDKs is maintained on the official OpenStack wiki. Known SDKs","title":"Python SDK"},{"location":"openstack/python-sdk/python-SDK/#references","text":"Python SDK page at PyPi OpenStack Python SDK User Guide From the Python SDK page at Pypi: Definition openstacksdk is a client library for building applications to work with OpenStack clouds. The project aims to provide a consistent and complete set of interactions with OpenStack's many services, along with complete documentation, examples, and tools. If you need to plug OpenStack into existing scripts using another language, there are a variety of other SDKs at various levels of active development. A list of known SDKs is maintained on the official OpenStack wiki. Known SDKs","title":"References"},{"location":"openstack/setting-up-a-network/create-a-router/","text":"Create a Router A router acts as a gateway for external connectivity. By connecting your private network to the public network via a router, you can connect your instance to the Internet, install packages, etc. without needing to associate it with a public IP address. You can view routers by clicking Project, then click Network panel and choose Routers from the tabs that appears. Click \"Create Network\" button on the right side of the screen. In the Create Router dialog box, specify a name for the router. From the External Network dropdown, select the \u2018provider\u2019 network, and click \"Create Router\" button. This will set the Gateway for the new router to public network. The new router is now displayed in the Routers tab. You should now see the router in the Network Topology view. (It also appears under Project -> Network -> Routers). Notice that it is now connected to the public network, but not your private network. Set Internal Interface on the Router In order to route between your private network and the outside world, you must give the router an interface on your private network. Perform the following steps in order to To connect a private network to the newly created router: a. On the Routers tab, click the name of the router. b. On the Router Details page, click the Interfaces tab, then click Add Interface. c. In the Add Interface dialog box, select a Subnet. Optionally, in the Add Interface dialog box, set an IP Address for the router interface for the selected subnet. If you choose not to set the IP Address value, then by default OpenStack Networking uses the first host IP address in the subnet. The Router Name and Router ID fields are automatically updated. d. Click \"Add Interface\". The Router will now appear connected to the private network in Network Topology tab. OR, You can set Internal Interface on the Router From the Network Topology view, click on the router you just created, and click \u2018Add Interface\u2019 on the popup that appears. Then, this will show Add Interface dialog box. So, you just complete steps b to c as mentioned above.","title":"Create a Router"},{"location":"openstack/setting-up-a-network/create-a-router/#create-a-router","text":"A router acts as a gateway for external connectivity. By connecting your private network to the public network via a router, you can connect your instance to the Internet, install packages, etc. without needing to associate it with a public IP address. You can view routers by clicking Project, then click Network panel and choose Routers from the tabs that appears. Click \"Create Network\" button on the right side of the screen. In the Create Router dialog box, specify a name for the router. From the External Network dropdown, select the \u2018provider\u2019 network, and click \"Create Router\" button. This will set the Gateway for the new router to public network. The new router is now displayed in the Routers tab. You should now see the router in the Network Topology view. (It also appears under Project -> Network -> Routers). Notice that it is now connected to the public network, but not your private network.","title":"Create a Router"},{"location":"openstack/setting-up-a-network/create-a-router/#set-internal-interface-on-the-router","text":"In order to route between your private network and the outside world, you must give the router an interface on your private network. Perform the following steps in order to To connect a private network to the newly created router: a. On the Routers tab, click the name of the router. b. On the Router Details page, click the Interfaces tab, then click Add Interface. c. In the Add Interface dialog box, select a Subnet. Optionally, in the Add Interface dialog box, set an IP Address for the router interface for the selected subnet. If you choose not to set the IP Address value, then by default OpenStack Networking uses the first host IP address in the subnet. The Router Name and Router ID fields are automatically updated. d. Click \"Add Interface\". The Router will now appear connected to the private network in Network Topology tab. OR, You can set Internal Interface on the Router From the Network Topology view, click on the router you just created, and click \u2018Add Interface\u2019 on the popup that appears. Then, this will show Add Interface dialog box. So, you just complete steps b to c as mentioned above.","title":"Set Internal Interface on the Router"},{"location":"openstack/setting-up-a-network/set-up-a-private-network/","text":"Set up a Private Network Create a Network You can view your/ existing network topology by clicking Project, then click Network panel and choose Network Topology from the tabs that appears. This shows public network which is accessible to all projects. Click on \"Networks\" tab and then click \"Create Network\" button on the right side of the screen. In the Create Network dialog box, specify the following values. Network tab: Network Name: Specify a name to identify the network. Shared: Share the network with other projects. Non admin users are not allowed to set shared option and not even visible to them. Admin State: The state to start the network in. Create Subnet: Select this check box to create a subnet Give your network a name, and leave the two checkboxes for \"Admin State\" and \"Create Subnet\" with the default settings. Subnet tab: You do not have to specify a subnet when you create a network, but if you do not specify a subnet, the network can not be attached to an instance. Subnet Name: Specify a name for the subnet. Network Address: Specify the IP address for the subnet. For your private networks, you should use IP addresses which fall within the ranges that are specifically reserved for private networks: 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 In the example below, we configure a network containing addresses 192.168.0.1 to 192.168.0.255 using CIDR 192.168.0.0/24 Technically, your private network will still work if you choose any IP outside these ranges, but this causes problems with connecting to IPs in the outside world - so don't do it! IP Version: Select IPv4 or IPv6. Gateway IP: Specify an IP address for a specific gateway. This parameter is optional. Disable Gateway: Select this check box to disable a gateway IP address. Subnet Details tab Enable DHCP: Select this check box to enable DHCP so that your VM instances will automatically be assigned an IP on the subnet. Allocation Pools: Specify IP address pools. DNS Name Servers: Specify a name for the DNS server. If you use '8.8.8.8' (you may recognize this as one of Google's public name servers). Host Routes: Specify the IP address of host routes. For now, you can leave the Allocation Pools and Host Routes boxes empty and click on \"Create\" button. But here we specify Allocation Pools of 192.168.0.2,192.168.0.254 . The Network Topology should now show your virtual private network next to the public network.","title":"Set up a Private Network"},{"location":"openstack/setting-up-a-network/set-up-a-private-network/#set-up-a-private-network","text":"","title":"Set up a Private Network"},{"location":"openstack/setting-up-a-network/set-up-a-private-network/#create-a-network","text":"You can view your/ existing network topology by clicking Project, then click Network panel and choose Network Topology from the tabs that appears. This shows public network which is accessible to all projects. Click on \"Networks\" tab and then click \"Create Network\" button on the right side of the screen. In the Create Network dialog box, specify the following values. Network tab: Network Name: Specify a name to identify the network. Shared: Share the network with other projects. Non admin users are not allowed to set shared option and not even visible to them. Admin State: The state to start the network in. Create Subnet: Select this check box to create a subnet Give your network a name, and leave the two checkboxes for \"Admin State\" and \"Create Subnet\" with the default settings. Subnet tab: You do not have to specify a subnet when you create a network, but if you do not specify a subnet, the network can not be attached to an instance. Subnet Name: Specify a name for the subnet. Network Address: Specify the IP address for the subnet. For your private networks, you should use IP addresses which fall within the ranges that are specifically reserved for private networks: 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 In the example below, we configure a network containing addresses 192.168.0.1 to 192.168.0.255 using CIDR 192.168.0.0/24 Technically, your private network will still work if you choose any IP outside these ranges, but this causes problems with connecting to IPs in the outside world - so don't do it! IP Version: Select IPv4 or IPv6. Gateway IP: Specify an IP address for a specific gateway. This parameter is optional. Disable Gateway: Select this check box to disable a gateway IP address. Subnet Details tab Enable DHCP: Select this check box to enable DHCP so that your VM instances will automatically be assigned an IP on the subnet. Allocation Pools: Specify IP address pools. DNS Name Servers: Specify a name for the DNS server. If you use '8.8.8.8' (you may recognize this as one of Google's public name servers). Host Routes: Specify the IP address of host routes. For now, you can leave the Allocation Pools and Host Routes boxes empty and click on \"Create\" button. But here we specify Allocation Pools of 192.168.0.2,192.168.0.254 . The Network Topology should now show your virtual private network next to the public network.","title":"Create a Network"}]}